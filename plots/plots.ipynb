{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-polyhedral disjunctive cuts plotting worksheet\n",
    "1. Table 1: Summary statistics for percent gap closed by VPCs --- avg (%) and number of strict wins (best by at least `EPS`), including set of all instances and set of ≥ 10% gap closed instances\n",
    "2. Table 2: Average percent gap closed by num disj terms\n",
    "3. Table 3: Summary statistics for time to solve instances with branch-and-bound\n",
    "\n",
    "We select instances that meet the following criteria:\n",
    "1. Belong to MIPLIB, NEOS, or COR@L\n",
    "2. IP optimal value is known\n",
    "3. ≤ 5000 variables and 5000 constraints (in presolved instance)\n",
    "4. The partial branch-and-bound tree with 64 leaves does not find an IP optimal solution\n",
    "5. The disjunctive lower bound is strictly less than the maximum objective value on any leaf node\n",
    "\n",
    "There are some instances for which we do not have data for all 6 partial tree sizes. We include these instances in most tables, except if we are showing how some statistic changes as the disjunction increases in size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Set variables, import whatever is needed, and read in data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global variables\n",
    "EPS = 1e-7\n",
    "GAP_DIFF_EPS = 1e-3\n",
    "INFINITY = 1e+100\n",
    "MAX_TIME = 3600.\n",
    "\n",
    "## Set up variables containing relevant directories\n",
    "import os\n",
    "repos_key = 'REPOS_DIR'\n",
    "try:\n",
    "    REPOS_DIR = os.environ[repos_key]\n",
    "    print(\"REPOS_DIR set to \\\"%s\\\".\" % REPOS_DIR)\n",
    "    HOME_DIR = os.environ['HOME']\n",
    "    print(\"HOME_DIR set to \\\"%s\\\".\" % HOME_DIR)\n",
    "except KeyError:\n",
    "    print(\"*** ERROR: %s not found!\" % repos_key)\n",
    "\n",
    "VPC_DIR = REPOS_DIR + \"/vpc/\"\n",
    "RESULTS_DIR = VPC_DIR + \"results/saved/\"\n",
    "# RESULTS_DIR = HOME_DIR + '/' + \"results/saved/\"\n",
    "DATA_DIR = VPC_DIR + \"data/\"\n",
    "\n",
    "ONLY_PURE_BINARY = False\n",
    "ONLY_MIXED_BINARY = False\n",
    "\n",
    "# What to multiply by when an instance times out for nodes and time\n",
    "TIMEOUT_TIME_FACTOR = 2. # TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "TIMEOUT_NODE_FACTOR = 2. # TIMEOUT_NODE_FACTOR * # nodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data processing, plotting, and export packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data processing, plotting, and export packages and functions\n",
    "from IPython.display import display\n",
    "\n",
    "from plots_helper import * # this includes matplotlib (+ params), pandas, and custom LaTeX helper functions\n",
    "\n",
    "import re # regular expressions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `initialize_df`: common way to process each data frame that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common way to process each data frame that we need\n",
    "def initialize_df(filename):\n",
    "    \"\"\"\n",
    "    Create a multilevel index df out of data from file `filename`.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=',', index_col=False, skiprows=1)\n",
    "    df.sort_values(by = ['INSTANCE','disj_terms'], inplace=True)\n",
    "    df.set_index(['INSTANCE','disj_terms'], inplace=True)\n",
    "    df.replace({\"\\'-inf\\'\": -np.inf, \"\\'inf\\'\": np.inf}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column name definitions\n",
    "\n",
    "(It is possible / likely that we are not using all of these...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of stats reported\n",
    "pct_gap_closed_stub = \"% GAP CLOSED\"\n",
    "bound_stub = \"BOUND\"\n",
    "obj_stub = \"OBJ\"\n",
    "time_stub = \"TIME\"\n",
    "time_w_cuts_stub = time_stub + \" \" + \"W/CUTGEN\"\n",
    "nodes_stub = \"NODES\"\n",
    "\n",
    "# Types of solvers\n",
    "solver_stubs = [ \"REF\", \"REF+V\" ]\n",
    "solver_stat_list = [ \"FIRST\", \"AVG\", \"BEST\", \"MIN\", \"MAX\" ]\n",
    "\n",
    "# Chosen statistics for computing with reference solver\n",
    "REF_TYPE = 'AVG'\n",
    "REFV_TYPE = 'AVG'\n",
    "selected_stat_list = [ REF_TYPE, REFV_TYPE ]\n",
    "\n",
    "# Gap closed cut columns\n",
    "reg_cut_type_long_list = [ \"GMIC\", \"ROOT\", \"BEST DISJ\", \"VPC\", \"MAX(GMIC,VPC)\", \"VPC+GMIC\" ]\n",
    "reg_cut_type_short_list = [ \"G\", \"R\", \"DB\", \"V\", \"max(G,V)\", \"V+G\" ]\n",
    "\n",
    "solver_cut_type_stubs = [ \"FIRST_CUT_PASS\", \"LAST_CUT_PASS\" ]\n",
    "solver_type = solver_stubs[0]\n",
    "ref_cut_type_long_list    = [ solver_type + ' ' + cut_type for cut_type in solver_cut_type_stubs ]\n",
    "ref_cut_type_short_list   = [ \"GurF\", \"GurL\" ]\n",
    "solver_type = solver_stubs[1]\n",
    "refv_cut_type_long_list   = [ solver_type + ' ' + cut_type for cut_type in solver_cut_type_stubs ]\n",
    "refv_cut_type_short_list  = [ \"V+GurF\", \"V+GurL\" ]\n",
    "\n",
    "solver_cut_type_long_list  = [ [solver_type + ' ' + cut_type for cut_type in solver_cut_type_stubs] for solver_type in solver_stubs ]\n",
    "solver_cut_type_short_list = [ ref_cut_type_short_list, refv_cut_type_short_list ]\n",
    "\n",
    "col_num_passes_stub = \"NUM PASSES\"\n",
    "\n",
    "# Objective value columns\n",
    "col_lp_obj = 'LP OBJ'\n",
    "col_ip_obj = 'IP OBJ'\n",
    "col_best_disj_stub = 'BEST DISJ'\n",
    "col_best_disj_obj = col_best_disj_stub + ' ' + obj_stub\n",
    "col_best_disj_gap = col_best_disj_stub + ' ' + pct_gap_closed_stub\n",
    "col_worst_disj_stub = 'WORST DISJ'\n",
    "col_worst_disj_obj = col_worst_disj_stub + ' ' + obj_stub\n",
    "col_worst_disj_gap = col_worst_disj_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "obj_val_col_list = \\\n",
    "  [ col_lp_obj, col_ip_obj, col_worst_disj_obj ] \\\n",
    "  + \\\n",
    "  [ cut_type + ' ' + obj_stub for cut_type in reg_cut_type_long_list if cut_type != \"MAX(GMIC,VPC)\" ] \\\n",
    "  + \\\n",
    "  [ stat_type + ' ' + solver_type + ' ' + cut_type\n",
    "      for cut_type in solver_cut_type_stubs\n",
    "      for stat_type in solver_stat_list[:3]\n",
    "      for solver_type in solver_stubs\n",
    "  ]\n",
    "\n",
    "# Time closed columns\n",
    "time_type_list = [ \"TIME\", \"NODES\" ]\n",
    "\n",
    "# Generation time\n",
    "col_vpc_gen_time = 'VPC_GEN_TIME'\n",
    "\n",
    "# Number rows/columns/cuts\n",
    "col_num_rows        = 'ROWS'\n",
    "col_num_cols        = 'COLS'\n",
    "col_num_vpc         = 'NUM VPC'\n",
    "col_num_gmic        = 'NUM GMIC'\n",
    "col_num_disj_terms  = 'NUM DISJ TERMS'\n",
    "col_num_obj         = 'NUM OBJ'\n",
    "col_num_fails       = 'NUM FAILS'\n",
    "\n",
    "# Instance information\n",
    "col_binary          = 'BINARY'\n",
    "col_integer         = 'INTEGER'\n",
    "col_continuous      = 'CONTINUOUS'\n",
    "col_gen_int         = 'GEN INT'\n",
    "col_pure_binary     = 'IS PURE BINARY'\n",
    "col_mixed_binary    = 'IS MIXED BINARY'\n",
    "\n",
    "# Run information\n",
    "col_exit_reason     = 'ExitReason'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gap column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gap column names\n",
    "\n",
    "# These will be calculated from the data\n",
    "col_gmic            = 'GMIC' + ' ' + pct_gap_closed_stub\n",
    "col_root            = 'ROOT' + ' ' + pct_gap_closed_stub\n",
    "col_best_disj       = 'BEST DISJ' + ' ' + pct_gap_closed_stub\n",
    "col_vpc             = 'VPC' + ' ' + pct_gap_closed_stub\n",
    "col_max_gmic_vpc    = 'MAX(GMIC,VPC)' + ' ' + pct_gap_closed_stub\n",
    "col_vpc_gmic        = 'VPC+GMIC' + ' ' + pct_gap_closed_stub\n",
    "\n",
    "col_best_disj_vpc                 = 'BEST VPC DISJ'\n",
    "col_best_disj_gmic_vpc            = 'BEST GMIC+VPC DISJ'\n",
    "col_best_disj_refv_first_cut_pass = 'Best ' + refv_cut_type_short_list[0] + ' DISJ'\n",
    "col_best_disj_refv_last_cut_pass  = 'Best ' + refv_cut_type_short_list[1] + ' DISJ'\n",
    "\n",
    "## First cut pass\n",
    "col_stub = solver_cut_type_stubs[0]\n",
    "col_first_cut_pass_stubs = [ \n",
    "    stat_type + ' ' + solver_type + ' ' + col_stub \n",
    "    for stat_type in solver_stat_list[:3]\n",
    "    for solver_type in solver_stubs \n",
    "]\n",
    "#col_refv_first_cut_pass_stubs = [ stat_type + ' ' + 'REF+V FIRST_CUT_PASS' for stat_type in ref_solver_stat_list ]\n",
    "\n",
    "# ref\n",
    "solver_type = solver_stubs[0]\n",
    "col_first_cut_pass_bound_ref_first = 'FIRST' + ' ' + solver_type + ' ' + col_stub\n",
    "col_first_cut_pass_bound_ref_avg   = 'AVG' + ' ' + solver_type + ' ' + col_stub\n",
    "col_first_cut_pass_bound_ref_best  = 'BEST' + ' ' + solver_type + ' ' + col_stub\n",
    "col_first_cut_pass_gap_ref_first   = 'FIRST' + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "col_first_cut_pass_gap_ref_avg     = 'AVG' + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "col_first_cut_pass_gap_ref_best    = 'BEST' + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "col_first_cut_pass_bound_ref       = REF_TYPE + ' ' + solver_type + ' ' + col_stub\n",
    "col_first_cut_pass_gap_ref         = REF_TYPE + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "# refv\n",
    "solver_type = solver_stubs[1]\n",
    "col_first_cut_pass_bound_ref_v     = REFV_TYPE + ' ' + solver_type + ' ' + col_stub\n",
    "col_first_cut_pass_gap_ref_v       = REFV_TYPE + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "## Last cut pass\n",
    "col_stub = solver_cut_type_stubs[1]\n",
    "col_last_cut_pass_stubs = [ \n",
    "    stat_type + ' ' + solver_type + ' ' + col_stub\n",
    "    for stat_type in solver_stat_list[:3]\n",
    "    for solver_type in solver_stubs \n",
    "]\n",
    "\n",
    "# col_ref_last_cut_pass_stubs = [ stat_type + ' ' + 'REF LAST_CUT_PASS' for stat_type in ref_solver_stat_list ]\n",
    "# col_refv_last_cut_pass_stubs = [ stat_type + ' ' + 'REF+V LAST_CUT_PASS' for stat_type in ref_solver_stat_list ]\n",
    "\n",
    "# ref\n",
    "solver_type = solver_stubs[0]\n",
    "col_last_cut_pass_bound_ref_first  = 'FIRST' + ' ' + solver_type + ' ' + col_stub\n",
    "col_last_cut_pass_bound_ref_avg    = 'AVG' + ' ' + solver_type + ' ' + col_stub\n",
    "col_last_cut_pass_bound_ref_best   = 'BEST' + ' ' + solver_type + ' ' + col_stub\n",
    "col_last_cut_pass_gap_ref_first    = 'FIRST' + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "col_last_cut_pass_gap_ref_avg      = 'AVG' + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "col_last_cut_pass_gap_ref_best     = 'BEST' + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "col_last_cut_pass_bound_ref        = REF_TYPE + ' ' + solver_type + ' ' + col_stub\n",
    "col_last_cut_pass_gap_ref          = REF_TYPE + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "# refv\n",
    "solver_type = solver_stubs[1]\n",
    "col_last_cut_pass_bound_ref_v      = REFV_TYPE + ' ' + solver_type + ' ' + col_stub\n",
    "col_last_cut_pass_gap_ref_v        = REFV_TYPE + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "\n",
    "## Set of all gap cols\n",
    "gap_cols = \\\n",
    "    [ col_stub + ' ' + pct_gap_closed_stub for col_stub in reg_cut_type_long_list] \\\n",
    "    + \\\n",
    "    [ col_stub + ' ' + pct_gap_closed_stub for col_stub in col_first_cut_pass_stubs] \\\n",
    "    + \\\n",
    "    [ col_stub + ' ' + pct_gap_closed_stub for col_stub in col_last_cut_pass_stubs]\n",
    "\n",
    "solver_type = solver_stubs[0]\n",
    "ref_solver_gap_cols = [\n",
    "    stat_type + ' ' + solver_type + ' ' + col_stub + ' ' + pct_gap_closed_stub\n",
    "    for stat_type in solver_stat_list[:3]\n",
    "    for col_stub in solver_cut_type_stubs\n",
    "]\n",
    "# refv_solver_gap_cols = [\n",
    "#     col_first_cut_pass_gap_ref_v,\n",
    "#     col_last_cut_pass_gap_ref_v,\n",
    "# ]\n",
    "\n",
    "# Create maps between short and long names for gap columns\n",
    "map_short_to_cols_gap = {\n",
    "    short_stub : long_stub + ' ' + pct_gap_closed_stub for short_stub, long_stub in zip(reg_cut_type_short_list, reg_cut_type_long_list)\n",
    "}\n",
    "# Add ref and refv for first/last cut pass\n",
    "for solver_ind in range(len(solver_stubs)):\n",
    "    for cut_type_ind in range(len(solver_cut_type_stubs)):\n",
    "        short_stub = solver_cut_type_short_list[cut_type_ind][solver_ind]\n",
    "        long_stub = selected_stat_list[solver_ind] + ' ' + solver_cut_type_long_list[cut_type_ind][solver_ind] + ' ' + pct_gap_closed_stub\n",
    "        map_short_to_cols_gap[short_stub] = long_stub\n",
    "        \n",
    "map_cols_to_short_gap = {v: k for k, v in map_short_to_cols_gap.items()}\n",
    "\n",
    "gap_cols_short = list(map_short_to_cols_gap.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time column names\n",
    "\n",
    "solver_ind = 0\n",
    "solver_type = solver_stubs[solver_ind]\n",
    "ref_time_stub   = solver_type + ' ' + time_stub\n",
    "ref_nodes_stub  = solver_type + ' ' + nodes_stub\n",
    "ref_time_col    = selected_stat_list[solver_ind] + ' ' + ref_time_stub\n",
    "ref_nodes_col   = selected_stat_list[solver_ind] + ' ' + ref_nodes_stub\n",
    "\n",
    "solver_ind = 1\n",
    "solver_type = solver_stubs[solver_ind]\n",
    "refv_time_stub        = solver_type + ' ' + time_stub\n",
    "refv_w_cut_time_stub  = solver_type + ' ' + 'W/CUTGEN' + ' ' + time_stub\n",
    "refv_nodes_stub       = solver_type + ' ' + nodes_stub\n",
    "refv_time_col         = selected_stat_list[solver_ind] + ' ' + refv_time_stub\n",
    "refv_w_cut_time_col   = selected_stat_list[solver_ind] + ' ' + refv_w_cut_time_stub\n",
    "refv_nodes_col        = selected_stat_list[solver_ind] + ' ' + refv_nodes_stub\n",
    "\n",
    "map_cols_to_short_time = {\n",
    "    ref_time_col          : 'Gur',\n",
    "    refv_time_col         : 'V',\n",
    "    col_vpc_gen_time      : 'Gen',\n",
    "}\n",
    "\n",
    "map_cols_to_short_nodes = {\n",
    "    ref_nodes_col        : 'Gur',\n",
    "    refv_nodes_col       : 'V',\n",
    "}\n",
    "\n",
    "map_short_to_cols_time = {v: k for k, v in map_cols_to_short_time.items()}\n",
    "map_short_to_cols_nodes = {v: k for k, v in map_cols_to_short_nodes.items()}\n",
    "\n",
    "time_cols_short = list(map_short_to_cols_time.keys())\n",
    "node_cols_short = list(map_short_to_cols_nodes.keys())\n",
    "# display(time_cols, node_cols)\n",
    "\n",
    "# Select a subset of columns for the \"long\" list used when updating the 0-row\n",
    "time_cols_long = [map_short_to_cols_time[col] for col in time_cols_short]\n",
    "node_cols_long = [map_short_to_cols_nodes[col] for col in node_cols_short]\n",
    "\n",
    "# How to handle timeouts (do we take average across the seeds, do we take the best, do we account for the timeout multiplicative factor?)\n",
    "#STAT_FOR_TIMEOUT = 'BEST'\n",
    "STAT_FOR_TIMEOUT = 'AVG'\n",
    "ref_timeout_col = STAT_FOR_TIMEOUT + ' ' + ref_time_stub\n",
    "refv_timeout_col = STAT_FOR_TIMEOUT + ' ' + refv_time_stub\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_ipopt`: Retrieve best known IP objective values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best known IP objective values\n",
    "df_ipopt = pd.read_csv(DATA_DIR + \"ip_obj.csv\")\n",
    "df_ipopt = df_ipopt.set_index(df_ipopt[df_ipopt.columns[0]])\n",
    "# df_ipopt.rename(columns = {'IP_OBJ' : col_ip_obj}, inplace=True) # for consistency with other dfs\n",
    "# df_ipopt.rename(columns = {'IP Objective' : col_ip_obj}, inplace=True) # for consistency with other dfs\n",
    "df_ipopt = df_ipopt[~df_ipopt.index.duplicated()]\n",
    "display(df_ipopt.head())\n",
    "display(df_ipopt[col_ip_obj]['bm23_presolved'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_preprocess`: Results from preprocessing instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results from preprocessing instances\n",
    "df_preprocess = pd.read_csv(RESULTS_DIR + \"vpc-preprocess.csv\", sep=',', index_col=False, skiprows=1)\n",
    "df_preprocess = df_preprocess.set_index(df_preprocess[df_preprocess.columns[0]])\n",
    "display(df_preprocess.head())\n",
    "\n",
    "col_cleaned_lp_obj = 'CLEANED LP OBJ'\n",
    "display(df_preprocess.loc['bm23',col_cleaned_lp_obj])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_bb`: Results from generating VPCs for various number of disjunctive terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results from generating VPCs for various number of disjunctive terms\n",
    "df_bb = initialize_df(RESULTS_DIR + \"vpc-bb0bb.csv\")\n",
    "display(df_bb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_disjset`: Read in results from using cuts across all 6 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disjset = initialize_df(RESULTS_DIR + \"vpc-disjset.csv\")\n",
    "\n",
    "# Set disj_terms index to be -1 for all instances\n",
    "df_disjset.index = df_disjset.index.set_levels([-1], level='disj_terms')\n",
    "\n",
    "display(df_disjset.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df`: Append to `df_bb` results from running baseline solver 7 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append results from running baseline solver 7 times\n",
    "#df = df_bb.append(initialize_df(RESULTS_DIR + \"vpc-bb0.csv\")) # deprecated\n",
    "df = pd.concat([df_disjset, df_bb, initialize_df(RESULTS_DIR + \"vpc-bb0.csv\")])\n",
    "df.sort_values(by = ['INSTANCE','disj_terms'], inplace=True)\n",
    "\n",
    "col_list = [col_best_disj_obj, col_worst_disj_obj]\n",
    "for col in col_list:\n",
    "    df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "# Create new column for number of disjunctive terms since original one is now index\n",
    "df[col_num_disj_terms] = df.index.get_level_values(1)\n",
    "\n",
    "## Identify pure binary instances, which are those where 'CLEANED BINARY' column equals 'CLEANED COLS'\n",
    "df[col_pure_binary] = (df[col_binary] == df[col_num_cols])\n",
    "\n",
    "## Identify mixed binary instances, which are those where 'CLEANED GEN INT' column = 0\n",
    "df[col_mixed_binary] = (df[col_gen_int] == 0)\n",
    "\n",
    "# col_list = [col_num_disj_terms]\n",
    "# for col in col_list:\n",
    "#     df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "# start = 220\n",
    "# end = start + 15\n",
    "# print(df.columns[start:end])\n",
    "# print(df.dtypes[start:end])\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the count of each instance-disj_terms pair\n",
    "# counts = df.groupby(level=[0]).size()\n",
    "\n",
    "# # get the instances that have only one occurrence\n",
    "# instances_with_one_occurrence = counts[counts == 1].index.get_level_values(0).unique()\n",
    "\n",
    "# # filter df_bb to only include instances with one occurrence\n",
    "# df_bb_one_occurrence = df.loc[instances_with_one_occurrence]\n",
    "\n",
    "# # display the filtered dataframe\n",
    "# display(df_bb_one_occurrence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove `stein*` instances (keep modified `stein*_nocard` instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unmodified stein instances from consideration\n",
    "df.drop(index = ['stein09_presolved', 'stein15_presolved', 'stein27_presolved', 'stein45_presolved'], inplace=True)\n",
    "df.index = df.index.remove_unused_levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify `mas` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is done to sort correctly with mas284\n",
    "df_preprocess.rename(index={'mas74': 'mas074', 'mas76': 'mas076'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute average running time to account for timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_df in [df, df_disjset]:\n",
    "  for solver in solver_stubs:\n",
    "    # Split values in 'ALL REF TIME' into new columns for 'REF TIME (SEED)' where 'SEED' takes values 628 * [1,2,3,4,5,6,7]\n",
    "    df_timing = curr_df['ALL '+solver+' TIME'].str.split(';', expand=True)\n",
    "    df_timing.columns = [solver+' TIME (%d)' % (i+1) for i in range(df_timing.shape[1])]\n",
    "    df_timing = df_timing.astype(float)\n",
    "    df_timing_cols = df_timing.columns[df_timing.columns.str.contains(re.escape(solver)+r' TIME (.+)')]\n",
    "\n",
    "    # Do the same for nodes\n",
    "    df_nodes = curr_df['ALL '+solver+' NODES'].str.split(';', expand=True)\n",
    "    df_nodes.columns = [solver+' NODES (%d)' % (i+1) for i in range(df_nodes.shape[1])]\n",
    "    df_nodes = df_nodes.astype(float)\n",
    "    df_nodes_cols = df_nodes.columns[df_nodes.columns.str.contains(re.escape(solver)+r' NODES (.+)')]\n",
    "\n",
    "    # Select entries in which the max time is greater than MAX_TIME\n",
    "    selected_entries = df_timing > (MAX_TIME - EPS)\n",
    "\n",
    "    # Add min and max of 'REF TIME (SEED)' columns\n",
    "    curr_df[solver+' TIME MIN'] = df_timing[df_timing_cols].min(axis=1)\n",
    "    curr_df[solver+' TIME MAX'] = df_timing[df_timing_cols].max(axis=1)\n",
    "\n",
    "    # Find average of 'REF TIME (SEED)' columns after adjusting for timeout\n",
    "    df_timing[selected_entries] = TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "    # df_timing[solver+' TIME AVG'] = df_timing.mean(axis=1)\n",
    "    curr_df['AVG '+solver+' TIME'] = df_timing[df_timing_cols].mean(axis=1)\n",
    "\n",
    "    ## Repeat for nodes\n",
    "    # Add min and max of 'REF NODES (SEED)' columns\n",
    "    curr_df[solver+' NODES MIN'] = df_nodes[df_nodes_cols].min(axis=1)\n",
    "    curr_df[solver+' NODES MAX'] = df_nodes[df_nodes_cols].max(axis=1)\n",
    "\n",
    "    # Find average of 'REF NODES (SEED)' columns after adjusting for timeout\n",
    "    selected_entries.columns = df_nodes_cols\n",
    "    df_nodes[selected_entries] *= TIMEOUT_NODE_FACTOR\n",
    "    # df_nodes[solver+' NODES AVG'] = df_nodes[df_nodes_cols].mean(axis=1)\n",
    "    curr_df['AVG '+solver+' NODES'] = df_nodes[df_nodes_cols].mean(axis=1)\n",
    "\n",
    "    # Append df_timing to df\n",
    "    curr_df = pd.concat([curr_df, df_timing], axis=1)\n",
    "\n",
    "    # Append df_nodes to df\n",
    "    curr_df = pd.concat([curr_df, df_nodes], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add col for avg b&b time counting cut generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add total time for running solver + generating cuts\n",
    "df[refv_w_cut_time_col] = df[refv_time_col] + df[col_vpc_gen_time]\n",
    "\n",
    "inst_set = ['a1c1s1_presolved']\n",
    "\n",
    "# display(df.loc['30n20b8_presolved'][[ref_time_col,refv_time_col,col_vpc_gen_time,refv_w_cut_time_col]])\n",
    "# display(df.loc['binkar10_1_presolved'][[ref_time_col,refv_time_col,col_vpc_gen_time,refv_w_cut_time_col]])\n",
    "display(df.loc[inst_set][[col_num_vpc, ref_time_col,refv_time_col,col_vpc_gen_time,refv_w_cut_time_col]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Disabled; code fixed in commit 4ed946c) Fix mistake in code for one root pass containing wrong bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prefix in solver_stat_list:\n",
    "#     inst_set = df[prefix + \" REF+V ROOT_PASSES\"] == 1\n",
    "#     tmp_df = df[inst_set]\n",
    "#     tmp_df = tmp_df[[col_lp_obj,\n",
    "#                      prefix + \" REF+V BOUND\", \n",
    "#                      prefix + \" REF+V FIRST_CUT_PASS\",\n",
    "#                      prefix + \" REF+V LAST_CUT_PASS\"]]\n",
    "#     tmp_df.tail(30)\n",
    "\n",
    "#     tmp_tmp_df = tmp_df[\"LP OBJ\"] - tmp_df[prefix + \" REF+V FIRST_CUT_PASS\"]\n",
    "#     assert(tmp_tmp_df.max() < EPS)\n",
    "\n",
    "#     refcol = prefix + \" REF+V BOUND\"\n",
    "#     col = prefix + \" REF+V FIRST_CUT_PASS\"\n",
    "#     df.loc[inst_set,col] = tmp_df[refcol].values\n",
    "#     col = prefix + \" REF+V LAST_CUT_PASS\"\n",
    "#     df.loc[inst_set,col] = tmp_df[refcol].values\n",
    "\n",
    "\n",
    "# df.loc  [[\"misc02_presolved\",'cap6000_presolved'],\n",
    "#          [\"FIRST REF+V BOUND\",\n",
    "#           \"FIRST REF+V FIRST_CUT_PASS\",\n",
    "#           \"FIRST REF+V LAST_CUT_PASS\"\n",
    "#          ]\n",
    "#          +\n",
    "#          [\"BEST REF+V BOUND\",\n",
    "#           \"BEST REF+V FIRST_CUT_PASS\",\n",
    "#           \"BEST REF+V LAST_CUT_PASS\"\n",
    "#          ]\n",
    "#         ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `instances`: get unique instance list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique instance list\n",
    "if (ONLY_PURE_BINARY):\n",
    "    # Select only instances in df in which column ['IS PURE BINARY'] is True\n",
    "    tmp_df = df[df['IS PURE BINARY'] == True]\n",
    "    tmp_df.index = tmp_df.index.remove_unused_levels()\n",
    "    instances = tmp_df.index.levels[0]\n",
    "elif (ONLY_MIXED_BINARY):\n",
    "    tmp_df = df[df['IS MIXED BINARY'] == True]\n",
    "    tmp_df.index = tmp_df.index.remove_unused_levels()\n",
    "    instances = tmp_df.index.levels[0]\n",
    "else:\n",
    "    instances = df.index.levels[0]\n",
    "\n",
    "instances.set_names(names = 'Instance', inplace=True)\n",
    "\n",
    "pure_binary_instances = df[df[col_pure_binary] == True].index.get_level_values(0).unique().to_list()\n",
    "\n",
    "print(\"Number of selected instances: \", len(instances))\n",
    "print(\"Number of binary instances: {:d}/{:d}\".format(len(pure_binary_instances), len(instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write instances to file\n",
    "with open('./' + \"instances.txt\", \"w\") as f:\n",
    "    for inst in instances:\n",
    "        f.write(inst + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_rejection_reason`: Track why instances were not selected for our statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_reasons = [\n",
    "    'SELECTED_GAP', # *not* rejected for gap experiments; _must_ be first column\n",
    "    'SELECTED_TIME', # *not* rejected for time experiments; _must_ be second column\n",
    "    'SELECTED_6TREES', # *not* rejected for 6trees set for time experiments; _must_ be third column\n",
    "    'NUM_WITH_OBJS', # number of attempts that successfully tried solving the PRLP\n",
    "    'NUM_WITH_CUTS', # number of attempts that successfully yielded cuts\n",
    "    'IP_OPT_UNKNOWN', # ip opt val must be known\n",
    "    'TOO_MANY_ROWS_OR_COLS', # require max(nrows, ncols) ≤ 5K\n",
    "    'OPTIMAL_SOLUTION_FOUND', # optimal solution should not be found by any of the partial trees\n",
    "    'LP_OPT_IS_NOT_CUT', # check if lp opt < ip opt\n",
    "    'DLB=DUB', # check if disj lb < disj ub\n",
    "    'LP=DLB=DUB', # require either lp opt < disj lb or disj lb < disj ub\n",
    "    'PRLP_INFEASIBLE', # require PRLP is feasible and solves within timelimit for at least one of the attempts\n",
    "    'PRLP_TIME_LIMIT', # require PRLP solves within timelimit for at least one of the attempts\n",
    "    'NO_CUTS', # there must be cuts from at least one of the partial b&b trees\n",
    "    'NO_GAP', # require that ip opt != lp opt\n",
    "    'GUR_TIMEOUT', # require Gur7 < 3600 (Gurobi is able to solve the instance to optimality within an hour either with or without using VPCs)'\n",
    "    '<7_ATTEMPTS', # indicates not all partial trees were successfully run\n",
    "]\n",
    "df_rejection_reason = pd.DataFrame(index = instances, columns = rejection_reasons, dtype=bool)\n",
    "df_rejection_reason.iloc[:,3:] = False # no rejection criteria at true\n",
    "\n",
    "for col in ['OPTIMAL_SOLUTION_FOUND']:\n",
    "    df_rejection_reason[col] = df_rejection_reason[col].astype(np.int64)\n",
    "for col in ['NUM_WITH_OBJS', 'NUM_WITH_CUTS', 'LP_OPT_IS_NOT_CUT', 'DLB=DUB', 'LP=DLB=DUB', 'PRLP_INFEASIBLE', 'PRLP_TIME_LIMIT', 'GUR_TIMEOUT']:\n",
    "    df_rejection_reason[col] = df_rejection_reason[col].astype(np.int8)\n",
    "display(df_rejection_reason.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `map_rejection_reason_to_number`: Reference paper's rejection criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_rejection_reason_to_number = {\n",
    "#     'OPTIMAL_SOLUTION_FOUND':   '(3)',\n",
    "#     'LP=DLB=DUB':               '(4a)',\n",
    "#     'PRLP_INFEASIBLE':          '(4b)',\n",
    "#     'PRLP_TIME_LIMIT':          '(4c)',\n",
    "#     '<7_ATTEMPTS':              '(?)',\n",
    "# }\n",
    "map_rejection_reason_to_number = {\n",
    "    'IP_OPT_UNKNOWN':           '\\\\ref{selection-criterion:ip-opt-known}',\n",
    "    'NO_GAP':                   '\\\\ref{selection-criterion:ip-opt-known}',\n",
    "    'TOO_MANY_ROWS_OR_COLS':    '\\\\ref{selection-criterion:max-instance-size}',\n",
    "    'OPTIMAL_SOLUTION_FOUND':   '\\\\ref{selection-criterion:partial-tree-does-not-find-opt}',\n",
    "    'LP=DLB=DUB':               '\\\\ref{selection-criterion:cuts-are-generated:not_lp=dlb=dub}',\n",
    "    'PRLP_INFEASIBLE':          '\\\\ref{selection-criterion:cuts-are-generated:PRLP-primal-feasible}',\n",
    "    'PRLP_TIME_LIMIT':          '\\\\ref{selection-criterion:cuts-are-generated:PRLP-time-limit}',\n",
    "    'NO_CUTS':                  '\\\\ref{selection-criterion:cuts-are-generated:cuts-are-generated}',\n",
    "    'GUR_TIMEOUT':              'G',\n",
    "    '<7_ATTEMPTS':              '?',\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_status_by_depth`: Track success or failure reason by depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2, 4, 8, 16, 32, 64]\n",
    "df_status_by_depth = pd.DataFrame(index = instances, columns = sizes, dtype=str)\n",
    "\n",
    "DEFAULT_STATUS = map_rejection_reason_to_number['<7_ATTEMPTS']\n",
    "\n",
    "df_status_by_depth[:] = DEFAULT_STATUS\n",
    "\n",
    "display(df_status_by_depth.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = \"REF+V FIRST_CUT_PASS\"\n",
    "# tmp = df[col]\n",
    "# display(tmp)\n",
    "\n",
    "# for col in df.columns:\n",
    "#     if str(col).endswith(\"FIRST_CUT_PASS\"):\n",
    "#         print(\"{}\".format(col))\n",
    "\n",
    "# inst = 'neos22_presolved'\n",
    "# col = 'NUM DISJ TERMS'\n",
    "# df.loc[inst][col]\n",
    "\n",
    "# display(df.loc[('bppc4-08_presolved',2)]['LP OBJ'])\n",
    "# display(df.loc[('bppc4-08_presolved',2)]['BEST DISJ OBJ'])\n",
    "# display(df.loc[('bppc4-08_presolved',2)]['WORST DISJ OBJ'])\n",
    "# display(df['BEST DISJ OBJ'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Select instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selected_gap_instances_dict` (original index, instance): Select instances for gap closed calculations\n",
    "\n",
    "Criteria to filter gap closed instances:\n",
    "* ip opt val is known\n",
    "* lp opt < ip opt\n",
    "* max(nrows, ncols) ≤ 5K\n",
    "* optimal solution should not be found by any of the partial trees\n",
    "* either lp opt < disj lb or disj lb < disj ub\n",
    "* PRLP is feasible and solves within timelimit for at least one of the attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select instances for gap closed calculations\n",
    "#\n",
    "# Criteria to filter gap closed instances:\n",
    "# * ip opt val is known\n",
    "# * lp opt < ip opt\n",
    "# * max(nrows, ncols) ≤ 5K\n",
    "# * optimal solution should not be found by any of the partial trees\n",
    "# * either lp opt < disj lb or disj lb < disj ub\n",
    "# * PRLP is feasible and solves within timelimit for at least one of the attempts\n",
    "\n",
    "# Constants\n",
    "MAX_ROWS = 5000\n",
    "MAX_COLS = MAX_ROWS\n",
    "PRINT_SKIP_REASON = False\n",
    "\n",
    "# Information to save\n",
    "selected_gap_instances_dict = {} # dictionary of (original index, instance)\n",
    "#selected_indices = []\n",
    "num_gap_errors = 0\n",
    "\n",
    "inst_set = instances\n",
    "num_attempts = np.zeros(len(inst_set), dtype=int)\n",
    "\n",
    "for i, inst in enumerate(inst_set):\n",
    "    if inst == 'cod105_presolved':\n",
    "        PRINT_SKIP_REASON = True\n",
    "    else:\n",
    "        PRINT_SKIP_REASON = False\n",
    "    print(\"{}/{}\".format(i+1,len(inst_set)), end='\\r', flush=True)\n",
    "    skip_instance = False\n",
    "    curr_df = df.loc[inst]\n",
    "    curr_df = curr_df[curr_df.index.get_level_values(0) >= 0]\n",
    "    \n",
    "    # Count number of times instance appears\n",
    "    num_attempts[i] = len(curr_df)\n",
    "\n",
    "    if num_attempts[i] < 7:\n",
    "        print(\"*** ERROR: Instance {:d} -- {}: {:d} < 7 attempts.\".format(i, inst, num_attempts[i]))\n",
    "        skip_instance = True\n",
    "        num_gap_errors += 1\n",
    "        df_rejection_reason.loc[inst, '<7_ATTEMPTS'] = True\n",
    "\n",
    "    # Check that LP opt < IP opt\n",
    "    lp_obj = np.float64(df_preprocess.loc[remove_presolved_from_name(inst),col_cleaned_lp_obj])\n",
    "    ip_obj = np.float64(df_ipopt.loc[inst,col_ip_obj])\n",
    "    YES_GAP = (ip_obj - lp_obj) >= EPS\n",
    "    if not YES_GAP:\n",
    "        print(\"*** ERROR: Instance {:d} -- {}: not YES GAP (lp = {:.10f}; ip = {:.10f}, diff = {:.2f})\".format(i, inst, lp_obj, ip_obj, ip_obj-lp_obj))\n",
    "        skip_instance = True\n",
    "        num_gap_errors += 1\n",
    "        df_rejection_reason.loc[inst, 'NO_GAP'] = True\n",
    "        \n",
    "    # Check that ExitReason != OPTIMAL_SOLUTION_FOUND\n",
    "    OPT_SOL_FOUND = False\n",
    "    for curr_index, row in curr_df.iterrows():\n",
    "        #print(i,j, curr_df['ExitReason'])\n",
    "        curr_depth = int(curr_index)\n",
    "        if curr_depth <= 0:\n",
    "            continue\n",
    "        exitreason = row[col_exit_reason]\n",
    "        if exitreason == 'OPTIMAL_SOLUTION_FOUND' and not OPT_SOL_FOUND:\n",
    "            if PRINT_SKIP_REASON:\n",
    "                print(\"Skipping instance {:d} -- {}: optimal IP solution found at depth {:d}.\".format(\n",
    "                    i, inst, curr_depth\n",
    "                ))\n",
    "            skip_instance = True\n",
    "            OPT_SOL_FOUND = True\n",
    "            df_rejection_reason.loc[inst, 'OPTIMAL_SOLUTION_FOUND'] = curr_depth\n",
    "        if OPT_SOL_FOUND:\n",
    "            df_status_by_depth.loc[inst, curr_depth] = map_rejection_reason_to_number['OPTIMAL_SOLUTION_FOUND']\n",
    "        else:\n",
    "            df_status_by_depth.loc[inst, curr_depth] = ''\n",
    "\n",
    "    # Check that best and worst bound on leaf nodes is not same (likely cause of primal infeasible PRLP)\n",
    "    num_successful_attempts = 0\n",
    "    has_zero = False # could check with \"0 in curr_df['NUM DISJ TERMS'], but that would implicitly add an extra (short) loop per instance\"\n",
    "    terms = curr_df.index\n",
    "    for curr_index in terms:\n",
    "        if curr_df[col_num_disj_terms][curr_index] == 0:\n",
    "            has_zero = True\n",
    "            continue\n",
    "            \n",
    "        curr_lp_obj = curr_df[col_lp_obj][curr_index]\n",
    "        curr_ip_obj = curr_df[col_ip_obj][curr_index]\n",
    "        best_disj_obj = curr_df[col_best_disj_obj][curr_index]\n",
    "        worst_disj_obj = curr_df[col_worst_disj_obj][curr_index]\n",
    "        num_frac = curr_df['NUM FRAC'][curr_index]\n",
    "        num_obj_tried = curr_df[col_num_obj][curr_index]\n",
    "        num_cuts = curr_df[col_num_vpc][curr_index] # can be > 0 even if num_obj_tried = 0, b/c of OPTIMAL_SOLUTION_FOUND exit reason\n",
    "        exitreason = curr_df[col_exit_reason][curr_index]\n",
    "\n",
    "        # Quick double check that LP and IP objectives are correct\n",
    "        if abs(curr_lp_obj - lp_obj) >= GAP_DIFF_EPS:\n",
    "            raise ValueError(\n",
    "                \"*** ERROR: Instance {:d} -- {}: at depth {:d}, curr lp obj {:.10f} != lp obj {:.10f}\".format(\n",
    "                    i, inst, curr_index, curr_lp_obj, lp_obj\n",
    "                )\n",
    "            )\n",
    "        if abs(curr_ip_obj - ip_obj) >= GAP_DIFF_EPS:\n",
    "            raise ValueError(\n",
    "                \"*** ERROR: Instance {:d} -- {}: at depth {:d}, curr ip obj {:.10f} != ip obj {:.10f}\".format(\n",
    "                    i, inst, curr_index, curr_ip_obj, ip_obj\n",
    "                )\n",
    "            )\n",
    "\n",
    "        curr_YES_GAP = (ip_obj - lp_obj) >= 1e-7\n",
    "        LP_OPT_IS_CUT = (num_frac > 0) and curr_YES_GAP and abs(lp_obj - worst_disj_obj) >= 1e-7\n",
    "        DLB_NE_DUB = (num_frac > 0) and abs(best_disj_obj - worst_disj_obj) >= 1e-7\n",
    "        df_rejection_reason.loc[inst, 'NO_GAP'] += (not curr_YES_GAP)\n",
    "        df_rejection_reason.loc[inst, 'LP_OPT_IS_NOT_CUT'] += (not LP_OPT_IS_CUT)\n",
    "        df_rejection_reason.loc[inst, 'DLB=DUB'] += (not DLB_NE_DUB)\n",
    "        df_rejection_reason.loc[inst, 'PRLP_INFEASIBLE'] += (exitreason == 'PRLP_INFEASIBLE')\n",
    "        df_rejection_reason.loc[inst, 'PRLP_TIME_LIMIT'] += (exitreason == 'PRLP_TIME_LIMIT')\n",
    "        # if not DLB_NE_DUB and num_obj_tried > 0:\n",
    "        #     raise ValueError(\n",
    "        #         \"*** ERROR: Instance {:d} -- {}: at depth {:d}, num obj tried = {:d} (num cuts = {:d}) but lp opj {:.10f}, best_disj_obj {:.10f} = worst_disj_obj {:.10f} with exit reason {}\".format(\n",
    "        #             i, inst, curr_index, num_obj_tried, num_cuts, lp_obj, best_disj_obj, worst_disj_obj, curr_df['ExitReason'][curr_index]\n",
    "        #         )\n",
    "        #     )\n",
    "        if LP_OPT_IS_CUT or DLB_NE_DUB:\n",
    "            if (num_obj_tried == 0) and (exitreason not in ['PRLP_TIME_LIMIT','PRLP_INFEASIBLE','OPTIMAL_SOLUTION_FOUND','TIME_LIMIT']):\n",
    "                # We should be trying objectives at this point, unless the initial PRLP timed out or was infeasible or an optimal solution was found\n",
    "                raise ValueError(\n",
    "                    \"*** ERROR: Instance {:d} -- {}: at depth {:d}, num obj tried = 0 but lp opj {:.10f} < best_disj_obj {:.10f} < worst_disj_obj {:.10f} with exit reason {}\".format(\n",
    "                        i, inst, curr_index, lp_obj, best_disj_obj, worst_disj_obj, curr_df['ExitReason'][curr_index]\n",
    "                    )\n",
    "                )\n",
    "            if num_obj_tried > 0:\n",
    "                df_rejection_reason.loc[inst, 'NUM_WITH_OBJS'] += 1\n",
    "                if num_cuts > 0:\n",
    "                    num_successful_attempts += 1\n",
    "                    df_rejection_reason.loc[inst, 'NUM_WITH_CUTS'] += 1\n",
    "                else:\n",
    "                    df_status_by_depth.loc[inst, curr_index] = map_rejection_reason_to_number['NO_CUTS']\n",
    "            elif exitreason == 'PRLP_INFEASIBLE':\n",
    "                df_status_by_depth.loc[inst, curr_index] = map_rejection_reason_to_number[exitreason]\n",
    "            elif exitreason == 'PRLP_TIME_LIMIT':\n",
    "                df_status_by_depth.loc[inst, curr_index] = map_rejection_reason_to_number[exitreason]\n",
    "        else:\n",
    "            # check that num obj tried is 0\n",
    "            if (num_obj_tried > 0):\n",
    "                raise ValueError(\n",
    "                    \"*** ERROR: Instance {:d} -- {}: at depth {:d}, num obj tried = {:d} > 0 but best_disj_obj {:f} = worst_disj_obj {:f}\".format(\n",
    "                        i, inst, curr_index, num_obj_tried, best_disj_obj, worst_disj_obj\n",
    "                    )\n",
    "                )\n",
    "            df_rejection_reason.loc[inst, 'LP=DLB=DUB'] += 1\n",
    "            df_status_by_depth.loc[inst, int(curr_index)] = map_rejection_reason_to_number['LP=DLB=DUB']\n",
    "\n",
    "    if not has_zero:\n",
    "        raise ValueError(\n",
    "            \"*** ERROR: Instance {:d} -- {}: has no bb0 entry.\".format(\n",
    "                i, inst, curr_index\n",
    "            )\n",
    "        )        \n",
    "    \n",
    "    if num_successful_attempts == 0 and not skip_instance:\n",
    "        if PRINT_SKIP_REASON:\n",
    "            print(\"Skipping instance {:d} -- {}: best and worst bound on leaf nodes coincide for all trees, no objectives ever tried, or no objectives successfully produced cuts.\".format(\n",
    "                i, inst, num_attempts[i]))\n",
    "        skip_instance = True\n",
    "        exitreason = 'NO_CUTS'\n",
    "        df_rejection_reason.loc[inst, exitreason] = True\n",
    "    else:        \n",
    "        # Ensure IP objective value is known\n",
    "        ip_obj = curr_df[col_ip_obj][curr_df.index[0]]\n",
    "        if not isinstance(ip_obj,float):\n",
    "            if PRINT_SKIP_REASON:\n",
    "                print(\n",
    "                    \"Skipping instance {:d} -- {}: IP objective value ({}) is not detected to be a float value.\".format(\n",
    "                    i, inst, ip_obj))\n",
    "            skip_instance = True\n",
    "            df_rejection_reason.loc[inst, 'IP_OPT_UNKNOWN'] = True\n",
    "            \n",
    "        # Ensure nrows and ncols is not too many\n",
    "        nrows = curr_df[col_num_rows][curr_df.index[0]]\n",
    "        ncols = curr_df[col_num_cols][curr_df.index[0]]\n",
    "        if (nrows > MAX_ROWS) or (ncols > MAX_COLS):\n",
    "            if PRINT_SKIP_REASON:\n",
    "                print(\"Skipping instance {:d} -- {}: nrows = {:d} > {:d} or ncols = {:d} > {:d}.\".format(\n",
    "                        i, inst, nrows, ncols, MAX_ROWS, MAX_COLS))\n",
    "            skip_instance = True\n",
    "            df_rejection_reason.loc[inst, 'TOO_MANY_ROWS_OR_COLS'] = True\n",
    "    \n",
    "    if not skip_instance:\n",
    "        #selected_gap_instances_dict[len(selected_gap_instances_dict)] = inst\n",
    "        selected_gap_instances_dict[inst] = i\n",
    "    else:\n",
    "        df_rejection_reason.loc[inst, 'SELECTED_GAP'] = False\n",
    "        df_rejection_reason.loc[inst, 'SELECTED_TIME'] = False\n",
    "        df_rejection_reason.loc[inst, 'SELECTED_6TREES'] = False\n",
    "\n",
    "selected_gap_instances = selected_gap_instances_dict.keys()\n",
    "num_selected_gap_instances = len(selected_gap_instances_dict)\n",
    "print(\"Total number of errors: {}\".format(num_gap_errors))\n",
    "print(\"Total number of selected instances for gap closed reporting: {}/{:d}\".format(num_selected_gap_instances,len(instances)))\n",
    "\n",
    "# intersect the pure binary instances with the selected gap instances\n",
    "binary_x_gap_instances = [inst for inst in pure_binary_instances if inst in selected_gap_instances]\n",
    "print(\"Total number of binary instances for gap closed reporting: {}/{:d}\".format(len(binary_x_gap_instances),num_selected_gap_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all instances from df with df_rejection_reason '<7_ATTEMPTS' == True\n",
    "instances_with_less_than_7_attempts = df_rejection_reason[df_rejection_reason['<7_ATTEMPTS'] == True].index.tolist()\n",
    "display(instances_with_less_than_7_attempts)\n",
    "\n",
    "# Get df_bb entries for instances_with_less_than_7_attempts\n",
    "df_bb_with_less_than_7_attempts = df.loc[instances_with_less_than_7_attempts]\n",
    "\n",
    "df_bb_with_less_than_7_attempts.loc[instances_with_less_than_7_attempts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_vpc_df = df.loc[df.index.get_level_values(1) > 0]\n",
    "max_num_vpc_df = max_num_vpc_df[col_num_vpc].groupby(level=0).max()\n",
    "print(\"Across selected_gap_instances, minimum number of VPCs generated across all depths: \",max_num_vpc_df.loc[selected_gap_instances].min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selected_time_instances_dict` and `all6_instances_dict` (original index, instance): Select instances for time tables\n",
    "\n",
    "Criteria to filter instances for reporting time:\n",
    "* ip opt val is known\n",
    "* lp opt < ip opt\n",
    "* max(nrows, ncols) ≤ 5K\n",
    "* optimal solution should not be found by any of the partial trees\n",
    "* either lp opt < disj lb or disj lb < disj ub\n",
    "* PRLP is feasible and solves within timelimit for at least one of the attempts\n",
    "* Gur7 < 3600 (Gurobi is able to solve the instance to optimality within an hour either with or without using VPCs)\n",
    "\n",
    "6 trees set\n",
    "* all six partial tree sizes produced VPCs and solved within timelimit (to make # of instances consistent across depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select instances for time tables\n",
    "#\n",
    "# Criteria to filter instances for reporting time:\n",
    "# * ip opt val is known\n",
    "# * lp opt < ip opt \n",
    "# * max(nrows, ncols) ≤ 5K\n",
    "# * optimal solution should not be found by any of the partial trees\n",
    "# * either lp opt < disj lb or disj lb < disj ub\n",
    "# * PRLP is feasible and solves within timelimit for at least one of the attempts\n",
    "# * min(Gur,Gur+V) < 3600 (Gurobi is able to solve the instance to optimality within an hour either with or without using VPCs)\n",
    "#\n",
    "# 6 trees set\n",
    "# * all six partial tree sizes produced VPCs \n",
    "# * ** and solved within timelimit (to make number of instances consistent across depths)\n",
    "\n",
    "# Constants\n",
    "PRINT_SKIP_REASON = False\n",
    "\n",
    "# Information to save\n",
    "selected_time_instances_dict = {}   # dictionary of (original index, instance)\n",
    "all6_instances_dict = {}            # dictionary of (original index, instance)\n",
    "skipped_instances_dict = {}         # dictionary of (original index, instance)\n",
    "error_instances_dict = {}           # dictionary of (original index, instance)\n",
    "\n",
    "num_timeouts = 0\n",
    "num_time_errors = 0\n",
    "\n",
    "inst_set = selected_gap_instances\n",
    "# inst_set = ['lotsize_presolved']\n",
    "# inst_set = ['neos-3754480-nidda_presolved']\n",
    "for i, inst in enumerate(inst_set):\n",
    "    print(\"{}/{}\".format(i+1,len(inst_set)), end='\\r', flush=True)\n",
    "    skip_instance = False\n",
    "    curr_df = df.loc[inst]\n",
    "    curr_df = curr_df[curr_df.index.get_level_values(0) >= 0]\n",
    "\n",
    "    # Check Gur < 3600 (Gurobi is able to solve the instance to optimality within an hour without using VPCs)\n",
    "    mintime_gur = float(curr_df.loc[2:64,ref_timeout_col].min())\n",
    "    \n",
    "    # Check V < 3600 (Gurobi is able to solve the instance to optimality within an hour either with using VPCs)\n",
    "    mintime_gur7 = float(curr_df.loc[2:64,refv_timeout_col].min())\n",
    "\n",
    "    mintime = min(mintime_gur, mintime_gur7)\n",
    "    if mintime >= MAX_TIME:\n",
    "        if PRINT_SKIP_REASON:\n",
    "            print(\"{:d}: Skipping instance {:d} -- {}: Gurobi's best time (with or without VPCs) is {:.7f} >= {:.7f}.\".format(\n",
    "                    len(skipped_instances_dict), i, inst, mintime, MAX_TIME-EPS\n",
    "                ))\n",
    "        skip_instance = True\n",
    "        skipped_instances_dict[inst] = i\n",
    "        num_timeouts += 1\n",
    "        df_rejection_reason.loc[inst, 'GUR_TIMEOUT'] += 1\n",
    "        # df_status_by_depth.loc[inst, int(curr_index)] = map_rejection_reason_to_number['GUR_TIMEOUT']\n",
    "\n",
    "    # Check how many times VPCs were successfully generated\n",
    "    num_successful_attempts = 0\n",
    "    curr_num_timeouts = 0\n",
    "    has_zero = False\n",
    "    for curr_index, row in curr_df.iterrows():\n",
    "        if row[col_num_disj_terms] == 0:\n",
    "            has_zero = True\n",
    "            continue\n",
    "\n",
    "        num_vpc = float(row[col_num_vpc])\n",
    "        num_successful_attempts += (num_vpc > 0)\n",
    "\n",
    "        curr_time = float(curr_df.loc[curr_index,refv_timeout_col])\n",
    "        if curr_time > MAX_TIME - EPS:\n",
    "            curr_num_timeouts += 1\n",
    "            if df_status_by_depth.loc[inst, int(curr_index)] == DEFAULT_STATUS:\n",
    "                df_status_by_depth.loc[inst, int(curr_index)] = map_rejection_reason_to_number['GUR_TIMEOUT']\n",
    "\n",
    "    if not has_zero:\n",
    "        raise ValueError(\n",
    "            \"*** ERROR: Instance {:d} -- {}: has no bb0 entry.\".format(\n",
    "                i, inst, curr_index\n",
    "            )\n",
    "        )        \n",
    "    \n",
    "    # if num_successful_attempts == 0 and not skip_instance:\n",
    "    #     if PRINT_SKIP_REASON:\n",
    "    #         print(\"Skipping instance {:d} -- {}: no VPCs generated successfully for any number of terms.\".format(i, inst, num_attempts[i]))\n",
    "    #     skip_instance = True\n",
    "    #     skipped_instances_dict[inst] = i\n",
    "\n",
    "    if not skip_instance:\n",
    "        if num_successful_attempts == 6 and curr_num_timeouts == 0:\n",
    "            all6_instances_dict[inst] = i\n",
    "        else:\n",
    "            df_rejection_reason.loc[inst, 'SELECTED_6TREES'] = False\n",
    "        selected_time_instances_dict[inst] = i\n",
    "    else:\n",
    "        df_rejection_reason.loc[inst, 'SELECTED_TIME'] = False\n",
    "        df_rejection_reason.loc[inst, 'SELECTED_6TREES'] = False\n",
    "\n",
    "selected_time_instances = selected_time_instances_dict.keys()\n",
    "all6_instances = all6_instances_dict.keys()\n",
    "\n",
    "num_selected_time_instances = len(selected_time_instances_dict)\n",
    "num_all6_instances = len(all6_instances_dict)\n",
    "print(\"Total number of errors: {}\".format(num_time_errors))\n",
    "print(\"Total number of timeouts: {}\".format(num_timeouts))\n",
    "print(\"Total number of instances for time reporting: {}/{}\".format(num_selected_time_instances, len(inst_set)))\n",
    "print(\"Total number of \\\"6 trees\\\" instances: {}\".format(num_all6_instances))\n",
    "\n",
    "#list(set(selected_gap_instances_dict.keys()).intersection(pure_binary_instances))\n",
    "#selected_pure_binary_instances.sort()\n",
    "\n",
    "# print(\"Found {:d} pure binary instances\".format(len(selected_pure_binary_instances)))\n",
    "\n",
    "binary_x_time_instances = [inst for inst in pure_binary_instances if inst in selected_time_instances_dict]\n",
    "print(\"Number of pure binary instances for time reporting: {}\".format(len(binary_x_time_instances)))\n",
    "\n",
    "all6_binary_x_time_instances = [inst for inst in all6_instances_dict.keys() if inst in binary_x_time_instances]\n",
    "print(\"Number of pure binary instances in all-6 set: {}\".format(len(all6_binary_x_time_instances)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG (check which instances were selected but do not have all six runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEBUG (check which instances were selected but do not have all six runs)\n",
    "not_all_6 = [key for key in selected_time_instances_dict.keys() if key not in all6_instances_dict.keys()]\n",
    "not_all_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file paths\n",
    "gap_file_path = 'selected_gap_instances.csv'\n",
    "time_file_path = 'selected_time_instances.csv'\n",
    "rejected_file_path = 'rejected_instances.csv'\n",
    "\n",
    "# Get the keys from selected_gap_instances_dict and selected_time_instances_dict\n",
    "selected_gap_keys = list(selected_gap_instances_dict.keys())\n",
    "selected_time_keys = list(selected_time_instances_dict.keys())\n",
    "\n",
    "# Write the keys to the CSV files\n",
    "with open(gap_file_path, 'w', newline='') as gap_file:\n",
    "  gap_writer = csv.writer(gap_file)\n",
    "  gap_writer.writerow(['Selected Gap Instances'])\n",
    "  gap_writer.writerows([[key] for key in selected_gap_keys])\n",
    "\n",
    "with open(time_file_path, 'w', newline='') as time_file:\n",
    "  time_writer = csv.writer(time_file)\n",
    "  time_writer.writerow(['Selected Time Instances'])\n",
    "  time_writer.writerows([[key] for key in selected_time_keys])\n",
    "\n",
    "df_rejection_reason.to_csv(rejected_file_path)\n",
    "\n",
    "print('Selected instances have been written to the CSV files.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Gap closed tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `calc_gap_closed` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate gap closed for GMICs, Gurobi, and VPCs\n",
    "def calc_gap_closed(gap_df, col):\n",
    "    return np.where(\n",
    "        ((gap_df[col] > EPS) & (gap_df[col] < INFINITY)) | ((gap_df[col] < -EPS) & (gap_df[col] > -INFINITY)), # condition\n",
    "        100. * (gap_df[col] - gap_df[col_lp_obj]) / (gap_df[col_ip_obj] - gap_df[col_lp_obj]), # if condition is true\n",
    "        0.0 # if condition is false\n",
    "    )\n",
    "\n",
    "# def calc_gap_closed2(gap_df, col):\n",
    "#     conditions = gap_df[col] > EPS & np.isfinite(gap_df[col])\n",
    "#     choices = 100. * (gap_df[col] - gap_df[\"LP OBJ\"]) / (gap_df[\"IP OBJ\"] - gap_df[\"LP OBJ\"])\n",
    "#     return np.select(conditions, choices, default=0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gap_df`: Calculate gap closed for GMICs, Gurobi, and VPCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of dataframe relevant to gap closed\n",
    "gap_df = df.loc[:, \n",
    "                [\n",
    "                    col_num_disj_terms,\n",
    "                    col_num_rows,\n",
    "                    col_num_cols\n",
    "                ]\n",
    "                +\n",
    "                obj_val_col_list\n",
    "                +\n",
    "                [\n",
    "                    col_num_gmic,\n",
    "                    col_num_vpc,\n",
    "                    col_num_obj,\n",
    "                    col_exit_reason\n",
    "                ]\n",
    "               ]\n",
    "\n",
    "# Calculate some missing % gap closed columns\n",
    "# gap closed = 100 * (post_cut_opt_val - lp_opt_val) / (ip_opt_val - lp_opt_val)\n",
    "for cut_type in reg_cut_type_long_list:\n",
    "    col = cut_type + ' ' + obj_stub\n",
    "    if col not in df.columns:\n",
    "        if cut_type == 'MAX(GMIC,VPC)':\n",
    "            # Add max(G,V) column\n",
    "            gap_df[cut_type + ' ' + pct_gap_closed_stub] = \\\n",
    "                np.maximum(\n",
    "                    gap_df['GMIC' + ' ' + pct_gap_closed_stub],\n",
    "                    gap_df[ 'VPC' + ' ' + pct_gap_closed_stub]\n",
    "                )\n",
    "        continue\n",
    "    gap_df[cut_type + ' ' + pct_gap_closed_stub] = calc_gap_closed(gap_df, col)\n",
    "\n",
    "# Compare against reference solver\n",
    "for stat_type in solver_stat_list[:3]:\n",
    "    for solver_type in solver_stubs:\n",
    "        for cut_type in solver_cut_type_stubs:\n",
    "            col = stat_type + ' ' + solver_type + ' ' + cut_type\n",
    "            gap_df[col + ' ' + pct_gap_closed_stub] = calc_gap_closed(gap_df, col)\n",
    "\n",
    "display(gap_df.loc[['bm23_presolved','maxgasflow_presolved']][gap_cols])\n",
    "display(gap_df.loc[(\"bm23_presolved\",2)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selected_gap_df`: Gap closed for selected instances, adding 0-row that has best for `V+` cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## `selected_gap_df`: Gap closed for selected instances, adding 0-row that has best for `V+` cols\n",
    "## Show the instances that have been selected (and their original index)\n",
    "## and then set the selected_gap_df as the selected instances from gap_df\n",
    "## We also set the '0' row to contain the best result for each method\n",
    "## (including the option of not using VPCs at all)\n",
    "## and we replace any runs with no VPCs with the values obtained without them\n",
    "selected_gap_df = gap_df.loc[selected_gap_instances]\n",
    "\n",
    "# From https://pandas.pydata.org/docs/user_guide/advanced.html#defined-levels\n",
    "# \"The MultiIndex keeps all the defined levels of an index, even if they are not actually used.\n",
    "# When slicing an index, you may notice this.\"\n",
    "# Even without using remove_unused_levels, index was correct with selected_gap_df.index.get_level_values(0).unique()\n",
    "selected_gap_df.index = selected_gap_df.index.remove_unused_levels()\n",
    "\n",
    "#display(selected_gap_df.index.difference(gap_df.index))\n",
    "#selected_gap_df.drop(['22433_presolved'])\n",
    "\n",
    "# # Check what the selected_gap_df contains for bm23\n",
    "# inst = \"bm23_presolved\"\n",
    "# display(selected_gap_df.loc[inst])\n",
    "\n",
    "#inst = \"10teams_presolved\"\n",
    "# inst = '22433_presolved'\n",
    "# curr_df = selected_gap_df.loc[inst]\n",
    "# display(curr_df)\n",
    "# # for i in curr_df.index:\n",
    "# #     display(curr_df.loc[i])\n",
    "\n",
    "#display(selected_gap_df.index.get_level_values(0).unique())\n",
    "\n",
    "# Do we update the value of the \"best\" in each column when no VPCs are generated for a run and we use the \"no-VPCs\" data?\n",
    "# This may cause the stats in the \"best\" row to improve\n",
    "# For example, we replace V+GurF with GurF when no VPCs are generated, since that is what would occur without VPCs\n",
    "# But if GurF is better than any V+GurF when VPCs are produced, then the average in the max-row is inflated\n",
    "SHOULD_UPDATE_MAX_WHEN_NO_VPCS = False\n",
    "\n",
    "# inst_set = selected_gap_df.index.get_level_values(0).unique()\n",
    "inst_set = selected_gap_df.index.levels[0]\n",
    "num_inst = len(inst_set)\n",
    "for curr_inst_ind, inst in enumerate(inst_set):\n",
    "    print(\"{}/{}\".format(curr_inst_ind+1,num_inst), end='\\r', flush=True)\n",
    "    curr_df = selected_gap_df.loc[inst].copy() # copy needed to not throw SettingWithCopyWarning\n",
    "    curr_df = curr_df[curr_df.index.get_level_values(0) >= 0] # to remove disjset = -1 row, for example\n",
    "\n",
    "    # Set 0-row to have max values across all rows for this instance\n",
    "    max_vals = curr_df[gap_cols].max()\n",
    "    selected_gap_df.loc[(inst,0),gap_cols] = max_vals\n",
    "\n",
    "    for ind in curr_df.index:\n",
    "        if ind <= 0:\n",
    "            continue\n",
    "\n",
    "        # Propogate GurF and GurL down\n",
    "        # sel_gap = [col_first_ref_first, col_first_ref, col_last_ref_first, col_last_ref]\n",
    "        sel_gap = ref_solver_gap_cols\n",
    "        selected_gap_df.loc[(inst,ind),sel_gap] = curr_df.loc[0,sel_gap]\n",
    "\n",
    "        # If no VPCs produced, the values for V+GurF and V+GurL have not been provided\n",
    "        # We replace these by GurF and GurL\n",
    "        # Currently disabled: update max for that column too (if disabled, we instead keep max as the value among those that generated VPCs)\n",
    "        num_vpc = curr_df.loc[ind,col_num_vpc]\n",
    "        if num_vpc == 0:\n",
    "            # print(\"Zero cuts for inst {} at depth {:d}\".format(inst, ind))\n",
    "            ref_gap = [col_first_cut_pass_gap_ref, col_last_cut_pass_gap_ref] # this is where we pull info from\n",
    "            refinds = [gap_cols.index(colname) for colname in ref_gap] \n",
    "            sel_gap = [col_first_cut_pass_gap_ref_v, col_last_cut_pass_gap_ref_v] # this is where we put the info\n",
    "            selected_gap_df.loc[(inst,ind),sel_gap] = curr_df.loc[0,ref_gap].to_numpy()\n",
    "\n",
    "            if SHOULD_UPDATE_MAX_WHEN_NO_VPCS:\n",
    "                for i in refinds:\n",
    "                    if curr_df.loc[0,gap_cols[i]] > selected_gap_df.loc[(inst,0),gap_cols[i+1]]:\n",
    "                        # if curr_df.loc[0,gap_cols[i]] > 0:\n",
    "                            # print(\"DEBUG: Updating {} for inst {} from {:f} to {:f}\".format(\n",
    "                            #     gap_cols[i+1], \n",
    "                            #     inst, \n",
    "                            #     selected_gap_df.loc[(inst,0),gap_cols[i+1]], \n",
    "                            #     curr_df.loc[0,gap_cols[i]]))\n",
    "                        selected_gap_df.loc[(inst,0),gap_cols[i+1]] = curr_df.loc[0,gap_cols[i]]\n",
    "\n",
    "display(selected_gap_df.head(21).loc[:,[col_num_vpc]+gap_cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG: Why REF+V is less than REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG\n",
    "# Why REF+V < REF\n",
    "\n",
    "# inst = 'f2gap801600_presolved'\n",
    "#inst = 'neos-1112787_presolved'\n",
    "inst = 'neos-1330346_presolved'\n",
    "\n",
    "tmp_df = gap_df.loc[inst,\n",
    "                      [col_num_vpc]\n",
    "                      +\n",
    "                      [col_first_cut_pass_bound_ref, col_first_cut_pass_bound_ref_v]\n",
    "                      +\n",
    "                      gap_cols\n",
    "                      +\n",
    "                      [col_lp_obj, col_ip_obj]\n",
    "                    ]\n",
    "\n",
    "# # Compare against reference solver\n",
    "# for stat_type in solver_stat_list:\n",
    "#     for solver_type in solver_stubs:\n",
    "#         for cut_type in solver_cut_type_list:\n",
    "#             col = stat_type + ' ' + solver_type + ' ' + cut_type\n",
    "#             gap_df[col + ' ' + pct_gap_closed_stub] = calc_gap_closed(gap_df, col)\n",
    "\n",
    "# display(tmp_df)\n",
    "# display(gap_df.loc[inst,['NUM VPC']+['BEST REF FIRST_CUT_PASS']+['FIRST REF+V FIRST_CUT_PASS']+gap_cols])\n",
    "\n",
    "# display(selected_gap_df.loc[inst,['NUM VPC']+['BEST REF FIRST_CUT_PASS']+['FIRST REF+V FIRST_CUT_PASS']+gap_cols])\n",
    "\n",
    "tmp_df[col_first_cut_pass_gap_ref_v] = calc_gap_closed(tmp_df, col_first_cut_pass_bound_ref_v)\n",
    "display(tmp_df)\n",
    "\n",
    "# display(gap_df.loc[inst,['NUM VPC']+['BEST REF FIRST_CUT_PASS']+['FIRST REF+V FIRST_CUT_PASS']+gap_cols])\n",
    "\n",
    "# display(selected_gap_df.loc[inst,['NUM VPC']+['BEST REF FIRST_CUT_PASS']+['FIRST REF+V FIRST_CUT_PASS']+gap_cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `best_gap_df`: For each instance, what the best gap closed is (and how that was obtained)\n",
    "\n",
    "The columns are defined by:\n",
    "* `gap_cols_short`\n",
    "* the best disjunction for 'V', 'V+G', 'V+GurF', 'V+GurL'\n",
    "* number of VPCs leading to best 'V' result\n",
    "* number of GMICs for the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create best df = for each instance, what the best gap closed is (and how that was obtained)\n",
    "\n",
    "# inst_set = selected_gap_instances_dict.keys()\n",
    "inst_set = selected_gap_df.index.levels[0]\n",
    "# inst_set = ['neos22_presolved']\n",
    "# inst_set = ['usAbbrv-8-25_70_presolved']\n",
    "\n",
    "best_gap_df = pd.DataFrame(\n",
    "    columns = gap_cols_short+[\n",
    "        col_best_disj_vpc,\n",
    "        col_best_disj_gmic_vpc,\n",
    "        col_best_disj_refv_first_cut_pass,\n",
    "        col_best_disj_refv_last_cut_pass,\n",
    "        col_num_vpc,\n",
    "        col_num_gmic,\n",
    "    ],\n",
    "    index = inst_set,\n",
    "    dtype = float,\n",
    ")\n",
    "\n",
    "best_disj_short_types = [ 'V', 'V+G' ] + refv_cut_type_short_list\n",
    "ind_of_gap_for_best_disj = [\n",
    "    gap_cols_short.index(cut_type) for cut_type in best_disj_short_types\n",
    "]\n",
    "\n",
    "num_inst = len(inst_set)\n",
    "for i, inst in enumerate(inst_set):\n",
    "    print(\"{}/{}\".format(i+1,num_inst), end='\\r', flush=True)\n",
    "    # print(\"Processing instance {:d} with name {}.\".format(i, inst))\n",
    "    best_gaps = [ -1 for _ in range(len(gap_cols_short)) ]\n",
    "    best_disj = [ -1 for _ in range(len(best_disj_short_types)) ]\n",
    "    best_num_vpc = -1\n",
    "    best_num_gmic = -1\n",
    "    \n",
    "    # # best_vpc = -1.\n",
    "    # best_vpc_disj = -1\n",
    "    # # best_max_gmic_vpc = -1.\n",
    "    # # best_vpcgmic = -1.\n",
    "    # best_vpcgmic_disj = -1\n",
    "    # # best_VGurF = -1.\n",
    "    # best_VGurF_disj = -1\n",
    "    # # best_VGurL = -1.\n",
    "    # best_VGurL_disj = -1\n",
    "    \n",
    "    curr_df = selected_gap_df.loc[inst]\n",
    "    # curr_df = curr_df[curr_df.index.get_level_values(0) > 0]\n",
    "    \n",
    "    # Get info for GurF and GurL from the no-VPC row\n",
    "    # row = curr_df.loc[0]\n",
    "    # gmic_gap = float(row[col_gmic])\n",
    "    # GurF_gap = float(row[col_first_cut_pass_gap_ref])\n",
    "    # GurL_gap = float(row[col_last_cut_pass_gap_ref])\n",
    "    # root_gap = float(row[col_root])\n",
    "    # disj_gap = float(row[col_best_disj])\n",
    "\n",
    "    for index, row in curr_df.iterrows():\n",
    "        num_disj_terms = int(row[col_num_disj_terms])\n",
    "        # num_obj_tried  = float(row[col_num_obj])\n",
    "        num_vpc        = float(row[col_num_vpc])\n",
    "        if num_disj_terms <= 0:\n",
    "            continue\n",
    "        if num_vpc <= 0:\n",
    "            # Check if we update 'DB' value\n",
    "            db_ind = gap_cols_short.index('DB')\n",
    "            if row[col_best_disj] > best_gaps[db_ind]:\n",
    "                best_gaps[db_ind] = row[col_best_disj]\n",
    "            continue\n",
    "            \n",
    "        # print(\"Index {:d}: Processing instance {} with {:d} disj terms.\".format(index, inst, num_disj_terms))\n",
    "        curr_gaps = [ float(row[map_short_to_cols_gap[col]]) for col in gap_cols_short ]\n",
    "        num_gmic    = float(row[col_num_gmic])\n",
    "        # vpc_gap     = float(row[col_vpc])\n",
    "        # vpcgmic_gap = float(row[col_vpc_gmic])\n",
    "        # VGurF_gap   = float(row[col_first_ref_v])\n",
    "        # VGurL_gap   = float(row[col_last_ref_v])\n",
    "        # num_vpc     = float(row['NUM VPC'])\n",
    "\n",
    "        # Update best gap closed for each method\n",
    "        for j, gap in enumerate(curr_gaps):\n",
    "            if gap > best_gaps[j]:\n",
    "                best_gaps[j] = gap\n",
    "                if j in ind_of_gap_for_best_disj:\n",
    "                    best_disj[ind_of_gap_for_best_disj.index(j)] = index\n",
    "                if gap_cols_short[j] == 'V':\n",
    "                    best_num_vpc = num_vpc\n",
    "                    best_num_gmic = num_gmic\n",
    "        \n",
    "        # if (best_vpc < vpc_gap): #or (is_val(best_vpc, vpc_gap) and best_num_vpc == 0):\n",
    "        #     best_vpc = vpc_gap\n",
    "        #     best_vpc_disj = index\n",
    "        #     best_num_vpc = num_vpc\n",
    "        #     best_num_gmic = num_gmic\n",
    "        # if best_vpcgmic < vpcgmic_gap:\n",
    "        #     best_vpcgmic = vpcgmic_gap\n",
    "        #     best_vpcgmic_disj = index\n",
    "        # if best_max_gmic_vpc < max(vpc_gap, gmic_gap):\n",
    "        #     best_max_gmic_vpc = max(vpc_gap, gmic_gap)\n",
    "        # if best_VGurF < VGurF_gap:\n",
    "        #     best_VGurF = VGurF_gap\n",
    "        #     best_VGurF_disj = index\n",
    "        # if best_VGurL < VGurL_gap:\n",
    "        #     best_VGurL = VGurL_gap\n",
    "        #     best_VGurL_disj = index\n",
    "\n",
    "\n",
    "    best_gaps = [ gap if gap >= EPS else 0. for gap in best_gaps ]\n",
    "    best_gap_df.iloc[i] = best_gaps + best_disj + [best_num_vpc, best_num_gmic]\n",
    "        # gmic_gap if gmic_gap >= EPS else 0.,\n",
    "        # root_gap if root_gap >= EPS else 0.,\n",
    "        # disj_gap if disj_gap >= EPS else 0.,\n",
    "        # best_vpc if best_vpc >= EPS else 0.,\n",
    "        # best_max_gmic_vpc if best_max_gmic_vpc >= EPS else 0.,\n",
    "        # best_vpcgmic if best_vpcgmic >= EPS else 0.,\n",
    "        # GurF_gap if GurF_gap >= EPS else 0.,\n",
    "        # best_VGurF if best_VGurF >= EPS else 0.,\n",
    "        # GurL_gap if GurL_gap >= EPS else 0.,\n",
    "        # best_VGurL if best_VGurL >= EPS else 0.,\n",
    "    #     best_vpc_disj,\n",
    "    #     best_vpcgmic_disj,\n",
    "    #     best_VGurF_disj,\n",
    "    #     best_VGurL_disj,\n",
    "    #     best_num_vpc,\n",
    "    #     best_num_gmic,\n",
    "    # ]\n",
    "\n",
    "int_col_list = [ col_best_disj_vpc, col_best_disj_gmic_vpc, col_best_disj_refv_first_cut_pass, col_best_disj_refv_last_cut_pass, col_num_vpc, col_num_gmic ]\n",
    "#['BEST VPC DISJ', 'BEST GMIC+VPC DISJ', 'BEST V+GurF DISJ', 'BEST V+GurL DISJ', 'NUM VPC', 'NUM GMIC']\n",
    "for col in int_col_list:\n",
    "    best_gap_df[col] = best_gap_df[col].astype(np.int64)\n",
    "\n",
    "display(best_gap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG: Look at `best_gap_df` entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_gap_df.to_csv('best_gap.csv')\n",
    "tmp_inst_set = ['cap6000_presolved','neos-1330346_presolved']\n",
    "best_gap_df.loc[tmp_inst_set]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG: In `best_gap_df`, can get V > V+G due to numerical issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEBUG: You can get V > V+G due to numerical issues\n",
    "\n",
    "col1 = best_gap_df['V']\n",
    "col2 = best_gap_df['V+G']\n",
    "\n",
    "display(best_gap_df[(col1 > col2 + EPS) == True])\n",
    "\n",
    "#df.loc['neos-1058477_presolved'] #.to_csv(\"neos-1058477_presolved_data.csv\")\n",
    "df.loc['seymour-disj-10_presolved']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG: Find instances in which V+GurF max does not match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEBUG: Find instances in which V+GurF max does not match up\n",
    "# This causes the value in Table 2 'Best' row to not match Table 1 'All'\n",
    "\n",
    "# For instance f2gap801600_presolved, the gap closed at the end of the root node is 0% whenever VPCs are used,\n",
    "# but without VPCs, the gap closed is 50%\n",
    "# In `best_gap_df`, for an instance in which no VPCs were generated,\n",
    "# we use the value of GurF/GurL for V+GurF/V+GurL\n",
    "# In `selected_gap_df`, the \"zero\" row contains\n",
    "\n",
    "num_inst = len(best_gap_df.index)\n",
    "col = 'V+GurF'\n",
    "origcol = map_short_to_cols_gap[col]\n",
    "num_errors = 0\n",
    "avg1 = 0\n",
    "avg2 = 0\n",
    "for inst in best_gap_df.index:\n",
    "    val1 = best_gap_df.loc[inst,col]\n",
    "    val2 = selected_gap_df.loc[(inst,0),origcol]\n",
    "    if abs(val1-val2) > EPS:\n",
    "        print(\"{} has best_gap_df = {:f} and selected_gap_df = {:f} for col {} (diff = {:e})\".format(inst,val1,val2,col,abs(val1-val2)))\n",
    "        num_errors += 1\n",
    "    avg1 += val1 / num_inst\n",
    "    avg2 += val2 / num_inst\n",
    "\n",
    "print(\"Average from best_gap_df = {}\".format(avg1))\n",
    "print(\"Average from selected_gap_df = {}\".format(avg2))\n",
    "print(\"Total # of errors =\", num_errors, flush=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG: Print relevant info from `selected_gap_df` and `best_gap_df` to further debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEBUG\n",
    "# inst = 'f2gap801600_presolved'\n",
    "# inst = 'neos22_presolved'\n",
    "# inst = 'neos-1112787_presolved'\n",
    "# display(best_gap_df.loc[inst])\n",
    "# display(selected_gap_df.loc[inst,[col_num_vpcs]+gap_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DEBUG\n",
    "# gap_cols = [\n",
    "#     'GMIC % GAP CLOSED',\n",
    "#     'BEST DISJ % GAP CLOSED',\n",
    "#     'VPC % GAP CLOSED',\n",
    "#     'VPC+GMIC % GAP CLOSED',\n",
    "#     'REF FIRST_CUT_PASS % GAP CLOSED',\n",
    "#     'REF+V FIRST_CUT_PASS % GAP CLOSED',\n",
    "#     'REF LAST_CUT_PASS % GAP CLOSED',\n",
    "#     'REF+V LAST_CUT_PASS % GAP CLOSED',\n",
    "# ]\n",
    "# col_num_vpcs = 'NUM VPC'\n",
    "\n",
    "inst = 'f2gap801600_presolved'\n",
    "tmp_selected_gap_df = gap_df.loc[selected_gap_instances_dict.keys()]\n",
    "# Check if inst is in tmp_selected_gap_df\n",
    "if inst not in tmp_selected_gap_df.index.get_level_values(0).unique():\n",
    "    print(ValueError(\"Instance {} is not in tmp_selected_gap_df\".format(inst)))\n",
    "else:\n",
    "    curr_df = tmp_selected_gap_df.loc[inst].copy() # copy needed to not throw SettingWithCopyWarning\n",
    "\n",
    "    # Set 0-row to have max values across all rows for this instance\n",
    "    max_vals = curr_df[gap_cols].max()\n",
    "    # selected_gap_df.loc[(inst,0),gap_cols] = max_vals\n",
    "\n",
    "    display(tmp_selected_gap_df.loc[inst])\n",
    "    display(max_vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1: `avg_gap_df`: average percent gap closed across different combinations of cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE 1: average percent gap closed across different combinations of cuts\n",
    "## Create avg_gap_df = average gap closed across instances\n",
    "all_set_name = 'All'\n",
    "good_vpc_set_name = tex_escape('≥10%')\n",
    "binary_set_name = 'Binary'\n",
    "avg_row_name = tex_escape('Avg (%)')\n",
    "wins_row_name = 'Wins'\n",
    "\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [ [all_set_name, good_vpc_set_name, binary_set_name], [avg_row_name, wins_row_name] ],\n",
    "    names = ['Set', '']\n",
    ")\n",
    "    \n",
    "ncols = len(best_gap_df.columns)\n",
    "nrows = len(idx)\n",
    "\n",
    "col = best_gap_df['V'].astype(float)\n",
    "good_vpc_df = best_gap_df[col >= 10.]\n",
    "\n",
    "binary_instances_df = best_gap_df.loc[binary_x_gap_instances]\n",
    "\n",
    "data = np.zeros((nrows, ncols), dtype=float)\n",
    "data[0,:] = [best_gap_df[col].mean() for col in best_gap_df.columns]\n",
    "data[2,:] = [good_vpc_df[col].mean() for col in best_gap_df.columns]\n",
    "data[4,:] = [binary_instances_df[col].mean() for col in best_gap_df.columns]\n",
    "\n",
    "# display(best_gap_df.head())\n",
    "avg_gap_df = pd.DataFrame(\n",
    "    data,\n",
    "    columns = best_gap_df.columns,\n",
    "    index = idx,\n",
    "    dtype = object\n",
    ")\n",
    "\n",
    "inst_col_name = '# inst'\n",
    "avg_gap_df[inst_col_name] = [len(best_gap_df), 0, len(good_vpc_df), 0, len(binary_instances_df), 0]\n",
    "\n",
    "avg_gap_df.iloc[1] = [\"\" for i in range(ncols+1)]\n",
    "avg_gap_df.iloc[3] = [\"\" for i in range(ncols+1)]\n",
    "avg_gap_df.iloc[5] = [\"\" for i in range(ncols+1)]\n",
    "\n",
    "display(avg_gap_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wins_df`: num wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create num wins df\n",
    "# x wins over y for an instance if x > y + EPS\n",
    "#shortcols = avg_gap_df.columns[0:-1]\n",
    "wins_df = pd.DataFrame(\n",
    "    np.zeros((len(gap_cols_short), len(gap_cols_short)), dtype=int),\n",
    "    columns = gap_cols_short,\n",
    "    index = gap_cols_short,\n",
    "    dtype = int,\n",
    ")\n",
    "\n",
    "WINS_EPS = GAP_DIFF_EPS\n",
    "\n",
    "from itertools import permutations\n",
    "for (ind1, ind2) in permutations(range(len(gap_cols_short)), 2):\n",
    "    wins_df.at[gap_cols_short[ind1],gap_cols_short[ind2]] =\\\n",
    "        int(sum(best_gap_df[gap_cols_short[ind1]] > best_gap_df[gap_cols_short[ind2]] + WINS_EPS))\n",
    "    wins_df.at[gap_cols_short[ind2],gap_cols_short[ind1]] =\\\n",
    "        int(sum(best_gap_df[gap_cols_short[ind2]] > best_gap_df[gap_cols_short[ind1]] + WINS_EPS))\n",
    "\n",
    "# Sets we are considering\n",
    "# all_set = 'Wins (All)'\n",
    "# good_vpc_set = 'Wins (V ≥ 10%)'\n",
    "all_set = (all_set_name,wins_row_name)\n",
    "good_vpc_set = (good_vpc_set_name,wins_row_name)\n",
    "binary_set = (binary_set_name,wins_row_name)\n",
    "\n",
    "# \"G\" are wins relative to \"V\"\n",
    "shortrefcol = 'V'\n",
    "#refcol = 'VPC % GAP CLOSED'\n",
    "#refcol = map_short_to_cols[shortrefcol]\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'G'\n",
    "#col = 'GMIC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "avg_gap_df.at[all_set,shortdestcol] = wins_df.at[shortdestcol,shortrefcol]\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > good_vpc_df[refcol] + WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"DB\", \"V\", \"V+G\": wins are relative to \"G\"\n",
    "shortrefcol = 'G'\n",
    "#refcol = 'GMIC % GAP CLOSED'\n",
    "#refcol = map_short_to_cols[shortrefcol]\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'DB'\n",
    "#col = 'BEST DISJ % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "avg_gap_df.at[all_set,shortdestcol] = wins_df.at[shortdestcol,shortrefcol]\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > good_vpc_df[refcol] + WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "shortdestcol = 'V'\n",
    "#col = 'VPC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "avg_gap_df.at[all_set,shortdestcol] = wins_df.at[shortdestcol,shortrefcol]\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > good_vpc_df[refcol] + WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "shortdestcol = 'V+G'\n",
    "#col = 'VPC+GMIC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "avg_gap_df.at[all_set,shortdestcol] = wins_df.at[shortdestcol,shortrefcol]\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > good_vpc_df[refcol] + WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"V+GurF\" are wins relative to \"GurF\"\n",
    "shortrefcol = 'GurF'\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'V+GurF'\n",
    "destcol = shortdestcol\n",
    "#col = map_short_to_cols[shortcol]\n",
    "avg_gap_df.at[all_set,shortdestcol] = wins_df.at[shortdestcol,shortrefcol]\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > good_vpc_df[refcol] + WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"V+GurL\" are wins relative to \"GurL\"\n",
    "shortrefcol = 'GurL'\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'V+GurL'\n",
    "destcol = shortdestcol\n",
    "wins_df.at[shortdestcol,shortrefcol] = int(sum(best_gap_df[destcol] > best_gap_df[refcol] + WINS_EPS))\n",
    "wins_df.at[shortrefcol,shortdestcol] = int(sum(best_gap_df[refcol] > best_gap_df[destcol] + WINS_EPS))\n",
    "avg_gap_df.at[all_set,shortdestcol] = wins_df.at[shortdestcol,shortrefcol]\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > good_vpc_df[refcol] + WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# Count number of instances that have V+G > 0\n",
    "shortdestcol = inst_col_name\n",
    "#col = 'V+GurL'\n",
    "destcol = 'V+G'\n",
    "avg_gap_df.at[all_set,shortdestcol] = sum(best_gap_df[destcol] > WINS_EPS)\n",
    "avg_gap_df.at[good_vpc_set,shortdestcol] = sum(good_vpc_df[destcol] > WINS_EPS)\n",
    "avg_gap_df.at[binary_set,shortdestcol] = sum(binary_instances_df[destcol] > WINS_EPS)\n",
    "\n",
    "display(avg_gap_df)\n",
    "display(wins_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_instances_df.to_csv('binary_instances.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze instances in which DB > G but V <= G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = 'DB'\n",
    "col2 = 'G'\n",
    "tmp_df = best_gap_df.loc[best_gap_df[col1] > best_gap_df[col2] + GAP_DIFF_EPS]\n",
    "\n",
    "col1 = 'V'\n",
    "tmp_df = tmp_df[tmp_df[col1] <= tmp_df[col2] + GAP_DIFF_EPS]\n",
    "display(tmp_df.head())\n",
    "\n",
    "# inst_set = tmp_df.index\n",
    "inst_depth_set = [(inst,tmp_df.at[inst,col_best_disj_vpc]) for inst in tmp_df.index]\n",
    "\n",
    "print(\"Total num inst with DB > G >= V is {:d}\".format(len(tmp_df)))\n",
    "print(\"Num times hit cut limit = {:d}\".format(sum(df.loc[inst_depth_set,col_exit_reason] == 'CUT_LIMIT')))\n",
    "\n",
    "# display(df.loc[inst_depth_set])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze instances in which V+G <= G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = 'V+G'\n",
    "col2 = 'G'\n",
    "tmp_df = best_gap_df.loc[best_gap_df[col1] <= best_gap_df[col2] + GAP_DIFF_EPS]\n",
    "\n",
    "display(tmp_df)\n",
    "\n",
    "inst_depth_set = [(inst,tmp_df.at[inst,col_best_disj_vpc]) for inst in tmp_df.index]\n",
    "\n",
    "print(\"Total num inst with V+G <= G is {:d}\".format(len(tmp_df)))\n",
    "refval = 100. - GAP_DIFF_EPS\n",
    "print(\"Num times with G > {}% gap closed = {:d}\".format(refval, sum(tmp_df['G'] > refval)))\n",
    "print(\"Num times with V+G = 0% gap closed = {:d}\".format(sum(tmp_df['V+G'] == 0.)))\n",
    "print(\"Num times hit cut limit = {:d}\".format(sum(df.loc[inst_depth_set,col_exit_reason] == 'CUT_LIMIT')))\n",
    "\n",
    "# display(df.loc[inst_depth_set])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze when G > V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = 'G'\n",
    "col2 = 'V'\n",
    "tmp_df = best_gap_df.loc[best_gap_df[col1] > best_gap_df[col2] + GAP_DIFF_EPS]\n",
    "\n",
    "display(tmp_df)\n",
    "\n",
    "inst_depth_set = [(inst,tmp_df.at[inst,col_best_disj_vpc]) for inst in tmp_df.index]\n",
    "\n",
    "print(\"Total num inst with G > V is {:d}\".format(len(tmp_df)))\n",
    "print(\"Num times with #V < 10 is {:d}\".format(len(tmp_df[(tmp_df[col_num_vpc] < 10)])))\n",
    "print(\"Num times with #V < 10 while #G > 10 is {:d}\".format(len(tmp_df[(tmp_df[col_num_vpc] < 10) & (tmp_df[col_num_gmic] > 10)])))\n",
    "# print(\"Num times with #V < 10 is {:d}\".format(sum(tmp_df['NUM VPC'] < 10)))\n",
    "\n",
    "# print(\"Num times with V+G = 0% gap closed = {:d}\".format(sum(tmp_df['V+G'] == 0.)))\n",
    "print(\"Num times hit cut limit = {:d}\".format(sum(df.loc[inst_depth_set,col_exit_reason] == 'CUT_LIMIT')))\n",
    "\n",
    "tmp_inst_set = tmp_df[(tmp_df[col_num_vpc] < 10) & (tmp_df[col_num_gmic] > 10)].index\n",
    "tmp_inst_depth_set = [(inst,tmp_df.at[inst,col_best_disj_vpc]) for inst in tmp_inst_set]\n",
    "print(\"Num times hit cut limit when #G > #V = {:d} (should be 0)\".format(sum(df.loc[tmp_inst_depth_set,col_exit_reason] == 'CUT_LIMIT')))\n",
    "\n",
    "# display(df.loc[inst_depth_set])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze when DB % gap closed nontrivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select instances in selected_gap_df for which value in col_best_disj is at least MIN_DISJ_GAP\n",
    "MIN_DISJ_GAP = 0.\n",
    "tmp_df = selected_gap_df.loc[selected_gap_df[col_best_disj_gap] >= MIN_DISJ_GAP]\n",
    "\n",
    "# Collect instance names with best disj gap >= MIN_DISJ_GAP\n",
    "tmp_df.index = tmp_df.index.remove_unused_levels()\n",
    "inst_set_db = tmp_df.index.get_level_values(0).unique()\n",
    "num_inst_db = len(inst_set_db)\n",
    "inst_set_orig = selected_gap_df.index.levels[0]\n",
    "num_inst_orig = len(inst_set)\n",
    "print(\"Total num inst with best disj gap >= {:f} is {:d} (out of {:d} total instances).\".format(MIN_DISJ_GAP, num_inst_db, num_inst_orig))\n",
    "\n",
    "# Report average in each column broken down by depth\n",
    "tmp_df_grouped = tmp_df.groupby(level='disj_terms').mean(numeric_only=True)\n",
    "display(tmp_df_grouped[gap_cols])\n",
    "\n",
    "# Repeat with MIN_DISJ_GAP = 1.0\n",
    "MIN_DISJ_GAP = 1.\n",
    "tmp_df = selected_gap_df.loc[selected_gap_df[col_best_disj_gap] >= MIN_DISJ_GAP]\n",
    "\n",
    "# Collect instance names with best disj gap >= MIN_DISJ_GAP\n",
    "tmp_df.index = tmp_df.index.remove_unused_levels()\n",
    "inst_set_db = tmp_df.index.get_level_values(0).unique()\n",
    "num_inst_db = len(inst_set_db)\n",
    "inst_set_orig = selected_gap_df.index.levels[0]\n",
    "num_inst_orig = len(inst_set)\n",
    "print(\"Total num inst with best disj gap >= {:f} is {:d} (out of {:d} total instances).\".format(MIN_DISJ_GAP, num_inst_db, num_inst_orig))\n",
    "\n",
    "# Report average in each column broken down by depth\n",
    "tmp_df_grouped = tmp_df.groupby(level='disj_terms').mean(numeric_only=True)\n",
    "display(tmp_df_grouped[gap_cols])\n",
    "\n",
    "# Repeat with MIN_DISJ_GAP = 10.\n",
    "MIN_DISJ_GAP = 10.\n",
    "tmp_df = selected_gap_df.loc[selected_gap_df[col_best_disj_gap] >= MIN_DISJ_GAP]\n",
    "\n",
    "# Collect instance names with best disj gap >= MIN_DISJ_GAP\n",
    "tmp_df.index = tmp_df.index.remove_unused_levels()\n",
    "inst_set_db = tmp_df.index.get_level_values(0).unique()\n",
    "num_inst_db = len(inst_set_db)\n",
    "inst_set_orig = selected_gap_df.index.levels[0]\n",
    "num_inst_orig = len(inst_set)\n",
    "print(\"Total num inst with best disj gap >= {:f} is {:d} (out of {:d} total instances).\".format(MIN_DISJ_GAP, num_inst_db, num_inst_orig))\n",
    "\n",
    "# Report average in each column broken down by depth\n",
    "tmp_df_grouped = tmp_df.groupby(level='disj_terms').mean(numeric_only=True)\n",
    "display(tmp_df_grouped[gap_cols])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2: `gap_by_size_df`: gap closed by num leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE 2: gap closed by num leaves\n",
    "## Note that ``best'' can be worse than for a single row\n",
    "## because when no VPCs are generated, we assume the \"no VPCs\" results hold for Gurobi,\n",
    "## but we do not count that for the ``best'' calculation,\n",
    "## since otherwise there is potential bias, as sometimes Gurobi does better without VPCs\n",
    "shortcols = [\n",
    "        'DB',\n",
    "        'V',\n",
    "        'max(G,V)',\n",
    "        'V+G'\n",
    "      ] + solver_cut_type_short_list[1]\n",
    "\n",
    "gap_by_size_df = pd.DataFrame(\n",
    "    columns = shortcols,\n",
    "    index = [0] + sizes + ['Best'],\n",
    "    # index = [str(size) + \" leaves\" for size in sizes]+['Best'],\n",
    "    dtype = float,\n",
    ")\n",
    "zero_row_name = 0\n",
    "\n",
    "# `grouped_df` will collect gap closed across instances, grouped by num terms\n",
    "grouped_df = selected_gap_df.groupby(level='disj_terms').mean(numeric_only=True)\n",
    "ungrouped_df = best_gap_df.mean(numeric_only=True)\n",
    "\n",
    "# For each of the columns (in shortcols),\n",
    "# save the average value for each size\n",
    "# (this will put in the right place as the index is based on sizes for both)\n",
    "for col in shortcols:\n",
    "    orig_col = map_short_to_cols_gap[col]\n",
    "    #gap_by_size_df.loc[2]['DB'] = best_gap_df[orig_col].mean()\n",
    "    gap_by_size_df[col] = grouped_df[orig_col]\n",
    "\n",
    "# Fill in the 'Best' row, since that is currently stored in `gap_by_size_df` in the \"0\" row\n",
    "gap_by_size_df.loc['Best'] = gap_by_size_df.loc[zero_row_name]\n",
    "\n",
    "# Now update the zero row with correct values\n",
    "col = 'DB'\n",
    "gap_by_size_df[col][zero_row_name] = 0.\n",
    "\n",
    "col = 'V'\n",
    "gap_by_size_df[col][zero_row_name] = 0.\n",
    "\n",
    "stubs = ['G', 'GurF', 'GurL']\n",
    "for stub in stubs:\n",
    "    col = 'V+'+stub\n",
    "    # orig_col = map_short_to_cols[stub]\n",
    "    gap_by_size_df[col][0] = ungrouped_df[stub]\n",
    "\n",
    "# Also replace the 0-row of the \"max(G,V)\" column with the value of G, since that corresponds to no VPCs\n",
    "gap_by_size_df['max(G,V)'][0] = gap_by_size_df['V+G'][0]\n",
    "\n",
    "# Reindex to add \"leaves\" to index\n",
    "idx = ['VPCs disabled']+[str(size) + \" leaves\" for size in sizes]+['Best']\n",
    "reidx = {old_id : new_id for old_id, new_id in zip(gap_by_size_df.index,idx)}\n",
    "gap_by_size_df.rename(reidx, inplace=True)\n",
    "\n",
    "# display(grouped_df[gap_cols])\n",
    "display(ungrouped_df)\n",
    "display(gap_by_size_df)\n",
    "\n",
    "# Create new df with additional columns:\n",
    "# (1) the ratio 'V'/'DB'\n",
    "# (2) the ratio 'max(G,V)'/'V+G'\n",
    "gap_by_size_df_new = gap_by_size_df.copy()\n",
    "gap_by_size_df_new['V/DB'] = gap_by_size_df_new['V'] / gap_by_size_df_new['DB']\n",
    "gap_by_size_df_new['max(G,V)/V+G'] = gap_by_size_df_new['max(G,V)'] / gap_by_size_df_new['V+G']\n",
    "display(gap_by_size_df_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 5: `all_gap_results_df`: complete gap closed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = selected_gap_df.index.levels[0]\n",
    "inst_set.set_names(\"Instance\",inplace=True)\n",
    "\n",
    "col_idx = pd.MultiIndex.from_arrays(\n",
    "    [\n",
    "        ['', '', '# cuts', '# cuts'] + ['% gap closed']*len(gap_cols_short),\n",
    "        ['Rows', 'Cols', 'G', 'V'] + gap_cols_short\n",
    "    ],\n",
    ")\n",
    "\n",
    "all_gap_results_df = pd.DataFrame(\n",
    "    columns = col_idx,\n",
    "    index = inst_set,\n",
    "    dtype = object,\n",
    ")\n",
    "\n",
    "# Enter number of rows and cols\n",
    "tmp_df = df.xs(0, level='disj_terms').loc[inst_set,[col_num_rows, col_num_cols]]\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[''],['Rows','Cols']])\n",
    "all_gap_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "# Enter number of cuts\n",
    "# tmp_df = best_gap_df.xs(0, level='disj_terms').loc[inst_set,['NUM GMIC', 'NUM VPC']]\n",
    "tmp_df = best_gap_df.loc[inst_set, [col_num_gmic, col_num_vpc]]\n",
    "tmp_df.columns = pd.MultiIndex.from_product([['# cuts'],['G','V']])\n",
    "all_gap_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "# Enter gap closed\n",
    "tmp_df = best_gap_df.loc[inst_set, gap_cols_short]\n",
    "tmp_df.columns = pd.MultiIndex.from_product([['% gap closed'],gap_cols_short])\n",
    "all_gap_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "# Add average row\n",
    "all_gap_results_df.loc[\"Average\"] = all_gap_results_df.loc[:,('% gap closed',gap_cols_short)].mean()\n",
    "\n",
    "# Now convert the % gap closed columns to objects so we can add an int row\n",
    "all_gap_results_df.loc[:,('% gap closed',gap_cols_short)] = all_gap_results_df.loc[:,('% gap closed',gap_cols_short)].astype(object)\n",
    "\n",
    "# Add wins row\n",
    "win_gap_cols_short = ['DB', 'V', 'V+G'] + solver_cut_type_short_list[1]\n",
    "all_gap_results_df.loc['Wins',('% gap closed',win_gap_cols_short)] = avg_gap_df.loc[all_set,win_gap_cols_short].values.tolist()\n",
    "# all_gap_results_df.loc['Wins',('% gap closed',win_gap_cols_short)] = avg_gap_df.loc[all_set,gap_cols_short].astype(np.int64).values.tolist()\n",
    "# all_gap_results_df.loc[\"Wins\"] = avg_gap_df.loc[all_set,gap_cols_short]\n",
    "# wins_df.at[cols[ind1],cols[ind2]] = int(sum(best_gap_df[cols[ind1]] > best_gap_df[cols[ind2]] + EPS))\n",
    "\n",
    "# Replace missing entries with empty string\n",
    "all_gap_results_df = all_gap_results_df.fillna('',downcast=False)\n",
    "\n",
    "# Convert rows, cols, # cuts to int values\n",
    "tmp_cols = pd.MultiIndex.from_product([[''],['Rows','Cols']])\n",
    "all_gap_results_df.loc[inst_set,tmp_cols] = all_gap_results_df.loc[inst_set,tmp_cols].astype(np.int64)\n",
    "tmp_cols = pd.MultiIndex.from_product([['# cuts'],['G','V']])\n",
    "all_gap_results_df.loc[inst_set,tmp_cols] = all_gap_results_df.loc[inst_set,tmp_cols].astype(np.int64)\n",
    "\n",
    "display(all_gap_results_df.tail())\n",
    "\n",
    "print(\"Num instances =\",len(all_gap_results_df)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Time tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import geometric_mean\n",
    "SHIFT_TIME  = 1\n",
    "SHIFT_GEN_TIME = 1\n",
    "SHIFT_NODES = 1000\n",
    "WIN_BY_TIME_FACTOR = 1.1\n",
    "WIN_BY_NODES_FACTOR = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables for row/col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare variables for row/col names\n",
    "\n",
    "# Row names\n",
    "bb_classes = ['All', '6 trees', 'Binary']\n",
    "num_bb_classes = len(bb_classes)\n",
    "\n",
    "bucket_min = [0, 10, 100, 1000]\n",
    "bucket_max = [3600, 3600, 3600, 3600]\n",
    "num_buckets = len(bucket_min)\n",
    "assert(len(bucket_max) == num_buckets)\n",
    "bb_buckets = ['[' + str(bucket_min[j]) + ',' + str(bucket_max[j]) + ')' for j in range(num_buckets)]\n",
    "# bucket_names = [classes[i] + ' [' + str(bucket_min[j]) + ',' + str(bucket_max[j]) + ')' for i in range(num_classes) for j in range(num_buckets)]\n",
    "# display(bucket_names)\n",
    "\n",
    "bb_metrics = ['Gmean', 'Wins']\n",
    "\n",
    "# Column names\n",
    "time_col_header = 'Time (s)'\n",
    "node_col_header = 'Nodes (#)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function `create_avg_bb_by_depth_df` that can be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_avg_bb_by_depth_df(inst_set, DEBUG = False):\n",
    "  bb_classes_by_depth = [str(t) + ' leaves' for t in sizes]\n",
    "  num_bb_classes_by_depth = len(bb_classes_by_depth)\n",
    "\n",
    "  bb_buckets_by_depth = bb_buckets\n",
    "  bb_metrics_by_depth = bb_metrics\n",
    "\n",
    "  cols_time_by_depth       = time_cols_long #[ref_time_col, refv_time_col, refv_w_cut_time_col]\n",
    "  shortcols_time_by_depth  = [map_cols_to_short_time[col] for col in cols_time_by_depth]\n",
    "  cols_nodes_by_depth      = [ref_nodes_col, refv_nodes_col]\n",
    "  shortcols_nodes_by_depth = [map_cols_to_short_nodes[col] for col in cols_nodes_by_depth]\n",
    "\n",
    "  avg_bb_cols_by_depth = pd.MultiIndex.from_arrays(\n",
    "      [\n",
    "        [time_col_header]*len(shortcols_time_by_depth) + \n",
    "        [node_col_header]*len(shortcols_nodes_by_depth), \n",
    "        shortcols_time_by_depth + shortcols_nodes_by_depth\n",
    "      ],\n",
    "      names = ['criterion', 'type'])\n",
    "\n",
    "  bb_row_names_by_depth = pd.MultiIndex.from_product(\n",
    "      [bb_classes_by_depth, bb_buckets_by_depth, bb_metrics_by_depth],\n",
    "      names=['class', 'bucket', 'metric'])\n",
    "\n",
    "  tmp_avg_bb_by_depth_df = pd.DataFrame(\n",
    "      columns = avg_bb_cols_by_depth,\n",
    "      index = bb_row_names_by_depth,\n",
    "      dtype = float\n",
    "  )\n",
    "\n",
    "  # Make all columns \"object\" type to allow for integer values\n",
    "  tmp_avg_bb_by_depth_df.loc[:,(time_col_header,shortcols_time_by_depth)] = tmp_avg_bb_by_depth_df.loc[:,(time_col_header,shortcols_time_by_depth)].astype(object)\n",
    "  tmp_avg_bb_by_depth_df.loc[:,(node_col_header,shortcols_nodes_by_depth)] = tmp_avg_bb_by_depth_df.loc[:,(node_col_header,shortcols_nodes_by_depth)].astype(object)\n",
    "\n",
    "  num_inst_by_depth = np.zeros(len(tmp_avg_bb_by_depth_df),dtype = np.int64)\n",
    "  row_ind = 0\n",
    "\n",
    "  # Calculate stats for instances by depth\n",
    "  cols = cols_time_by_depth + cols_nodes_by_depth + [col_num_vpc, ref_timeout_col, refv_timeout_col]\n",
    "  cols = list(set(cols)) # remove duplicates\n",
    "  curr_df = df.loc[inst_set,cols]\n",
    "  curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "  for curr_size_ind in range(0,len(bb_classes_by_depth)):\n",
    "      # print(\"{}\".format(bb_classes_by_depth[curr_size_ind]))\n",
    "      curr_by_depth_df = curr_df[curr_df.index.get_level_values(1) == sizes[curr_size_ind]] # take only chosen depth\n",
    "      \n",
    "      # Take only instances in which num vpcs > 0\n",
    "      curr_by_depth_df = curr_by_depth_df[curr_by_depth_df[col_num_vpc] > 0]\n",
    "\n",
    "      if DEBUG:\n",
    "          curr_by_depth_timeouts_df = curr_by_depth_df[\n",
    "              (curr_by_depth_df[ref_timeout_col] >= MAX_TIME)\n",
    "              & (curr_by_depth_df[refv_timeout_col] >= MAX_TIME)\n",
    "          ]\n",
    "          if (len(curr_by_depth_timeouts_df) > 0):\n",
    "            display(curr_by_depth_timeouts_df)\n",
    "\n",
    "      # Remove instances in which both Gurobi and VPCs timed out\n",
    "      # (If inst_set = selected_time_instances, then this is not necessary, as we already filtered out instances that timed out for both solvers)\n",
    "      curr_by_depth_df = curr_by_depth_df[\n",
    "          (curr_by_depth_df[ref_timeout_col] < MAX_TIME) \n",
    "          | (curr_by_depth_df[refv_timeout_col] < MAX_TIME)\n",
    "      ]\n",
    "\n",
    "      if DEBUG:\n",
    "          print(\"Num instances selected for depth {:d} = {:d}\".format(sizes[curr_size_ind],len(curr_by_depth_df)))\n",
    "          print(\"Num instances solved w/i timelimit for Gur: {:d}\".format(len(curr_by_depth_df[(curr_by_depth_df[ref_timeout_col] < MAX_TIME)])))\n",
    "          print(\"Num instances solved w/i timelimit for Gur+V: {:d}\".format(len(curr_by_depth_df[(curr_by_depth_df[refv_timeout_col] < MAX_TIME)])))\n",
    "\n",
    "      for bucket_ind in range(num_buckets):\n",
    "          # Decide which instances to include in this bucket\n",
    "          # This will be the instances for which the average solver time is at least the bucket minimum, for all solvers\n",
    "          # TODO: is it fair to use \"&\" or \"|\" here?\n",
    "          curr_by_depth_df = curr_by_depth_df[ \n",
    "              (curr_by_depth_df[ref_time_col] >= bucket_min[bucket_ind])\n",
    "              & (curr_by_depth_df[refv_time_col] >= bucket_min[bucket_ind])\n",
    "            ]\n",
    "          \n",
    "          bb_metric_ind = 0\n",
    "          # For each column, compute the geometric mean of the values in the column\n",
    "          # Use SHIFT_TIME for the first two time columns, and maybe use a different shift for generation time\n",
    "          for col_ind in range(len(cols_time_by_depth)):\n",
    "              col = cols_time_by_depth[col_ind]\n",
    "              SHIFT = SHIFT_TIME if col_ind < 2 else SHIFT_GEN_TIME\n",
    "              tmp_avg_bb_by_depth_df.loc[\n",
    "                  (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "                  (time_col_header,shortcols_time_by_depth[col_ind])] = \\\n",
    "              geometric_mean(curr_by_depth_df[col] + SHIFT) - SHIFT\n",
    "\n",
    "          # display(avg_bb_by_depth_df.loc[\n",
    "          #         (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[0]),\n",
    "          #         (time_col_header,shortcols_time_by_depth)].head())\n",
    "          tmp_avg_bb_by_depth_df.loc[\n",
    "                  (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "                  (node_col_header,shortcols_nodes_by_depth)] = \\\n",
    "              [geometric_mean(curr_by_depth_df[col] + SHIFT_NODES) - SHIFT_NODES for col in cols_nodes_by_depth]\n",
    "          \n",
    "          # print(\"row {:d}: {:d}\".format(row_ind,len(curr_by_depth_df)))\n",
    "\n",
    "          num_inst_by_depth[row_ind:row_ind+len(bb_metrics_by_depth)] = len(bb_metrics_by_depth)*[len(curr_by_depth_df)]\n",
    "          row_ind += len(bb_metrics_by_depth)\n",
    "\n",
    "          ## Update wins rows\n",
    "          # A win in terms of time is counted when the ``Gur'' baseline seconds taken \n",
    "          # is at least 10\\% slower, to account for some variability in runtimes.\n",
    "          bb_metric_ind = 1\n",
    "          refcol = ref_time_col\n",
    "          tmp_avg_bb_by_depth_df.loc[\n",
    "                  (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "                  (time_col_header,shortcols_time_by_depth)] = \\\n",
    "              [ int(sum( curr_by_depth_df[refv_time_col] > WIN_BY_TIME_FACTOR * curr_by_depth_df[ref_time_col] )),\n",
    "                int(sum( curr_by_depth_df[ref_time_col] > WIN_BY_TIME_FACTOR * curr_by_depth_df[refv_time_col] )),\n",
    "                int(sum( curr_by_depth_df[ref_time_col] > WIN_BY_TIME_FACTOR * (curr_by_depth_df[refv_time_col] + curr_by_depth_df[col_vpc_gen_time]) )),\n",
    "              ]\n",
    "\n",
    "          # A win in terms of nodes is when the ``Gur'' baseline number of nodes is higher.\n",
    "          refcol = ref_nodes_col\n",
    "          tmp_avg_bb_by_depth_df.loc[\n",
    "                  (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "                  (node_col_header,shortcols_nodes_by_depth)] = \\\n",
    "              [ int(sum(curr_by_depth_df[refv_nodes_col] > curr_by_depth_df[ref_nodes_col])) ] + \\\n",
    "              [ int(sum(curr_by_depth_df[refcol] > curr_by_depth_df[col])) for col in cols_nodes_by_depth[1:] ]\n",
    "\n",
    "  tmp_avg_bb_by_depth_df[inst_col_name] = num_inst_by_depth\n",
    "\n",
    "  # for i in range(num_buckets):\n",
    "  #     curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "      \n",
    "  display(tmp_avg_bb_by_depth_df.loc[(bb_classes_by_depth, bb_buckets_by_depth, bb_metrics_by_depth),:])\n",
    "\n",
    "  return tmp_avg_bb_by_depth_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: \"all\": `avg_bb_df_by_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = selected_time_instances # will be filtered down\n",
    "\n",
    "avg_bb_by_depth_df = create_avg_bb_by_depth_df(inst_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3 \"all 6\": `all6_avg_bb_by_depth` for `all6_instances`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = all6_instances\n",
    "\n",
    "all6_avg_bb_by_depth_df = create_avg_bb_by_depth_df(inst_set, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: \"binary\": `binary_avg_bb_df_by_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = binary_x_time_instances # will be filtered down\n",
    "\n",
    "binary_avg_bb_by_depth_df = create_avg_bb_by_depth_df(inst_set)\n",
    "\n",
    "# bb_classes_by_depth = [str(t) + ' leaves' for t in sizes]\n",
    "# num_bb_classes_by_depth = len(bb_classes_by_depth)\n",
    "\n",
    "# bb_buckets_by_depth = bb_buckets\n",
    "# bb_metrics_by_depth = bb_metrics\n",
    "\n",
    "# cols_time_by_depth       = time_cols_long #[ref_time_col, refv_time_col, refv_w_cut_time_col]\n",
    "# shortcols_time_by_depth  = [map_cols_to_short_time[col] for col in cols_time_by_depth]\n",
    "# cols_nodes_by_depth      = [ref_nodes_col, refv_nodes_col]\n",
    "# shortcols_nodes_by_depth = [map_cols_to_short_nodes[col] for col in cols_nodes_by_depth]\n",
    "\n",
    "# avg_bb_cols_by_depth = pd.MultiIndex.from_arrays(\n",
    "#     [\n",
    "#       [time_col_header]*len(shortcols_time_by_depth) + \n",
    "#       [node_col_header]*len(shortcols_nodes_by_depth), \n",
    "#       shortcols_time_by_depth + shortcols_nodes_by_depth\n",
    "#     ],\n",
    "#     names = ['criterion', 'type'])\n",
    "\n",
    "# bb_row_names_by_depth = pd.MultiIndex.from_product(\n",
    "#     [bb_classes_by_depth, bb_buckets_by_depth, bb_metrics_by_depth],\n",
    "#     names=['class', 'bucket', 'metric'])\n",
    "\n",
    "# binary_avg_bb_by_depth_df = pd.DataFrame(\n",
    "#     columns = avg_bb_cols_by_depth,\n",
    "#     index = bb_row_names_by_depth,\n",
    "#     dtype = float\n",
    "# )\n",
    "\n",
    "# # Make all columns \"object\" type to allow for integer values\n",
    "# binary_avg_bb_by_depth_df.loc[:,(time_col_header,shortcols_time_by_depth)] = avg_bb_by_depth_df.loc[:,(time_col_header,shortcols_time_by_depth)].astype(object)\n",
    "# binary_avg_bb_by_depth_df.loc[:,(node_col_header,shortcols_nodes_by_depth)] = avg_bb_by_depth_df.loc[:,(node_col_header,shortcols_nodes_by_depth)].astype(object)\n",
    "\n",
    "# num_inst_by_depth = np.zeros(len(avg_bb_by_depth_df),dtype = np.int64)\n",
    "# row_ind = 0\n",
    "\n",
    "# # Calculate stats for instances by depth\n",
    "# cols = cols_time_by_depth + cols_nodes_by_depth + [col_num_vpc, ref_timeout_col, refv_timeout_col]\n",
    "# cols = list(set(cols)) # remove duplicates\n",
    "# curr_df = df.loc[inst_set,cols]\n",
    "# curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# for curr_size_ind in range(0,len(bb_classes_by_depth)):\n",
    "#     # print(\"{}\".format(bb_classes_by_depth[curr_size_ind]))\n",
    "#     curr_by_depth_df = curr_df[curr_df.index.get_level_values(1) == sizes[curr_size_ind]] # take only chosen depth\n",
    "    \n",
    "#     # Take only instances in which num vpcs > 0\n",
    "#     curr_by_depth_df = curr_by_depth_df[curr_by_depth_df[col_num_vpc] > 0]\n",
    "\n",
    "#     # Remove instances in which both Gurobi and VPCs timed out\n",
    "#     curr_by_depth_df = curr_by_depth_df[\n",
    "#         (curr_by_depth_df[ref_timeout_col] < MAX_TIME)\n",
    "#         | (curr_by_depth_df[refv_timeout_col] < MAX_TIME)\n",
    "#     ]\n",
    "\n",
    "#     for bucket_ind in range(num_buckets):\n",
    "#         # Decide which instances to include in this bucket\n",
    "#         # This will be the instances for which the average solver time is at least the bucket minimum, for all solvers\n",
    "#         # TODO: is it fair to use \"&\" or \"|\" here?\n",
    "#         curr_by_depth_df = curr_by_depth_df[ \n",
    "#             (curr_by_depth_df[ref_time_col] >= bucket_min[bucket_ind])\n",
    "#             & (curr_by_depth_df[refv_time_col] >= bucket_min[bucket_ind])\n",
    "#           ]\n",
    "        \n",
    "#         bb_metric_ind = 0\n",
    "#         # For each column, compute the geometric mean of the values in the column\n",
    "#         # Use SHIFT_TIME for the first two time columns, and maybe use a different shift for generation time\n",
    "#         for col_ind in range(len(cols_time_by_depth)):\n",
    "#             col = cols_time_by_depth[col_ind]\n",
    "#             SHIFT = SHIFT_TIME if col_ind < 2 else SHIFT_GEN_TIME\n",
    "#             binary_avg_bb_by_depth_df.loc[\n",
    "#                 (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "#                 (time_col_header,shortcols_time_by_depth[col_ind])] = \\\n",
    "#             geometric_mean(curr_by_depth_df[col] + SHIFT) - SHIFT\n",
    "\n",
    "#         # display(avg_bb_by_depth_df.loc[\n",
    "#         #         (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[0]),\n",
    "#         #         (time_col_header,shortcols_time_by_depth)].head())\n",
    "#         binary_avg_bb_by_depth_df.loc[\n",
    "#                 (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "#                 (node_col_header,shortcols_nodes_by_depth)] = \\\n",
    "#             [geometric_mean(curr_by_depth_df[col] + SHIFT_NODES) - SHIFT_NODES for col in cols_nodes_by_depth]\n",
    "        \n",
    "#         # print(\"row {:d}: {:d}\".format(row_ind,len(curr_by_depth_df)))\n",
    "\n",
    "#         num_inst_by_depth[row_ind:row_ind+len(bb_metrics_by_depth)] = len(bb_metrics_by_depth)*[len(curr_by_depth_df)]\n",
    "#         row_ind += len(bb_metrics_by_depth)\n",
    "\n",
    "#         ## Update wins rows\n",
    "#         # A win in terms of time is counted when the ``Gur'' baseline seconds taken \n",
    "#         # is at least 10\\% slower, to account for some variability in runtimes.\n",
    "#         bb_metric_ind = 1\n",
    "#         refcol = ref_time_col\n",
    "#         binary_avg_bb_by_depth_df.loc[\n",
    "#                 (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "#                 (time_col_header,shortcols_time_by_depth)] = \\\n",
    "#             [ int(sum( curr_by_depth_df[refv_time_col] > WIN_BY_TIME_FACTOR * curr_by_depth_df[ref_time_col] )),\n",
    "#               int(sum( curr_by_depth_df[ref_time_col] > WIN_BY_TIME_FACTOR * curr_by_depth_df[refv_time_col] )),\n",
    "#               int(sum( curr_by_depth_df[ref_time_col] > WIN_BY_TIME_FACTOR * (curr_by_depth_df[refv_time_col] + curr_by_depth_df[col_vpc_gen_time]) )),\n",
    "#             ]\n",
    "\n",
    "#         # A win in terms of nodes is when the ``Gur'' baseline number of nodes is higher.\n",
    "#         refcol = ref_nodes_col\n",
    "#         binary_avg_bb_by_depth_df.loc[\n",
    "#                 (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[bucket_ind], bb_metrics_by_depth[bb_metric_ind]),\n",
    "#                 (node_col_header,shortcols_nodes_by_depth)] = \\\n",
    "#             [ int(sum(curr_by_depth_df[refv_nodes_col] > curr_by_depth_df[ref_nodes_col])) ] + \\\n",
    "#             [ int(sum(curr_by_depth_df[refcol] > curr_by_depth_df[col])) for col in cols_nodes_by_depth[1:] ]\n",
    "\n",
    "# binary_avg_bb_by_depth_df[inst_col_name] = num_inst_by_depth\n",
    "\n",
    "# # for i in range(num_buckets):\n",
    "# #     curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "# display(binary_avg_bb_by_depth_df.loc[(bb_classes_by_depth, bb_buckets_by_depth, bb_metrics_by_depth),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute `best_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_time_by_depth = time_cols_long #[ref_time_col, refv_time_col, refv_w_cut_time_col]\n",
    "cols_nodes_by_depth = [ref_nodes_col, refv_nodes_col]\n",
    "\n",
    "curr_df = df.loc[selected_time_instances, [col_num_vpc] + cols_time_by_depth + cols_nodes_by_depth]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# Calculate minimum value for each column, per instance (index level 0)\n",
    "curr_argmin_df = curr_df.groupby(level=0).idxmin()\n",
    "best_df = curr_df.loc[curr_argmin_df[refv_time_col]]\n",
    "\n",
    "# Sort by increasing value of cols_times_by_depth[1\n",
    "print(\"Sorting by increasing value of\",cols_time_by_depth[1],\"...\")\n",
    "best_df = best_df.sort_values(by=cols_time_by_depth[1])\n",
    "\n",
    "best_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'disj_terms' (second level of index) to column\n",
    "best_df.reset_index(level=1, inplace=True)\n",
    "best_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary metrics: shifted geometric mean and wins for ref_time_col, refv_time_col, ref_nodes_col, and refv_nodes_col\n",
    "\n",
    "# Set up best_df_summary_metrics_df to store the summary metrics\n",
    "best_df_summary_metrics_df = best_df.copy(deep=True)\n",
    "\n",
    "# Compute shifted geometric mean\n",
    "cols_for_shifted_time_gmean = [col for col in best_df.columns if 'TIME' in col]\n",
    "cols_for_shifted_nodes_gmean = [col for col in best_df.columns if 'NODES' in col]\n",
    "\n",
    "# Apply shift to each column\n",
    "for col in cols_for_shifted_time_gmean:\n",
    "    best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] + SHIFT_TIME\n",
    "for col in cols_for_shifted_nodes_gmean:\n",
    "    best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] + SHIFT_NODES\n",
    "\n",
    "# Compute shifted geometric mean for time\n",
    "best_df_summary_metrics_df.loc['Gmean'] = best_df_summary_metrics_df[cols_for_shifted_time_gmean+cols_for_shifted_nodes_gmean].apply(geometric_mean, axis=0)\n",
    "\n",
    "# Change shift back\n",
    "for col in cols_for_shifted_time_gmean:\n",
    "    best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] - SHIFT_TIME\n",
    "for col in cols_for_shifted_nodes_gmean:\n",
    "    best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] - SHIFT_NODES\n",
    "best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_time_gmean] = best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_time_gmean] - SHIFT_TIME\n",
    "best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_nodes_gmean] = best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_nodes_gmean] - SHIFT_NODES\n",
    "\n",
    "# Compute wins for ref_time_col, refv_time_col, ref_nodes_col, and refv_nodes_col\n",
    "best_df_summary_metrics_df.loc['Wins', cols_for_shifted_time_gmean] = \\\n",
    "    [ \n",
    "      int(sum( best_df_summary_metrics_df[refv_time_col] > WIN_BY_TIME_FACTOR * best_df_summary_metrics_df[ref_time_col] )),\n",
    "      int(sum( best_df_summary_metrics_df[ref_time_col] > WIN_BY_TIME_FACTOR * best_df_summary_metrics_df[refv_time_col] )),\n",
    "      int(sum( best_df_summary_metrics_df[ref_time_col] > WIN_BY_TIME_FACTOR * (best_df_summary_metrics_df[refv_time_col] + best_df_summary_metrics_df[col_vpc_gen_time]) )),\n",
    "    ]\n",
    "\n",
    "best_df_summary_metrics_df.loc['Wins', cols_for_shifted_nodes_gmean] = \\\n",
    "    [ \n",
    "      int(sum(best_df_summary_metrics_df[refv_nodes_col] > best_df_summary_metrics_df[ref_nodes_col])),\n",
    "      int(sum(best_df_summary_metrics_df[ref_nodes_col] > best_df_summary_metrics_df[refv_nodes_col]))\n",
    "    ]\n",
    "\n",
    "best_df_summary_metrics_df.loc[['Gmean','Wins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put short cols in place of orig_cols\n",
    "orig_cols = best_df_summary_metrics_df.columns\n",
    "\n",
    "new_orig_cols = [col for col in orig_cols if ('TIME' not in col) and ('NODES' not in col)]\n",
    "new_orig_cols = ['# terms', '# cuts']\n",
    "new_time_cols = [map_cols_to_short_time[col] for col in orig_cols if 'TIME' in col]\n",
    "new_nodes_cols = [map_cols_to_short_nodes[col] for col in orig_cols if 'NODES' in col]\n",
    "\n",
    "display(new_orig_cols + new_time_cols + new_nodes_cols)\n",
    "\n",
    "# Add second level to columns\n",
    "new_orig_cols = pd.MultiIndex.from_product(\n",
    "  [[''],new_orig_cols],\n",
    ")\n",
    "new_time_cols = pd.MultiIndex.from_product(\n",
    "  [['Time (s)'],new_time_cols],\n",
    ")\n",
    "new_nodes_cols = pd.MultiIndex.from_product(\n",
    "  [['Nodes (#)'],new_nodes_cols],\n",
    ")\n",
    "\n",
    "best_df_summary_metrics_df.columns = new_orig_cols.append(new_time_cols).append(new_nodes_cols)\n",
    "best_df_summary_metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: \"Combined\" results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select \"Combined\" instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_nodes dataframe, in which each row is an instance, and each column is a solver (with or w/o cuts, and with a fixed random seed)\n",
    "# These will be pulled from 'ALL REF TIME' and 'ALL REF+V TIME' columns\n",
    "inst_set = list(instances)\n",
    "# inst_set = ['lotsize_presolved']\n",
    "PRINT_SKIP_REASON = False\n",
    "\n",
    "disjset_num_gap_errors = 0\n",
    "\n",
    "selected_disjset_gap_instances_dict = {} # dictionary of (original index, instance)\n",
    "selected_disjset_time_instances_dict = {} # dictionary of (original index, instance)\n",
    "for i, inst in enumerate(inst_set):\n",
    "  print(\"{}/{}\".format(i+1,len(inst_set)), end='\\r', flush=True)\n",
    "  skip_instance = False\n",
    "\n",
    "  # # Check that -1 depth exists\n",
    "  # if not (-1 in df.loc[inst].index):\n",
    "  #   # if PRINT_SKIP_REASON:\n",
    "  #   print(\"Skipping instance {:d} -- {}: no -1 depth.\".format(\n",
    "  #       i, inst\n",
    "  #   ))\n",
    "  #   skip_instance = True\n",
    "  #   continue\n",
    "  if not inst in df_disjset.index:\n",
    "    if PRINT_SKIP_REASON:\n",
    "      print(\"Skipping instance {:d} -- {}: no disjset results.\".format(\n",
    "          i, inst\n",
    "      ))\n",
    "    skip_instance = True\n",
    "    continue\n",
    "  \n",
    "  curr_df = df_disjset.loc[(inst,-1)]\n",
    "\n",
    "  # Ensure nrows and ncols is not too many\n",
    "  nrows = curr_df[col_num_rows]\n",
    "  ncols = curr_df[col_num_cols]\n",
    "  if (nrows > MAX_ROWS) or (ncols > MAX_COLS):\n",
    "    if PRINT_SKIP_REASON:\n",
    "        print(\"Skipping instance {:d} -- {}: nrows = {:d} > {:d} or ncols = {:d} > {:d}.\".format(\n",
    "                i, inst, nrows, ncols, MAX_ROWS, MAX_COLS))\n",
    "    skip_instance = True\n",
    "\n",
    "  # Ensure IP objective value is known\n",
    "  ip_obj = np.float64(df_ipopt.loc[inst,col_ip_obj])\n",
    "  if not isinstance(ip_obj,float):\n",
    "    if PRINT_SKIP_REASON:\n",
    "        print(\n",
    "            \"Skipping instance {:d} -- {}: IP objective value ({}) is not detected to be a float value.\".format(\n",
    "            i, inst, ip_obj))\n",
    "    skip_instance = True\n",
    "\n",
    "  check_ip_obj = curr_df[col_ip_obj]\n",
    "  if not is_val(ip_obj,check_ip_obj):\n",
    "    print(\"*** ERROR: Instance {:d} -- {}: IP objective value ({}) does not match value in dataframe ({}). REPLACING WITH KNOWN VALUE!\".format(\n",
    "        i, inst, ip_obj, check_ip_obj))\n",
    "    df.loc[(inst,-1),col_ip_obj] = ip_obj\n",
    "    df_disjset.loc[(inst,-1),col_ip_obj] = ip_obj\n",
    "    # skip_instance = True\n",
    "\n",
    "  # Check that LP opt < IP opt\n",
    "  lp_obj = np.float64(df_preprocess.loc[remove_presolved_from_name(inst),col_cleaned_lp_obj])\n",
    "  YES_GAP = (ip_obj - lp_obj) >= EPS\n",
    "  if not YES_GAP:\n",
    "    print(\"*** ERROR: Instance {:d} -- {}: not YES GAP (lp = {:.10f}; ip = {:.10f}, diff = {:.2f})\".format(i, inst, lp_obj, ip_obj, ip_obj-lp_obj))\n",
    "    skip_instance = True\n",
    "    disjset_num_gap_errors += 1\n",
    "\n",
    "  # Check that ExitReason != OPTIMAL_SOLUTION_FOUND\n",
    "  exitreason = curr_df['ExitReason']\n",
    "  if exitreason == 'OPTIMAL_SOLUTION_FOUND':\n",
    "    if PRINT_SKIP_REASON:\n",
    "      print(\"Skipping instance {:d} -- {}: optimal IP solution found.\".format(\n",
    "          i, inst\n",
    "      ))\n",
    "    skip_instance = True\n",
    "\n",
    "  # Check that VPCs were generated\n",
    "  num_vpc = curr_df[col_num_vpc]\n",
    "  if num_vpc == 0:\n",
    "    if PRINT_SKIP_REASON:\n",
    "      print(\"Skipping instance {:d} -- {}: no VPCs generated.\".format(\n",
    "          i, inst\n",
    "      ))\n",
    "    skip_instance = True\n",
    "\n",
    "  if not skip_instance:\n",
    "    selected_disjset_gap_instances_dict[inst] = i\n",
    "\n",
    "    # If either ref or refv times are < MAX_TIME - EPS, include in nodes experiments\n",
    "    ref_time = curr_df[ref_time_col]\n",
    "    refv_time = curr_df[refv_time_col]\n",
    "    if (ref_time < MAX_TIME - EPS) or (refv_time < MAX_TIME - EPS):\n",
    "      selected_disjset_time_instances_dict[inst] = i\n",
    "\n",
    "selected_disjset_gap_instances = list(selected_disjset_gap_instances_dict.keys())\n",
    "print(\"Total number of errors: {}\".format(disjset_num_gap_errors))\n",
    "print(\"Num instances selected for disjset gap closed results = {:d}\".format(len(selected_disjset_gap_instances)))\n",
    "\n",
    "selected_disjset_time_instances = list(selected_disjset_time_instances_dict.keys())\n",
    "print(\"Num instances selected for disjset time results = {:d}\".format(len(selected_disjset_time_instances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List instances that are in exactly one of the two sets `selected_gap_instances` and `selected_disjset_gap_instances`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric difference between two sets selected_gap_instances and selected_disjset_gap_instances\n",
    "print(\"Instances in selected_disjset_gap_instances that are not in selected_gap_instances:\")\n",
    "newly_selected_instances = [inst for inst in selected_disjset_gap_instances if inst not in selected_gap_instances]\n",
    "print(newly_selected_instances)\n",
    "\n",
    "# Remove newly selected instances\n",
    "for inst in newly_selected_instances:\n",
    "  selected_disjset_gap_instances.remove(inst)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_gap_instances that are not in selected_disjset_gap_instances:\")\n",
    "print([inst for inst in selected_gap_instances if inst not in selected_disjset_gap_instances])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_time_instances that are not in selected_disjset_time_instances:\")\n",
    "print([inst for inst in selected_time_instances if inst not in selected_disjset_time_instances])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_disjset_time_instances that are not in selected_time_instances:\")\n",
    "print([inst for inst in selected_disjset_time_instances if inst not in selected_time_instances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Combined\" gap summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `disjset_gap_df`: Analyze disjset gap closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of dataframe relevant to gap closed\n",
    "disjset_gap_df = df.loc[selected_disjset_gap_instances, \n",
    "                [\n",
    "                    col_num_disj_terms,\n",
    "                    col_num_rows,\n",
    "                    col_num_cols\n",
    "                ]\n",
    "                +\n",
    "                obj_val_col_list\n",
    "                +\n",
    "                [\n",
    "                    col_num_gmic,\n",
    "                    col_num_vpc,\n",
    "                    col_num_obj,\n",
    "                    col_exit_reason\n",
    "                ]\n",
    "               ]\n",
    "\n",
    "# Take only -1 depth\n",
    "disjset_gap_df = disjset_gap_df.xs(-1, level='disj_terms')\n",
    "\n",
    "# Calculate some missing % gap closed columns\n",
    "# gap closed = 100 * (post_cut_opt_val - lp_opt_val) / (ip_opt_val - lp_opt_val)\n",
    "for cut_type in reg_cut_type_long_list:\n",
    "    col = cut_type + ' ' + obj_stub\n",
    "    if col not in df.columns:\n",
    "        if cut_type == 'MAX(GMIC,VPC)':\n",
    "            # Add max(G,V) column\n",
    "            disjset_gap_df[cut_type + ' ' + pct_gap_closed_stub] = \\\n",
    "                np.maximum(\n",
    "                    disjset_gap_df['GMIC' + ' ' + pct_gap_closed_stub],\n",
    "                    disjset_gap_df[ 'VPC' + ' ' + pct_gap_closed_stub]\n",
    "                )\n",
    "        continue\n",
    "    disjset_gap_df[cut_type + ' ' + pct_gap_closed_stub] = calc_gap_closed(disjset_gap_df, col)\n",
    "\n",
    "# Compare against reference solver\n",
    "for stat_type in solver_stat_list[:3]:\n",
    "    for solver_type in solver_stubs:\n",
    "        for cut_type in solver_cut_type_stubs:\n",
    "            col = stat_type + ' ' + solver_type + ' ' + cut_type\n",
    "            disjset_gap_df[col + ' ' + pct_gap_closed_stub] = calc_gap_closed(disjset_gap_df, col)\n",
    "\n",
    "# Rename each column with its shortname\n",
    "disjset_gap_df.rename(columns=map_cols_to_short_gap,inplace=True)\n",
    "\n",
    "display(disjset_gap_df.loc[['bm23_presolved','maxgasflow_presolved']][gap_cols_short])\n",
    "display(disjset_gap_df.loc[(\"bm23_presolved\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disjset_gap_df.to_csv('disjset_gap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `disjset_avg_gap_df`: summary of results with disjset instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE 1.2: average percent gap closed across different combinations of cuts\n",
    "## Create disjset_avg_gap_df = average gap closed across instances\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [ [all_set_name, good_vpc_set_name, binary_set_name], [avg_row_name, wins_row_name] ],\n",
    "    names = ['Set', '']\n",
    ")\n",
    "    \n",
    "ncols = len(gap_cols_short)\n",
    "nrows = len(idx)\n",
    "\n",
    "col = disjset_gap_df['V'].astype(float)\n",
    "disjset_good_vpc_df = disjset_gap_df[col >= 10.]\n",
    "disjset_binary_instances_df = disjset_gap_df.loc[ [inst for inst in pure_binary_instances if inst in selected_disjset_gap_instances ] ]\n",
    "\n",
    "data = np.zeros((nrows, ncols), dtype=float)\n",
    "data[0,:] = [disjset_gap_df[col].mean() for col in gap_cols_short]\n",
    "data[2,:] = [disjset_good_vpc_df[col].mean() for col in gap_cols_short]\n",
    "data[4,:] = [disjset_binary_instances_df[col].mean() for col in gap_cols_short]\n",
    "\n",
    "# display(best_gap_df.head())\n",
    "disjset_avg_gap_df = pd.DataFrame(\n",
    "    data,\n",
    "    columns = gap_cols_short,\n",
    "    index = idx,\n",
    "    dtype = object\n",
    ")\n",
    "\n",
    "inst_col_name = '# inst'\n",
    "disjset_avg_gap_df[inst_col_name] = [len(disjset_gap_df), 0, len(disjset_good_vpc_df), 0, len(disjset_binary_instances_df), 0]\n",
    "\n",
    "disjset_avg_gap_df.iloc[1] = [\"\" for i in range(ncols+1)]\n",
    "disjset_avg_gap_df.iloc[3] = [\"\" for i in range(ncols+1)]\n",
    "disjset_avg_gap_df.iloc[5] = [\"\" for i in range(ncols+1)]\n",
    "\n",
    "display(disjset_avg_gap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `disjset_wins_df`: number of wins across methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create num wins df\n",
    "# x wins over y for an instance if x > y + EPS\n",
    "#shortcols = avg_gap_df.columns[0:-1]\n",
    "disjset_wins_df = pd.DataFrame(\n",
    "    np.zeros((len(gap_cols_short), len(gap_cols_short)), dtype=int),\n",
    "    columns = gap_cols_short,\n",
    "    index = gap_cols_short,\n",
    "    dtype = int,\n",
    ")\n",
    "\n",
    "WINS_EPS = GAP_DIFF_EPS\n",
    "\n",
    "from itertools import permutations\n",
    "for (ind1, ind2) in permutations(range(len(gap_cols_short)), 2):\n",
    "    disjset_wins_df.at[gap_cols_short[ind1],gap_cols_short[ind2]] =\\\n",
    "        int(sum(disjset_gap_df[gap_cols_short[ind1]] > disjset_gap_df[gap_cols_short[ind2]] + WINS_EPS))\n",
    "    disjset_wins_df.at[gap_cols_short[ind2],gap_cols_short[ind1]] =\\\n",
    "        int(sum(disjset_gap_df[gap_cols_short[ind2]] > disjset_gap_df[gap_cols_short[ind1]] + WINS_EPS))\n",
    "\n",
    "# Sets we are considering\n",
    "# all_set = 'Wins (All)'\n",
    "# good_vpc_set = 'Wins (V ≥ 10%)'\n",
    "disjset_all_set = (all_set_name,wins_row_name)\n",
    "disjset_good_vpc_set = (good_vpc_set_name,wins_row_name)\n",
    "disjset_binary_set = (binary_set_name,wins_row_name)\n",
    "\n",
    "# \"G\" are wins relative to \"V\"\n",
    "shortrefcol = 'V'\n",
    "#refcol = 'VPC % GAP CLOSED'\n",
    "#refcol = map_short_to_cols[shortrefcol]\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'G'\n",
    "#col = 'GMIC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = disjset_wins_df.at[shortdestcol,shortrefcol]\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > disjset_good_vpc_df[refcol] + WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > disjset_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"DB\", \"V\", \"V+G\": wins are relative to \"G\"\n",
    "shortrefcol = 'G'\n",
    "#refcol = 'GMIC % GAP CLOSED'\n",
    "#refcol = map_short_to_cols[shortrefcol]\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'DB'\n",
    "#col = 'BEST DISJ % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = disjset_wins_df.at[shortdestcol,shortrefcol]\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > disjset_good_vpc_df[refcol] + WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > disjset_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "shortdestcol = 'V'\n",
    "#col = 'VPC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = disjset_wins_df.at[shortdestcol,shortrefcol]\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > disjset_good_vpc_df[refcol] + WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > disjset_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "shortdestcol = 'V+G'\n",
    "#col = 'VPC+GMIC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = disjset_wins_df.at[shortdestcol,shortrefcol]\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > disjset_good_vpc_df[refcol] + WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > disjset_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"V+GurF\" are wins relative to \"GurF\"\n",
    "shortrefcol = 'GurF'\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'V+GurF'\n",
    "destcol = shortdestcol\n",
    "#col = map_short_to_cols[shortcol]\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = disjset_wins_df.at[shortdestcol,shortrefcol]\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > disjset_good_vpc_df[refcol] + WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > disjset_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"V+GurL\" are wins relative to \"GurL\"\n",
    "shortrefcol = 'GurL'\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'V+GurL'\n",
    "destcol = shortdestcol\n",
    "disjset_wins_df.at[shortdestcol,shortrefcol] = int(sum(disjset_gap_df[destcol] > disjset_gap_df[refcol] + WINS_EPS))\n",
    "disjset_wins_df.at[shortrefcol,shortdestcol] = int(sum(disjset_gap_df[refcol] > disjset_gap_df[destcol] + WINS_EPS))\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = disjset_wins_df.at[shortdestcol,shortrefcol]\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > disjset_good_vpc_df[refcol] + WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > disjset_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# Count number of instances that have V+G > 0\n",
    "shortdestcol = inst_col_name\n",
    "#col = 'V+GurL'\n",
    "destcol = 'V+G'\n",
    "disjset_avg_gap_df.at[disjset_all_set,shortdestcol] = sum(disjset_gap_df[destcol] > WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_good_vpc_set,shortdestcol] = sum(disjset_good_vpc_df[destcol] > WINS_EPS)\n",
    "disjset_avg_gap_df.at[disjset_binary_set,shortdestcol] = sum(disjset_binary_instances_df[destcol] > WINS_EPS)\n",
    "\n",
    "display(disjset_avg_gap_df)\n",
    "display(disjset_wins_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Table 2 `gap_by_size_df` with \"Combined\" results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new row with label \"Combined\"\n",
    "all_row = pd.DataFrame(columns=gap_by_size_df.columns, index=[\"Combined\"])\n",
    "\n",
    "# Calculate the average values for each column from the `disjset` dataframe\n",
    "all_row = disjset_avg_gap_df.loc['All',gap_by_size_df.columns][0:1].to_numpy()\n",
    "\n",
    "# Append the new row to the `gap_by_size` dataframe\n",
    "gap_by_size_df.loc['Combined'] = all_row[0]\n",
    "\n",
    "# Display the updated `gap_by_size` dataframe\n",
    "display(gap_by_size_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Combined\" timing summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `disjset_timing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = selected_disjset_time_instances\n",
    "NUM_SEEDS = 7\n",
    "disjset_timing_cols = \\\n",
    "    ['ALL' + ' ' + solver_type + ' ' + time_stub for solver_type in solver_stubs] \\\n",
    "        + \\\n",
    "        time_cols_long \\\n",
    "        + \\\n",
    "        [col_num_vpc]\n",
    "new_disjset_timing_cols = [ \n",
    "        [\n",
    "            solver_type + ' ' + '(%d)' % seed + ' ' + time_stub\n",
    "            for seed in range(1,NUM_SEEDS+1)\n",
    "        ]\n",
    "        for solver_type in solver_stubs\n",
    "    ]\n",
    "new_disjset_wins_cols = [\n",
    "    \"Wins ({} over {})\".format(solver_stubs[0], solver_stubs[1]),\n",
    "    \"Wins ({} over {})\".format(solver_stubs[1], solver_stubs[0])\n",
    "]\n",
    "disjset_timing_df = df_disjset.loc[inst_set, disjset_timing_cols]\n",
    "# display(disjet_timing_df.head())\n",
    "\n",
    "# Add columns solver_type (%d) + ' ' + time_stub for the 7 random seeds\n",
    "# These will be parsed from the ALL REF TIME and ALL REF+V TIME columns,\n",
    "# which contain semicolon-separated values\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    for solver_ind in range(len(solver_stubs)):\n",
    "        solver_type = solver_stubs[solver_ind]\n",
    "        orig_col = disjset_timing_cols[solver_ind]\n",
    "        new_col = new_disjset_timing_cols[solver_ind][seed_ind]\n",
    "        disjset_timing_df[new_col] = disjset_timing_df[orig_col].str.split(';').str[seed_ind]\n",
    "        disjset_timing_df[new_col] = disjset_timing_df[new_col].astype(float)\n",
    "\n",
    "# Add wins columns\n",
    "for solver_ind in range(len(solver_stubs)):\n",
    "    other_solver_ind = 1 - solver_ind\n",
    "\n",
    "    disjset_timing_df[new_disjset_wins_cols[solver_ind]] = sum(\n",
    "        disjset_timing_df[new_disjset_timing_cols[other_solver_ind][seed_ind]]\n",
    "        > \n",
    "        WIN_BY_TIME_FACTOR * disjset_timing_df[new_disjset_timing_cols[solver_ind][seed_ind]]\n",
    "        for seed_ind in range(NUM_SEEDS)\n",
    "    )\n",
    "\n",
    "display(disjset_timing_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disjset_timing_df.to_csv(\"disjet_timing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `disjset_time_geomean_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row names\n",
    "disjset_bb_classes = ['Combined', 'Combined-Binary']\n",
    "disjset_bb_classes_lists = [ selected_disjset_time_instances, [inst for inst in pure_binary_instances if inst in selected_disjset_time_instances] ]\n",
    "disjset_num_bb_classes = len(disjset_bb_classes)\n",
    "\n",
    "# Rows are geomean and wins for each bucket\n",
    "disjset_bb_row_names = pd.MultiIndex.from_product(\n",
    "    [disjset_bb_classes, bb_buckets, bb_metrics],\n",
    "    names=['class', 'bucket', 'metric'])\n",
    "disjset_time_geomean_cols = [\n",
    "    [\n",
    "        new_disjset_timing_cols[solver_ind][seed_ind] \n",
    "        for seed_ind in range(NUM_SEEDS) \n",
    "        for solver_ind in range(len(solver_stubs))\n",
    "    ]\n",
    "    + time_cols_long\n",
    "    + new_disjset_wins_cols\n",
    "]\n",
    "disjset_time_geomean_df = pd.DataFrame(\n",
    "    columns = disjset_time_geomean_cols,\n",
    "    index = disjset_bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "disjset_time_geomean_df.loc[:,(disjset_time_geomean_cols[0])] = disjset_time_geomean_df.loc[:,(disjset_time_geomean_cols[0])].astype(object)\n",
    "\n",
    "# Prepare num_inst columns\n",
    "disjset_num_inst_by_seed_bucket_class = [\n",
    "    np.zeros(len(disjset_time_geomean_df),dtype = np.int64)\n",
    "    for _ in range(NUM_SEEDS) \n",
    "    # for _ in range(num_buckets) \n",
    "    # for _ in range(disjset_num_bb_classes)\n",
    "]\n",
    "disjset_num_inst_avg = np.zeros(len(disjset_time_geomean_df),dtype = np.int64)\n",
    "disjset_num_ref_seed_wins = np.zeros(len(disjset_time_geomean_df),dtype = np.int64)\n",
    "disjset_num_refv_seed_wins = np.zeros(len(disjset_time_geomean_df),dtype = np.int64)\n",
    "\n",
    "# For each class, calculate geomean\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    row_ind = 0\n",
    "    seed_cols = [ new_disjset_timing_cols[solver_ind][seed_ind] for solver_ind in range(len(solver_stubs)) ]\n",
    "    \n",
    "    for class_ind in range(disjset_num_bb_classes):\n",
    "        print(\"\\nClass: {}\".format(disjset_bb_classes[class_ind]))\n",
    "        curr_df = disjset_timing_df.loc[disjset_bb_classes_lists[class_ind]]\n",
    "        seed_curr_df = curr_df[seed_cols]\n",
    "\n",
    "        # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply by TIMEOUT_TIME_FACTOR\n",
    "        for col_ind in range(len(seed_cols)):\n",
    "            time_col = seed_cols[col_ind]\n",
    "            seed_curr_df.loc[seed_curr_df[time_col] >= MAX_TIME - EPS, time_col] = TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "\n",
    "        for bucket_ind in range(num_buckets):\n",
    "            print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "            # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "            seed_curr_df = seed_curr_df[\n",
    "                (seed_curr_df[seed_cols].min(axis=1) >= bucket_min[bucket_ind])\n",
    "                & (seed_curr_df[seed_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "            ]\n",
    "\n",
    "            # Calculate geomean\n",
    "            metric_ind = 0\n",
    "            disjset_time_geomean_df.loc[\n",
    "                (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                seed_cols\n",
    "            ] = [\n",
    "                    geometric_mean(seed_curr_df[col] + SHIFT_TIME) - SHIFT_TIME\n",
    "                    for col in seed_cols\n",
    "                ]\n",
    "            \n",
    "            # Calculate wins\n",
    "            metric_ind = 1\n",
    "            for solver_ind in range(len(solver_stubs)):\n",
    "                other_solver_ind = 1 - solver_ind\n",
    "                disjset_time_geomean_df.loc[\n",
    "                    (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                    seed_cols[solver_ind]\n",
    "                ] = sum(\n",
    "                    seed_curr_df[seed_cols[other_solver_ind]]\n",
    "                    > \n",
    "                    WIN_BY_TIME_FACTOR * seed_curr_df[seed_cols[solver_ind]]\n",
    "                )\n",
    "\n",
    "            print(\"row {:d}: {:d}\".format(row_ind,len(seed_curr_df)))\n",
    "            disjset_num_inst_by_seed_bucket_class[seed_ind][row_ind:row_ind+len(bb_metrics)] = [len(seed_curr_df)] * len(bb_metrics)\n",
    "\n",
    "            row_ind += len(bb_metrics)\n",
    "\n",
    "            # for solver_ind in range(len(solver_stubs)):\n",
    "            #     other_solver_ind = 1 - solver_ind\n",
    "            #     disjset_time_geomean_df.loc[\n",
    "            #         (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "            #         new_disjset_wins_cols[solver_ind]\n",
    "            #     ] = sum(\n",
    "            #         seed_curr_df[new_disjset_timing_cols[other_solver_ind][seed_ind]]\n",
    "            #         > \n",
    "            #         WIN_BY_TIME_FACTOR * seed_curr_df[new_disjset_timing_cols[solver_ind][seed_ind]]\n",
    "            #     )\n",
    "\n",
    "\n",
    "row_ind = 0\n",
    "only_time_cols = [ ref_time_col, refv_time_col ]\n",
    "cols = time_cols_long\n",
    "wins_cols = new_disjset_wins_cols\n",
    "for class_ind in range(disjset_num_bb_classes):\n",
    "    print(\"\\nClass: {}\".format(disjset_bb_classes[class_ind]))\n",
    "    curr_df = disjset_timing_df.loc[disjset_bb_classes_lists[class_ind]]\n",
    "\n",
    "    # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply by TIMEOUT_TIME_FACTOR\n",
    "    for col_ind in range(len(only_time_cols)):\n",
    "        time_col = only_time_cols[col_ind]\n",
    "        curr_df.loc[curr_df[time_col] >= MAX_TIME - EPS, time_col] = TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "\n",
    "    for bucket_ind in range(num_buckets):\n",
    "        print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "        # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "        curr_df = curr_df[\n",
    "            (curr_df[only_time_cols].min(axis=1) >= bucket_min[bucket_ind])\n",
    "            & (curr_df[only_time_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "        ]\n",
    "\n",
    "        # Calculate geomean\n",
    "        metric_ind = 0\n",
    "        disjset_time_geomean_df.loc[\n",
    "            (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "            cols\n",
    "        ] = [\n",
    "                geometric_mean(curr_df[col] + SHIFT_TIME) - SHIFT_TIME\n",
    "                for col in cols\n",
    "            ]\n",
    "        \n",
    "        # Calculate wins\n",
    "        metric_ind = 1\n",
    "        for solver_ind in range(len(solver_stubs)):\n",
    "            other_solver_ind = 1 - solver_ind\n",
    "            disjset_time_geomean_df.loc[\n",
    "                (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                only_time_cols[solver_ind]\n",
    "            ] = sum(\n",
    "                curr_df[only_time_cols[other_solver_ind]]\n",
    "                > \n",
    "                WIN_BY_TIME_FACTOR * curr_df[only_time_cols[solver_ind]]\n",
    "            )\n",
    "\n",
    "            # Also report average number of seeds for which REF and REF+V win\n",
    "            disjset_time_geomean_df.loc[\n",
    "                (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[0:len(bb_metrics)]),\n",
    "                wins_cols[solver_ind]\n",
    "            ] = sum(curr_df[wins_cols[solver_ind]]) / len(curr_df)\n",
    "\n",
    "        print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "        disjset_num_inst_avg[row_ind:row_ind+len(bb_metrics)] = [len(curr_df)] * len(bb_metrics)\n",
    "\n",
    "        row_ind += len(bb_metrics)\n",
    "\n",
    "# Insert columns with # inst (seed)\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    disjset_time_geomean_df.insert(\n",
    "        2 + seed_ind * (len(solver_stubs) + 1),\n",
    "        \"# inst ({:d})\".format(seed_ind+1),\n",
    "        disjset_num_inst_by_seed_bucket_class[seed_ind]\n",
    "    )\n",
    "\n",
    "# Insert columns with # inst (avg)\n",
    "disjset_time_geomean_df.insert(\n",
    "    2 + NUM_SEEDS * (len(solver_stubs) + 1),\n",
    "    \"# inst (avg)\",\n",
    "    disjset_num_inst_avg\n",
    ")\n",
    "\n",
    "disjset_time_geomean_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Combined\" nodes summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disjset_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = selected_disjset_time_instances\n",
    "\n",
    "disjset_nodes_cols = \\\n",
    "    ['ALL' + ' ' + solver_type + ' ' + nodes_stub for solver_type in solver_stubs] \\\n",
    "    + \\\n",
    "    [ref_nodes_col, refv_nodes_col] \\\n",
    "    + \\\n",
    "    [col_num_vpc]\n",
    "new_disjset_nodes_cols = [ \n",
    "        [\n",
    "            solver_type + ' ' + '(%d)' % seed + ' ' + nodes_stub\n",
    "            for seed in range(1,NUM_SEEDS+1)\n",
    "        ]\n",
    "        for solver_type in solver_stubs\n",
    "    ]\n",
    "new_disjset_wins_cols = [\n",
    "    \"Wins ({} over {})\".format(solver_stubs[0], solver_stubs[1]),\n",
    "    \"Wins ({} over {})\".format(solver_stubs[1], solver_stubs[0])\n",
    "]\n",
    "disjset_nodes_df = df_disjset.loc[inst_set, disjset_nodes_cols]\n",
    "# display(disjet_timing_df.head())\n",
    "\n",
    "# Add columns solver_type (%d) + ' ' + time_stub for the 7 random seeds\n",
    "# These will be parsed from the ALL REF TIME and ALL REF+V TIME columns,\n",
    "# which contain semicolon-separated values\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    for solver_ind in range(len(solver_stubs)):\n",
    "        solver_type = solver_stubs[solver_ind]\n",
    "        orig_col = disjset_nodes_cols[solver_ind]\n",
    "        new_col = new_disjset_nodes_cols[solver_ind][seed_ind]\n",
    "        disjset_nodes_df[new_col] = disjset_nodes_df[orig_col].str.split(';').str[seed_ind]\n",
    "        disjset_nodes_df[new_col] = disjset_nodes_df[new_col].astype(float)\n",
    "\n",
    "# Add wins columns\n",
    "for solver_ind in range(len(solver_stubs)):\n",
    "    other_solver_ind = 1 - solver_ind\n",
    "\n",
    "    disjset_nodes_df[new_disjset_wins_cols[solver_ind]] = sum(\n",
    "        disjset_nodes_df[new_disjset_nodes_cols[other_solver_ind][seed_ind]]\n",
    "        > \n",
    "        WIN_BY_TIME_FACTOR * disjset_nodes_df[new_disjset_nodes_cols[solver_ind][seed_ind]]\n",
    "        for seed_ind in range(NUM_SEEDS)\n",
    "    )\n",
    "\n",
    "display(disjset_nodes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disjset_nodes_df.to_csv(\"disjset_nodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disjset_nodes_geomean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row names\n",
    "disjset_nodes_geomean_cols = [\n",
    "    [\n",
    "        new_disjset_nodes_cols[solver_ind][seed_ind] \n",
    "        for seed_ind in range(NUM_SEEDS) \n",
    "        for solver_ind in range(len(solver_stubs))\n",
    "    ]\n",
    "    + [ref_nodes_col, refv_nodes_col]\n",
    "    + new_disjset_wins_cols\n",
    "]\n",
    "disjset_nodes_geomean_df = pd.DataFrame(\n",
    "    columns = disjset_nodes_geomean_cols,\n",
    "    index = disjset_bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "disjset_nodes_geomean_df.loc[:,(disjset_nodes_geomean_cols[0])] = disjset_nodes_geomean_df.loc[:,(disjset_nodes_geomean_cols[0])].astype(object)\n",
    "\n",
    "# Prepare num_inst columns\n",
    "disjset_num_inst_by_seed_bucket_class = [\n",
    "    np.zeros(len(disjset_nodes_geomean_df),dtype = np.int64)\n",
    "    for _ in range(NUM_SEEDS) \n",
    "]\n",
    "disjset_num_inst_avg = np.zeros(len(disjset_nodes_geomean_df),dtype = np.int64)\n",
    "disjset_num_ref_seed_wins = np.zeros(len(disjset_nodes_geomean_df),dtype = np.int64)\n",
    "disjset_num_refv_seed_wins = np.zeros\n",
    "\n",
    "# For each class, calculate geomean\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    row_ind = 0\n",
    "    for class_ind in range(disjset_num_bb_classes):\n",
    "        print(\"\\nClass: {}\".format(disjset_bb_classes[class_ind]))\n",
    "        curr_df = disjset_nodes_df.loc[disjset_bb_classes_lists[class_ind]]\n",
    "        seed_cols = [ new_disjset_nodes_cols[solver_ind][seed_ind] for solver_ind in range(len(solver_stubs)) ]\n",
    "        seed_curr_df = curr_df[seed_cols]\n",
    "        \n",
    "        time_curr_df = disjset_timing_df.loc[disjset_bb_classes_lists[class_ind]]\n",
    "        time_seed_cols = [ new_disjset_timing_cols[solver_ind][seed_ind] for solver_ind in range(len(solver_stubs)) ]\n",
    "        seed_time_curr_df = time_curr_df[time_seed_cols]\n",
    "\n",
    "        # if (len(seed_curr_df) != len(seed_time_curr_df)):\n",
    "        #     display(len(seed_curr_df))\n",
    "        #     display(len(seed_time_curr_df))\n",
    "        # break\n",
    "\n",
    "        # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply nodes processed by TIMEOUT_NODE_FACTOR\n",
    "        for col_ind in range(len(time_seed_cols)):\n",
    "            time_col = time_seed_cols[col_ind]\n",
    "            node_col = seed_cols[col_ind]\n",
    "            curr_selected_indices = seed_time_curr_df[time_col] >= MAX_TIME - EPS\n",
    "            seed_curr_df.loc[curr_selected_indices, node_col] = TIMEOUT_NODE_FACTOR * seed_curr_df[node_col]\n",
    "\n",
    "        for bucket_ind in range(num_buckets):\n",
    "            print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "            # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "            curr_selected_indices = \\\n",
    "                (seed_time_curr_df[time_seed_cols].min(axis=1) >= bucket_min[bucket_ind]) \\\n",
    "                & (seed_time_curr_df[time_seed_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "            seed_curr_df = seed_curr_df[curr_selected_indices]\n",
    "            seed_time_curr_df = seed_time_curr_df[curr_selected_indices]\n",
    "\n",
    "            # Calculate geomean\n",
    "            metric_ind = 0\n",
    "            disjset_nodes_geomean_df.loc[\n",
    "                (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                seed_cols\n",
    "            ] = [\n",
    "                    geometric_mean(seed_curr_df[col] + SHIFT_NODES) - SHIFT_NODES\n",
    "                    for col in seed_cols\n",
    "                ]\n",
    "            \n",
    "            # Calculate wins\n",
    "            metric_ind = 1\n",
    "            for solver_ind in range(len(solver_stubs)):\n",
    "                other_solver_ind = 1 - solver_ind\n",
    "                disjset_nodes_geomean_df.loc[\n",
    "                    (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                    seed_cols[solver_ind]\n",
    "                ] = sum(\n",
    "                    seed_curr_df[seed_cols[other_solver_ind]]\n",
    "                    > \n",
    "                    WIN_BY_NODES_FACTOR * seed_curr_df[seed_cols[solver_ind]]\n",
    "                )\n",
    "\n",
    "            print(\"row {:d}: {:d}\".format(row_ind,len(seed_curr_df)))\n",
    "            disjset_num_inst_by_seed_bucket_class[seed_ind][row_ind:row_ind+len(bb_metrics)] = [len(seed_curr_df)] * len(bb_metrics)\n",
    "\n",
    "            row_ind += len(bb_metrics)\n",
    "\n",
    "row_ind = 0\n",
    "time_cols = [ ref_time_col, refv_time_col ]\n",
    "nodes_cols = [ ref_nodes_col, refv_nodes_col ]\n",
    "wins_cols = new_disjset_wins_cols\n",
    "for class_ind in range(disjset_num_bb_classes):\n",
    "    print(\"\\nClass: {}\".format(disjset_bb_classes[class_ind]))\n",
    "    curr_df = disjset_nodes_df.loc[disjset_bb_classes_lists[class_ind]]\n",
    "    time_curr_df = disjset_timing_df.loc[disjset_bb_classes_lists[class_ind]]\n",
    "\n",
    "    # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply nodes processed by TIMEOUT_NODE_FACTOR\n",
    "    for col_ind in range(len(time_cols)):\n",
    "        time_col = time_cols[col_ind]\n",
    "        node_col = nodes_cols[col_ind]\n",
    "        curr_selected_indices = time_curr_df[time_col] >= MAX_TIME - EPS\n",
    "        curr_df.loc[curr_selected_indices, node_col] = TIMEOUT_NODE_FACTOR * curr_df[node_col]\n",
    "\n",
    "    for bucket_ind in range(num_buckets):\n",
    "        print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "        # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "        curr_selected_indices = \\\n",
    "            (time_curr_df[time_cols].min(axis=1) >= bucket_min[bucket_ind]) \\\n",
    "            & (time_curr_df[time_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "        curr_df = curr_df[curr_selected_indices]\n",
    "        time_curr_df = time_curr_df[curr_selected_indices]\n",
    "\n",
    "        # Calculate geomean\n",
    "        metric_ind = 0\n",
    "        disjset_nodes_geomean_df.loc[\n",
    "            (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "            nodes_cols\n",
    "        ] = [\n",
    "                geometric_mean(curr_df[col] + SHIFT_NODES) - SHIFT_NODES\n",
    "                for col in nodes_cols\n",
    "            ]\n",
    "        \n",
    "        # Calculate wins\n",
    "        metric_ind = 1\n",
    "        for solver_ind in range(len(solver_stubs)):\n",
    "            other_solver_ind = 1 - solver_ind\n",
    "            disjset_nodes_geomean_df.loc[\n",
    "                (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                nodes_cols[solver_ind]\n",
    "            ] = sum(\n",
    "                curr_df[nodes_cols[other_solver_ind]]\n",
    "                > \n",
    "                WIN_BY_NODES_FACTOR * curr_df[nodes_cols[solver_ind]]\n",
    "            )\n",
    "\n",
    "            # Also report average number of seeds for which REF and REF+V win\n",
    "            disjset_nodes_geomean_df.loc[\n",
    "                (disjset_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[0:len(bb_metrics)]),\n",
    "                wins_cols[solver_ind]\n",
    "            ] = sum(curr_df[wins_cols[solver_ind]]) / len(curr_df)\n",
    "\n",
    "        print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "        disjset_num_inst_avg[row_ind:row_ind+len(bb_metrics)] = [len(curr_df)] * len(bb_metrics)\n",
    "\n",
    "        row_ind += len(bb_metrics)\n",
    "\n",
    "# Insert columns with # inst (seed)\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    disjset_nodes_geomean_df.insert(\n",
    "        2 + seed_ind * (len(solver_stubs) + 1),\n",
    "        \"# inst ({:d})\".format(seed_ind+1),\n",
    "        disjset_num_inst_by_seed_bucket_class[seed_ind]\n",
    "    )\n",
    "\n",
    "# Insert columns with # inst (avg)\n",
    "disjset_nodes_geomean_df.insert(\n",
    "    2 + NUM_SEEDS * (len(solver_stubs) + 1),\n",
    "    \"# inst (avg)\",\n",
    "    disjset_num_inst_avg\n",
    ")\n",
    "\n",
    "disjset_nodes_geomean_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: `disjset_avg_bb_df` for \"Combined\" instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up empty `disjset_avg_bb_df`\n",
    "\n",
    "disjset_time_cols_short = time_cols_short\n",
    "disjset_node_cols_short = node_cols_short\n",
    "\n",
    "disjset_avg_bb_cols = pd.MultiIndex.from_arrays(\n",
    "    [\n",
    "      [time_col_header]*len(disjset_time_cols_short) + [node_col_header]*len(disjset_node_cols_short),\n",
    "      disjset_time_cols_short + disjset_node_cols_short\n",
    "    ],\n",
    "    names = ['criterion', 'type'])\n",
    "\n",
    "#bb_row_names = pd.MultiIndex.from_product([bb_buckets, bb_row_names], names=['bucket', 'metric'])\n",
    "disjset_bb_row_names = pd.MultiIndex.from_product(\n",
    "    [disjset_bb_classes, bb_buckets, bb_metrics],\n",
    "    names=['class', 'bucket', 'metric'])\n",
    "\n",
    "disjset_avg_bb_df = pd.DataFrame(\n",
    "    columns = disjset_avg_bb_cols,\n",
    "    index = disjset_bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "disjset_avg_bb_df.loc[:,(time_col_header,disjset_time_cols_short)] = disjset_avg_bb_df.loc[:,(time_col_header,disjset_time_cols_short)].astype(object)\n",
    "disjset_avg_bb_df.loc[:,(node_col_header,disjset_node_cols_short)] = disjset_avg_bb_df.loc[:,(node_col_header,disjset_node_cols_short)].astype(object)\n",
    "\n",
    "num_inst = np.zeros(len(disjset_avg_bb_df),dtype = np.int64)\n",
    "# cols = disjset_time_cols_short + disjset_node_cols_short\n",
    "\n",
    "## Fill in values from disjset_time_geomean_df and disjset_nodes_geomean_df (from the ref and refv cols)\n",
    "# Fill in values from disjset_time_geomean_df\n",
    "for index, row in disjset_time_geomean_df.iterrows():\n",
    "    class_val, bucket_val, metric_val = index\n",
    "    for col in disjset_time_cols_short:\n",
    "        disjset_avg_bb_df.loc[(class_val, bucket_val, metric_val), (time_col_header, col)] = \\\n",
    "            row[map_short_to_cols_time[col]]\n",
    "\n",
    "# Fill in values from disjset_nodes_geomean_df\n",
    "for index, row in disjset_nodes_geomean_df.iterrows():\n",
    "    class_val, bucket_val, metric_val = index\n",
    "    for col in disjset_node_cols_short:\n",
    "        disjset_avg_bb_df.loc[(class_val, bucket_val, metric_val), (node_col_header, col)] = \\\n",
    "            row[map_short_to_cols_nodes[col]]\n",
    "        \n",
    "# Add in number of instances in each bucket as '# inst' col\n",
    "disjset_avg_bb_df[inst_col_name] = disjset_num_inst_avg\n",
    "\n",
    "display(disjset_avg_bb_df.loc[:,disjset_avg_bb_df.columns.get_level_values(0)==node_col_header].head(8))\n",
    "#display(avg_bb_df.loc[(bb_classes[0], bb_buckets[1], bb_metrics[0]),:])\n",
    "display(disjset_avg_bb_df.loc[(disjset_bb_classes, bb_buckets, bb_metrics[0]),:])\n",
    "display(disjset_avg_bb_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: rounds results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select rounds instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in rounds data\n",
    "#df_rounds = pd.read_csv(RESULTS_DIR + '/' + 'vpc-rounds.csv', index_col=0)\n",
    "df_rounds = initialize_df(RESULTS_DIR + '/' + 'vpc-rounds.csv')\n",
    "\n",
    "## Change disj_terms (second level of index) to -1\n",
    "df_rounds.index = df_rounds.index.set_levels([-1], level=1)\n",
    "\n",
    "## Update column types\n",
    "col_list = [col_best_disj_obj, col_worst_disj_obj]\n",
    "for col in col_list:\n",
    "    df_rounds[col] = pd.to_numeric(df_rounds[col])\n",
    "\n",
    "## Create new column for number of disjunctive terms since original one is now index\n",
    "df_rounds[col_num_disj_terms] = df_rounds.index.get_level_values(1)\n",
    "\n",
    "## Identify pure binary instances, which are those where 'CLEANED BINARY' column equals 'CLEANED COLS'\n",
    "df_rounds[col_pure_binary] = (df_rounds[col_binary] == df_rounds[col_num_cols])\n",
    "\n",
    "## Identify mixed binary instances, which are those where 'CLEANED GEN INT' column = 0\n",
    "df_rounds[col_mixed_binary] = (df_rounds[col_gen_int] == 0)\n",
    "\n",
    "## Recompute average running times\n",
    "curr_df = df_rounds\n",
    "solver_stub = solver_stubs[1]\n",
    "    \n",
    "# Split values in 'ALL REF TIME' into new columns for 'REF TIME (SEED)' where 'SEED' takes values 628 * [1,2,3,4,5,6,7]\n",
    "df_timing = curr_df['ALL '+solver+' TIME'].str.split(';', expand=True)\n",
    "df_timing.columns = [solver+' TIME (%d)' % (i+1) for i in range(df_timing.shape[1])]\n",
    "df_timing = df_timing.astype(float)\n",
    "df_timing_cols = df_timing.columns[df_timing.columns.str.contains(re.escape(solver)+r' TIME (.+)')]\n",
    "\n",
    "# Do the same for nodes\n",
    "df_nodes = curr_df['ALL '+solver+' NODES'].str.split(';', expand=True)\n",
    "df_nodes.columns = [solver+' NODES (%d)' % (i+1) for i in range(df_nodes.shape[1])]\n",
    "df_nodes = df_nodes.astype(float)\n",
    "df_nodes_cols = df_nodes.columns[df_nodes.columns.str.contains(re.escape(solver)+r' NODES (.+)')]\n",
    "\n",
    "# Select entries in which the max time is greater than MAX_TIME\n",
    "selected_entries = df_timing > (MAX_TIME - EPS)\n",
    "\n",
    "# Add min and max of 'REF TIME (SEED)' columns\n",
    "curr_df[solver+' TIME MIN'] = df_timing[df_timing_cols].min(axis=1)\n",
    "curr_df[solver+' TIME MAX'] = df_timing[df_timing_cols].max(axis=1)\n",
    "\n",
    "# Find average of 'REF TIME (SEED)' columns after adjusting for timeout\n",
    "df_timing[selected_entries] = TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "# df_timing[solver+' TIME AVG'] = df_timing.mean(axis=1)\n",
    "curr_df['AVG '+solver+' TIME'] = df_timing[df_timing_cols].mean(axis=1)\n",
    "\n",
    "## Repeat for nodes\n",
    "# Add min and max of 'REF NODES (SEED)' columns\n",
    "curr_df[solver+' NODES MIN'] = df_nodes[df_nodes_cols].min(axis=1)\n",
    "curr_df[solver+' NODES MAX'] = df_nodes[df_nodes_cols].max(axis=1)\n",
    "\n",
    "# Find average of 'REF NODES (SEED)' columns after adjusting for timeout\n",
    "selected_entries.columns = df_nodes_cols\n",
    "df_nodes[selected_entries] *= TIMEOUT_NODE_FACTOR\n",
    "# df_nodes[solver+' NODES AVG'] = df_nodes[df_nodes_cols].mean(axis=1)\n",
    "curr_df['AVG '+solver+' NODES'] = df_nodes[df_nodes_cols].mean(axis=1)\n",
    "\n",
    "# Append df_timing to df\n",
    "curr_df = pd.concat([curr_df, df_timing], axis=1)\n",
    "\n",
    "# Append df_nodes to df\n",
    "curr_df = pd.concat([curr_df, df_nodes], axis=1)\n",
    "\n",
    "## Add col for avg time with cuts\n",
    "df_rounds[refv_w_cut_time_col] = df_rounds[refv_time_col] + df_rounds[col_vpc_gen_time]\n",
    "\n",
    "display(df_rounds.head(2))\n",
    "display(df_rounds.loc['mas284_presolved'][obj_val_col_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_nodes dataframe, in which each row is an instance, and each column is a solver (with or w/o cuts, and with a fixed random seed)\n",
    "# These will be pulled from 'ALL REF TIME' and 'ALL REF+V TIME' columns\n",
    "inst_set = list(instances)\n",
    "# inst_set = ['bm23_presolved', 'cvs16r106-72_presolved', 'cvs16r128-89_presolved', 'mine-90-10_presolved', 'misc03_presolved', 'neos-1058477_presolved', 'neos-3083819-nubu_presolved', 'neos-593853_presolved', 'neos-880324_presolved', 'ns1688347_presolved']\n",
    "PRINT_SKIP_REASON = False\n",
    "PRINT_NEW_SKIP_REASON = True\n",
    "\n",
    "rounds_num_gap_errors = 0\n",
    "\n",
    "selected_rounds_gap_instances_dict = {} # dictionary of (original index, instance)\n",
    "selected_rounds_time_instances_dict = {} # dictionary of (original index, instance)\n",
    "for i, inst in enumerate(inst_set):\n",
    "  print(\"{}/{}\".format(i+1,len(inst_set)), end='\\r', flush=True)\n",
    "  skip_instance = False\n",
    "  prev_selected_instance = inst in selected_gap_instances\n",
    "  CURR_PRINT_SKIP_REASON = PRINT_SKIP_REASON or (PRINT_NEW_SKIP_REASON and prev_selected_instance)\n",
    "\n",
    "  # # Check that -1 depth exists\n",
    "  # if not (-1 in df.loc[inst].index):\n",
    "  #   # if PRINT_SKIP_REASON:\n",
    "  #   print(\"Skipping instance {:d} -- {}: no -1 depth.\".format(\n",
    "  #       i, inst\n",
    "  #   ))\n",
    "  #   skip_instance = True\n",
    "  #   continue\n",
    "  if not inst in df_rounds.index:\n",
    "    if CURR_PRINT_SKIP_REASON:\n",
    "      print(\"Skipping instance {:d} -- {}: no rounds results.\".format(\n",
    "          i, inst\n",
    "      ))\n",
    "    skip_instance = True\n",
    "    continue\n",
    "  \n",
    "  curr_df = df_rounds.loc[(inst,-1)]\n",
    "\n",
    "  # Ensure nrows and ncols is not too many\n",
    "  nrows = curr_df[col_num_rows]\n",
    "  ncols = curr_df[col_num_cols]\n",
    "  if (nrows > MAX_ROWS) or (ncols > MAX_COLS):\n",
    "    if CURR_PRINT_SKIP_REASON:\n",
    "        print(\"Skipping instance {:d} -- {}: nrows = {:d} > {:d} or ncols = {:d} > {:d}.\".format(\n",
    "                i, inst, nrows, ncols, MAX_ROWS, MAX_COLS))\n",
    "    skip_instance = True\n",
    "\n",
    "  # Ensure IP objective value is known\n",
    "  ip_obj = np.float64(df_ipopt.loc[inst,col_ip_obj])\n",
    "  if not isinstance(ip_obj,float):\n",
    "    if CURR_PRINT_SKIP_REASON:\n",
    "        print(\n",
    "            \"Skipping instance {:d} -- {}: IP objective value ({}) is not detected to be a float value.\".format(\n",
    "            i, inst, ip_obj))\n",
    "    skip_instance = True\n",
    "\n",
    "  check_ip_obj = curr_df[col_ip_obj]\n",
    "  if not is_val(ip_obj,check_ip_obj):\n",
    "    print(\"*** ERROR: Instance {:d} -- {}: IP objective value ({}) does not match value in dataframe ({}). REPLACING WITH KNOWN VALUE!\".format(\n",
    "        i, inst, ip_obj, check_ip_obj))\n",
    "    df_rounds.loc[(inst,-1),col_ip_obj] = ip_obj\n",
    "    # skip_instance = True\n",
    "\n",
    "  # Check that LP opt < IP opt\n",
    "  lp_obj = np.float64(df_preprocess.loc[remove_presolved_from_name(inst),col_cleaned_lp_obj])\n",
    "  YES_GAP = (ip_obj - lp_obj) >= EPS\n",
    "  if not YES_GAP:\n",
    "    print(\"*** ERROR: Instance {:d} -- {}: not YES GAP (lp = {:.10f}; ip = {:.10f}, diff = {:.2f})\".format(i, inst, lp_obj, ip_obj, ip_obj-lp_obj))\n",
    "    skip_instance = True\n",
    "    rounds_num_gap_errors += 1\n",
    "\n",
    "  # Check that ExitReason != OPTIMAL_SOLUTION_FOUND\n",
    "  exitreason = curr_df['ExitReason']\n",
    "  if exitreason == 'OPTIMAL_SOLUTION_FOUND':\n",
    "    if CURR_PRINT_SKIP_REASON:\n",
    "      print(\"Skipping instance {:d} -- {}: optimal IP solution found.\".format(\n",
    "          i, inst\n",
    "      ))\n",
    "    skip_instance = True\n",
    "\n",
    "  # Check that VPCs were generated\n",
    "  num_vpc = curr_df[col_num_vpc]\n",
    "  if num_vpc == 0:\n",
    "    if CURR_PRINT_SKIP_REASON:\n",
    "      print(\"Skipping instance {:d} -- {}: no VPCs generated.\".format(\n",
    "          i, inst\n",
    "      ))\n",
    "    skip_instance = True\n",
    "\n",
    "  if not skip_instance:\n",
    "    selected_rounds_gap_instances_dict[inst] = i\n",
    "\n",
    "    # If both ref and refv times are < MAX_TIME - EPS, include in nodes experiments\n",
    "    ref_time = curr_df[ref_time_col]\n",
    "    refv_time = curr_df[refv_time_col]\n",
    "    if (ref_time < MAX_TIME - EPS) and (refv_time < MAX_TIME - EPS):\n",
    "      selected_rounds_time_instances_dict[inst] = i\n",
    "\n",
    "selected_rounds_gap_instances = list(selected_rounds_gap_instances_dict.keys())\n",
    "print(\"Total number of errors: {}\".format(rounds_num_gap_errors))\n",
    "print(\"Num instances selected for rounds gap closed results = {:d}\".format(len(selected_rounds_gap_instances)))\n",
    "\n",
    "selected_rounds_time_instances = list(selected_rounds_time_instances_dict.keys())\n",
    "print(\"Num instances selected for rounds nodes results = {:d}\".format(len(selected_rounds_time_instances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List instances that are in exactly one of the two sets `selected_gap_instances` and `selected_rounds_gap_instances`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric difference between two sets selected_gap_instances and selected_rounds_gap_instances\n",
    "print(\"Instances in selected_rounds_gap_instances that are not in selected_gap_instances:\")\n",
    "newly_selected_instances = [inst for inst in selected_rounds_gap_instances if inst not in selected_gap_instances]\n",
    "print(newly_selected_instances)\n",
    "\n",
    "# Remove newly selected instances\n",
    "for inst in newly_selected_instances:\n",
    "  selected_rounds_gap_instances.remove(inst)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_gap_instances that are not in selected_rounds_gap_instances:\")\n",
    "print([inst for inst in selected_gap_instances if inst not in selected_rounds_gap_instances])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_time_instances that are not in selected_rounds_time_instances:\")\n",
    "print([inst for inst in selected_time_instances if inst not in selected_rounds_time_instances])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_rounds_time_instances that are not in selected_time_instances:\")\n",
    "print([inst for inst in selected_rounds_time_instances if inst not in selected_time_instances])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Instances in selected_rounds_time_instances that are not in selected_disjset_time_instances:\")\n",
    "print([inst for inst in selected_rounds_time_instances if inst not in selected_disjset_time_instances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG: why instance is missing from rounds results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if instance is in df_rounds and identify reason it was not selected if yes\n",
    "# inst = 'lotsize_presolved'\n",
    "# if inst in df_rounds.index:\n",
    "#   print(\"Instance {} is in df_rounds.\".format(inst))\n",
    "#   curr_df = df_rounds.loc[(inst,-1)]\n",
    "#   ip_obj = np.float64(df_ipopt.loc[inst,col_ip_obj])\n",
    "#   check_ip_obj = curr_df[col_ip_obj]\n",
    "#   if not is_val(ip_obj,check_ip_obj):\n",
    "#     print(\"*** ERROR: Instance {}: IP objective value ({}) does not match value in dataframe ({}). NEED TO REPLACE WITH KNOWN VALUE! (NOT DONE HERE.)\".format(inst, ip_obj, check_ip_obj))\n",
    "#     # df_rounds.loc[(inst,-1),col_ip_obj] = ip_obj\n",
    "#   exitreason = curr_df['ExitReason']\n",
    "#   if exitreason == 'OPTIMAL_SOLUTION_FOUND':\n",
    "#     print(\"Instance {} excluded because optimal IP solution found.\".format(inst))\n",
    "#   num_vpc = curr_df[col_num_vpc]\n",
    "#   if num_vpc == 0:\n",
    "#     print(\"Instance {} excluded because no VPCs generated.\".format(inst))\n",
    "# else:\n",
    "#     print(\"Instance {} is not in df_rounds.\".format(inst))\n",
    "\n",
    "# # Check timing\n",
    "# solver = 'REF+V'\n",
    "# df_rounds.loc[inst, 'ALL '+solver+' TIME'].str.split(';', expand=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rounds gap summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rounds_gap_df`: Analyze rounds gap closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of dataframe relevant to gap closed\n",
    "rounds_gap_df = df_rounds.loc[selected_rounds_gap_instances, \n",
    "                [\n",
    "                    col_num_disj_terms,\n",
    "                    col_num_rows,\n",
    "                    col_num_cols\n",
    "                ]\n",
    "                +\n",
    "                obj_val_col_list\n",
    "                +\n",
    "                [\n",
    "                    col_num_gmic,\n",
    "                    col_num_vpc,\n",
    "                    col_num_obj,\n",
    "                    col_exit_reason\n",
    "                ]\n",
    "               ]\n",
    "\n",
    "# Take only -1 depth\n",
    "rounds_gap_df = rounds_gap_df.xs(-1, level='disj_terms')\n",
    "\n",
    "# Calculate some missing % gap closed columns\n",
    "# gap closed = 100 * (post_cut_opt_val - lp_opt_val) / (ip_opt_val - lp_opt_val)\n",
    "for cut_type in reg_cut_type_long_list:\n",
    "    col = cut_type + ' ' + obj_stub\n",
    "    if col not in df.columns:\n",
    "        if cut_type == 'MAX(GMIC,VPC)':\n",
    "            # Add max(G,V) column\n",
    "            rounds_gap_df[cut_type + ' ' + pct_gap_closed_stub] = \\\n",
    "                np.maximum(\n",
    "                    rounds_gap_df['GMIC' + ' ' + pct_gap_closed_stub],\n",
    "                    rounds_gap_df[ 'VPC' + ' ' + pct_gap_closed_stub]\n",
    "                )\n",
    "        continue\n",
    "    rounds_gap_df[cut_type + ' ' + pct_gap_closed_stub] = calc_gap_closed(rounds_gap_df, col)\n",
    "\n",
    "# Compare against reference solver\n",
    "for stat_type in solver_stat_list[:3]:\n",
    "    for solver_type in solver_stubs:\n",
    "        for cut_type in solver_cut_type_stubs:\n",
    "            col = stat_type + ' ' + solver_type + ' ' + cut_type\n",
    "            rounds_gap_df[col + ' ' + pct_gap_closed_stub] = calc_gap_closed(rounds_gap_df, col)\n",
    "\n",
    "# Rename each column with its shortname\n",
    "rounds_gap_df.rename(columns=map_cols_to_short_gap,inplace=True)\n",
    "\n",
    "# Fill in values of col_first_cut_pass_gap_ref and col_last_cut_pass_gap_ref from disjset_gap_df\n",
    "for inst in selected_rounds_gap_instances:\n",
    "    for col in [col_first_cut_pass_gap_ref, col_last_cut_pass_gap_ref]:\n",
    "        col = map_cols_to_short_gap[col]\n",
    "        rounds_gap_df.loc[inst,col] = disjset_gap_df.loc[inst,col]\n",
    "\n",
    "tmp_inst_set = ['mas284_presolved','maxgasflow_presolved']\n",
    "display(disjset_gap_df.loc[tmp_inst_set][gap_cols_short])\n",
    "display(rounds_gap_df.loc[tmp_inst_set][gap_cols_short])\n",
    "display(rounds_gap_df.loc[(\"mas284_presolved\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_gap_df.to_csv('rounds_gap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rounds_avg_gap_df`: summary of results with rounds instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE 1.2: average percent gap closed across different combinations of cuts\n",
    "## Create rounds_avg_gap_df = average gap closed across instances\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [ [all_set_name, good_vpc_set_name, binary_set_name], [avg_row_name, wins_row_name] ],\n",
    "    names = ['Set', '']\n",
    ")\n",
    "    \n",
    "ncols = len(gap_cols_short)\n",
    "nrows = len(idx)\n",
    "\n",
    "col = rounds_gap_df['V'].astype(float)\n",
    "rounds_good_vpc_df = rounds_gap_df[col >= 10.]\n",
    "rounds_binary_instances_df = rounds_gap_df.loc[ [inst for inst in pure_binary_instances if inst in selected_rounds_gap_instances ] ]\n",
    "\n",
    "data = np.zeros((nrows, ncols), dtype=float)\n",
    "data[0,:] = [rounds_gap_df[col].mean() for col in gap_cols_short]\n",
    "data[2,:] = [rounds_good_vpc_df[col].mean() for col in gap_cols_short]\n",
    "data[4,:] = [rounds_binary_instances_df[col].mean() for col in gap_cols_short]\n",
    "\n",
    "# display(best_gap_df.head())\n",
    "rounds_avg_gap_df = pd.DataFrame(\n",
    "    data,\n",
    "    columns = gap_cols_short,\n",
    "    index = idx,\n",
    "    dtype = object\n",
    ")\n",
    "\n",
    "inst_col_name = '# inst'\n",
    "rounds_avg_gap_df[inst_col_name] = [len(rounds_gap_df), 0, len(rounds_good_vpc_df), 0, len(rounds_binary_instances_df), 0]\n",
    "\n",
    "rounds_avg_gap_df.iloc[1] = [\"\" for i in range(ncols+1)]\n",
    "rounds_avg_gap_df.iloc[3] = [\"\" for i in range(ncols+1)]\n",
    "rounds_avg_gap_df.iloc[5] = [\"\" for i in range(ncols+1)]\n",
    "\n",
    "display(rounds_avg_gap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rounds_wins_df`: number of wins across methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create num wins df\n",
    "# x wins over y for an instance if x > y + EPS\n",
    "#shortcols = avg_gap_df.columns[0:-1]\n",
    "rounds_wins_df = pd.DataFrame(\n",
    "    np.zeros((len(gap_cols_short), len(gap_cols_short)), dtype=int),\n",
    "    columns = gap_cols_short,\n",
    "    index = gap_cols_short,\n",
    "    dtype = int,\n",
    ")\n",
    "\n",
    "WINS_EPS = GAP_DIFF_EPS\n",
    "\n",
    "from itertools import permutations\n",
    "for (ind1, ind2) in permutations(range(len(gap_cols_short)), 2):\n",
    "    rounds_wins_df.at[gap_cols_short[ind1],gap_cols_short[ind2]] =\\\n",
    "        int(sum(rounds_gap_df[gap_cols_short[ind1]] > rounds_gap_df[gap_cols_short[ind2]] + WINS_EPS))\n",
    "    rounds_wins_df.at[gap_cols_short[ind2],gap_cols_short[ind1]] =\\\n",
    "        int(sum(rounds_gap_df[gap_cols_short[ind2]] > rounds_gap_df[gap_cols_short[ind1]] + WINS_EPS))\n",
    "\n",
    "# Sets we are considering\n",
    "# all_set = 'Wins (All)'\n",
    "# good_vpc_set = 'Wins (V ≥ 10%)'\n",
    "rounds_all_set = (all_set_name,wins_row_name)\n",
    "rounds_good_vpc_set = (good_vpc_set_name,wins_row_name)\n",
    "rounds_binary_set = (binary_set_name,wins_row_name)\n",
    "\n",
    "# \"G\" are wins relative to \"V\"\n",
    "shortrefcol = 'V'\n",
    "#refcol = 'VPC % GAP CLOSED'\n",
    "#refcol = map_short_to_cols[shortrefcol]\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'G'\n",
    "#col = 'GMIC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = rounds_wins_df.at[shortdestcol,shortrefcol]\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > rounds_good_vpc_df[refcol] + WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > rounds_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"DB\", \"V\", \"V+G\": wins are relative to \"G\"\n",
    "shortrefcol = 'G'\n",
    "#refcol = 'GMIC % GAP CLOSED'\n",
    "#refcol = map_short_to_cols[shortrefcol]\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'DB'\n",
    "#col = 'BEST DISJ % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = rounds_wins_df.at[shortdestcol,shortrefcol]\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > rounds_good_vpc_df[refcol] + WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > rounds_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "shortdestcol = 'V'\n",
    "#col = 'VPC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = rounds_wins_df.at[shortdestcol,shortrefcol]\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > rounds_good_vpc_df[refcol] + WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > rounds_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "shortdestcol = 'V+G'\n",
    "#col = 'VPC+GMIC % GAP CLOSED'\n",
    "#col = map_short_to_cols[shortcol]\n",
    "destcol = shortdestcol\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = rounds_wins_df.at[shortdestcol,shortrefcol]\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > rounds_good_vpc_df[refcol] + WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > rounds_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"V+GurF\" are wins relative to \"GurF\"\n",
    "shortrefcol = 'GurF'\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'V+GurF'\n",
    "destcol = shortdestcol\n",
    "#col = map_short_to_cols[shortcol]\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = rounds_wins_df.at[shortdestcol,shortrefcol]\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > rounds_good_vpc_df[refcol] + WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > rounds_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# \"V+GurL\" are wins relative to \"GurL\"\n",
    "shortrefcol = 'GurL'\n",
    "refcol = shortrefcol\n",
    "shortdestcol = 'V+GurL'\n",
    "destcol = shortdestcol\n",
    "rounds_wins_df.at[shortdestcol,shortrefcol] = int(sum(rounds_gap_df[destcol] > rounds_gap_df[refcol] + WINS_EPS))\n",
    "rounds_wins_df.at[shortrefcol,shortdestcol] = int(sum(rounds_gap_df[refcol] > rounds_gap_df[destcol] + WINS_EPS))\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = rounds_wins_df.at[shortdestcol,shortrefcol]\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > rounds_good_vpc_df[refcol] + WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > rounds_binary_instances_df[refcol] + WINS_EPS)\n",
    "\n",
    "# Count number of instances that have V+G > 0\n",
    "shortdestcol = inst_col_name\n",
    "#col = 'V+GurL'\n",
    "destcol = 'V+G'\n",
    "rounds_avg_gap_df.at[rounds_all_set,shortdestcol] = sum(rounds_gap_df[destcol] > WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_good_vpc_set,shortdestcol] = sum(rounds_good_vpc_df[destcol] > WINS_EPS)\n",
    "rounds_avg_gap_df.at[rounds_binary_set,shortdestcol] = sum(rounds_binary_instances_df[destcol] > WINS_EPS)\n",
    "\n",
    "display(rounds_avg_gap_df)\n",
    "display(rounds_wins_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Table 2 `gap_by_size_df` with rounds results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new row with label \"Rounds\"\n",
    "rounds_row = pd.DataFrame(columns=gap_by_size_df.columns, index=[\"Rounds\"])\n",
    "\n",
    "# Calculate the average values for each column from the `rounds` dataframe\n",
    "rounds_row = rounds_avg_gap_df.loc['All',gap_by_size_df.columns][0:1].to_numpy()\n",
    "\n",
    "# Append the new row to the `gap_by_size` dataframe\n",
    "gap_by_size_df.loc['Rounds'] = rounds_row[0]\n",
    "\n",
    "# Display the updated `gap_by_size` dataframe\n",
    "display(gap_by_size_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long did it take to generate these cuts, on average, compared to just one round with half the cut limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_disjset_rounds_gap_instances = list(set(selected_disjset_gap_instances) & set(selected_rounds_gap_instances))\n",
    "disjset_avg_gen_time = df_disjset.loc[intersection_disjset_rounds_gap_instances,[col_vpc_gen_time]].mean().iloc[0]\n",
    "rounds_avg_gen_time = df_rounds.loc[intersection_disjset_rounds_gap_instances,[col_vpc_gen_time]].mean().iloc[0]\n",
    "print(\"Average VPC generation time for disjset instances: {:.2f}\".format(disjset_avg_gen_time))\n",
    "print(\"Average VPC generation time for rounds instances: {:.2f}\".format(rounds_avg_gen_time))\n",
    "\n",
    "# Repeat for instances by disj size\n",
    "curr_df = df.loc[intersection_disjset_rounds_gap_instances,[col_vpc_gen_time]]\n",
    "for size in sizes:\n",
    "    # report average time to generate vpcs from df for this size\n",
    "    curr_size_avg_gen_time = curr_df.xs(size, level='disj_terms')[col_vpc_gen_time].mean()\n",
    "    print(\"Average VPC generation time for instances of size {:d}: {:.2f}\".format(size,curr_size_avg_gen_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rounds timing summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rounds_timing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = selected_rounds_time_instances\n",
    "NUM_SEEDS = 7\n",
    "rounds_timing_cols = ['ALL' + ' ' + solver_type + ' ' + time_stub for solver_type in solver_stubs] + [ref_time_col, refv_time_col] + [col_vpc_gen_time] + [col_num_vpc]\n",
    "new_rounds_timing_cols = [ \n",
    "        [\n",
    "            solver_type + ' ' + '(%d)' % seed + ' ' + time_stub\n",
    "            for seed in range(1,NUM_SEEDS+1)\n",
    "        ]\n",
    "        for solver_type in solver_stubs\n",
    "    ]\n",
    "new_rounds_wins_cols = [\n",
    "    \"Wins ({} over {})\".format(solver_stubs[0], solver_stubs[1]),\n",
    "    \"Wins ({} over {})\".format(solver_stubs[1], solver_stubs[0])\n",
    "]\n",
    "rounds_timing_df = df_rounds.loc[inst_set, rounds_timing_cols]\n",
    "\n",
    "# Replace values of rounds_timing_df associated to ref with the values from disjset_timing_df for the same columns\n",
    "cols = ['ALL' + ' ' + solver_stubs[0] + ' ' + time_stub, ref_time_col]\n",
    "rounds_timing_df.loc[:,cols] = disjset_timing_df.loc[:,cols]\n",
    "\n",
    "# display(disjet_timing_df.head())\n",
    "\n",
    "# Add columns solver_type (%d) + ' ' + time_stub for the 7 random seeds\n",
    "# These will be parsed from the ALL REF TIME and ALL REF+V TIME columns,\n",
    "# which contain semicolon-separated values\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    for solver_ind in range(len(solver_stubs)):\n",
    "        solver_type = solver_stubs[solver_ind]\n",
    "        orig_col = rounds_timing_cols[solver_ind]\n",
    "        new_col = new_rounds_timing_cols[solver_ind][seed_ind]\n",
    "        rounds_timing_df[new_col] = rounds_timing_df[orig_col].str.split(';').str[seed_ind]\n",
    "        rounds_timing_df[new_col] = rounds_timing_df[new_col].astype(float)\n",
    "\n",
    "# Add wins columns\n",
    "for solver_ind in range(len(solver_stubs)):\n",
    "    other_solver_ind = 1 - solver_ind\n",
    "\n",
    "    rounds_timing_df[new_rounds_wins_cols[solver_ind]] = sum(\n",
    "        rounds_timing_df[new_rounds_timing_cols[other_solver_ind][seed_ind]]\n",
    "        > \n",
    "        WIN_BY_TIME_FACTOR * rounds_timing_df[new_rounds_timing_cols[solver_ind][seed_ind]]\n",
    "        for seed_ind in range(NUM_SEEDS)\n",
    "    )\n",
    "\n",
    "display(rounds_timing_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_timing_df.to_csv(\"rounds_timing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rounds_time_geomean_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row names\n",
    "rounds_bb_classes = ['Rounds','RoundsBinary']\n",
    "rounds_bb_classes_lists = [ selected_rounds_time_instances, [inst for inst in pure_binary_instances if inst in selected_rounds_time_instances] ]\n",
    "rounds_num_bb_classes = len(rounds_bb_classes)\n",
    "\n",
    "# Rows are geomean and wins for each bucket\n",
    "rounds_bb_row_names = pd.MultiIndex.from_product(\n",
    "    [rounds_bb_classes, bb_buckets, bb_metrics],\n",
    "    names=['class', 'bucket', 'metric'])\n",
    "rounds_time_geomean_cols = [\n",
    "    [\n",
    "        new_rounds_timing_cols[solver_ind][seed_ind] \n",
    "        for seed_ind in range(NUM_SEEDS) \n",
    "        for solver_ind in range(len(solver_stubs))\n",
    "    ]\n",
    "    + [ref_time_col, refv_time_col]\n",
    "    + [col_vpc_gen_time]\n",
    "    + new_rounds_wins_cols\n",
    "]\n",
    "rounds_time_geomean_df = pd.DataFrame(\n",
    "    columns = rounds_time_geomean_cols,\n",
    "    index = rounds_bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "rounds_time_geomean_df.loc[:,(rounds_time_geomean_cols[0])] = rounds_time_geomean_df.loc[:,(rounds_time_geomean_cols[0])].astype(object)\n",
    "\n",
    "# Prepare num_inst columns\n",
    "rounds_num_inst_by_seed_bucket_class = [\n",
    "    np.zeros(len(rounds_time_geomean_df),dtype = np.int64)\n",
    "    for _ in range(NUM_SEEDS) \n",
    "    # for _ in range(num_buckets) \n",
    "    # for _ in range(rounds_num_bb_classes)\n",
    "]\n",
    "rounds_num_inst_avg = np.zeros(len(rounds_time_geomean_df),dtype = np.int64)\n",
    "rounds_num_ref_seed_wins = np.zeros(len(rounds_time_geomean_df),dtype = np.int64)\n",
    "rounds_num_refv_seed_wins = np.zeros(len(rounds_time_geomean_df),dtype = np.int64)\n",
    "\n",
    "# For each class, calculate geomean\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    row_ind = 0\n",
    "    for class_ind in range(rounds_num_bb_classes):\n",
    "        print(\"\\nClass: {}\".format(rounds_bb_classes[class_ind]))\n",
    "        curr_df = rounds_timing_df.loc[rounds_bb_classes_lists[class_ind]]\n",
    "        seed_cols = [ new_rounds_timing_cols[solver_ind][seed_ind] for solver_ind in range(len(solver_stubs)) ]\n",
    "        seed_curr_df = curr_df[seed_cols]\n",
    "\n",
    "        # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply by TIMEOUT_TIME_FACTOR\n",
    "        for col_ind in range(len(seed_cols)):\n",
    "            time_col = seed_cols[col_ind]\n",
    "            seed_curr_df.loc[seed_curr_df[time_col] >= MAX_TIME - EPS, time_col] = TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "\n",
    "        for bucket_ind in range(num_buckets):\n",
    "            print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "            # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "            seed_curr_df = seed_curr_df[\n",
    "                (seed_curr_df[seed_cols].min(axis=1) >= bucket_min[bucket_ind])\n",
    "                & (seed_curr_df[seed_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "            ]\n",
    "\n",
    "            # Calculate geomean\n",
    "            metric_ind = 0\n",
    "            rounds_time_geomean_df.loc[\n",
    "                (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                seed_cols\n",
    "            ] = [\n",
    "                    geometric_mean(seed_curr_df[col] + SHIFT_TIME) - SHIFT_TIME\n",
    "                    for col in seed_cols\n",
    "                ]\n",
    "            \n",
    "            # Calculate wins\n",
    "            metric_ind = 1\n",
    "            for solver_ind in range(len(solver_stubs)):\n",
    "                other_solver_ind = 1 - solver_ind\n",
    "                rounds_time_geomean_df.loc[\n",
    "                    (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                    seed_cols[solver_ind]\n",
    "                ] = sum(\n",
    "                    seed_curr_df[seed_cols[other_solver_ind]]\n",
    "                    > \n",
    "                    WIN_BY_TIME_FACTOR * seed_curr_df[seed_cols[solver_ind]]\n",
    "                )\n",
    "\n",
    "            print(\"row {:d}: {:d}\".format(row_ind,len(seed_curr_df)))\n",
    "            rounds_num_inst_by_seed_bucket_class[seed_ind][row_ind:row_ind+len(bb_metrics)] = [len(seed_curr_df)] * len(bb_metrics)\n",
    "\n",
    "            row_ind += len(bb_metrics)\n",
    "\n",
    "            # for solver_ind in range(len(solver_stubs)):\n",
    "            #     other_solver_ind = 1 - solver_ind\n",
    "            #     rounds_time_geomean_df.loc[\n",
    "            #         (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "            #         new_rounds_wins_cols[solver_ind]\n",
    "            #     ] = sum(\n",
    "            #         seed_curr_df[new_rounds_timing_cols[other_solver_ind][seed_ind]]\n",
    "            #         > \n",
    "            #         WIN_BY_TIME_FACTOR * seed_curr_df[new_rounds_timing_cols[solver_ind][seed_ind]]\n",
    "            #     )\n",
    "\n",
    "\n",
    "row_ind = 0\n",
    "only_time_cols = [ ref_time_col, refv_time_col ]\n",
    "cols = time_cols_long\n",
    "wins_cols = new_rounds_wins_cols\n",
    "for class_ind in range(rounds_num_bb_classes):\n",
    "    print(\"\\nClass: {}\".format(rounds_bb_classes[class_ind]))\n",
    "    curr_df = rounds_timing_df.loc[rounds_bb_classes_lists[class_ind]]\n",
    "\n",
    "    # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply by TIMEOUT_TIME_FACTOR\n",
    "    for col_ind in range(len(only_time_cols)):\n",
    "        time_col = only_time_cols[col_ind]\n",
    "        curr_df.loc[curr_df[time_col] >= MAX_TIME - EPS, time_col] = TIMEOUT_TIME_FACTOR * MAX_TIME\n",
    "        \n",
    "    for bucket_ind in range(num_buckets):\n",
    "        print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "        # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "        curr_df = curr_df[\n",
    "            (curr_df[only_time_cols].min(axis=1) >= bucket_min[bucket_ind])\n",
    "            & (curr_df[only_time_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "        ]\n",
    "\n",
    "        # Calculate geomean\n",
    "        metric_ind = 0\n",
    "        rounds_time_geomean_df.loc[\n",
    "            (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "            cols\n",
    "        ] = [\n",
    "                geometric_mean(curr_df[col] + SHIFT_TIME) - SHIFT_TIME\n",
    "                for col in cols\n",
    "            ]\n",
    "        \n",
    "        # Calculate wins\n",
    "        metric_ind = 1\n",
    "        for solver_ind in range(len(solver_stubs)):\n",
    "            other_solver_ind = 1 - solver_ind\n",
    "            rounds_time_geomean_df.loc[\n",
    "                (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                only_time_cols[solver_ind]\n",
    "            ] = sum(\n",
    "                curr_df[only_time_cols[other_solver_ind]]\n",
    "                > \n",
    "                WIN_BY_TIME_FACTOR * curr_df[only_time_cols[solver_ind]]\n",
    "            )\n",
    "\n",
    "            # Also report average number of seeds for which REF and REF+V win\n",
    "            rounds_time_geomean_df.loc[\n",
    "                (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[0:len(bb_metrics)]),\n",
    "                wins_cols[solver_ind]\n",
    "            ] = sum(curr_df[wins_cols[solver_ind]]) / len(curr_df)\n",
    "\n",
    "        print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "        rounds_num_inst_avg[row_ind:row_ind+len(bb_metrics)] = [len(curr_df)] * len(bb_metrics)\n",
    "\n",
    "        row_ind += len(bb_metrics)\n",
    "\n",
    "# Insert columns with # inst (seed)\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    rounds_time_geomean_df.insert(\n",
    "        2 + seed_ind * (len(solver_stubs) + 1),\n",
    "        \"# inst ({:d})\".format(seed_ind+1),\n",
    "        rounds_num_inst_by_seed_bucket_class[seed_ind]\n",
    "    )\n",
    "\n",
    "# Insert columns with # inst (avg)\n",
    "rounds_time_geomean_df.insert(\n",
    "    2 + NUM_SEEDS * (len(solver_stubs) + 1),\n",
    "    \"# inst (avg)\",\n",
    "    rounds_num_inst_avg\n",
    ")\n",
    "\n",
    "rounds_time_geomean_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rounds nodes summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rounds_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = selected_rounds_time_instances\n",
    "\n",
    "rounds_nodes_cols = \\\n",
    "    ['ALL' + ' ' + solver_type + ' ' + nodes_stub for solver_type in solver_stubs] \\\n",
    "    + \\\n",
    "    [ref_nodes_col, refv_nodes_col] \\\n",
    "    + \\\n",
    "    [col_num_vpc]\n",
    "new_rounds_nodes_cols = [ \n",
    "        [\n",
    "            solver_type + ' ' + '(%d)' % seed + ' ' + nodes_stub\n",
    "            for seed in range(1,NUM_SEEDS+1)\n",
    "        ]\n",
    "        for solver_type in solver_stubs\n",
    "    ]\n",
    "new_rounds_wins_cols = [\n",
    "    \"Wins ({} over {})\".format(solver_stubs[0], solver_stubs[1]),\n",
    "    \"Wins ({} over {})\".format(solver_stubs[1], solver_stubs[0])\n",
    "]\n",
    "rounds_nodes_df = df_rounds.loc[inst_set, rounds_nodes_cols]\n",
    "# display(disjet_timing_df.head())\n",
    "\n",
    "# Replace values of rounds_timing_df associated to ref with the values from disjset_timing_df for the same columns\n",
    "cols = ['ALL' + ' ' + solver_stubs[0] + ' ' + nodes_stub, ref_nodes_col]\n",
    "rounds_nodes_df.loc[:,cols] = disjset_nodes_df.loc[:,cols]\n",
    "\n",
    "# Add columns solver_type (%d) + ' ' + time_stub for the 7 random seeds\n",
    "# These will be parsed from the ALL REF TIME and ALL REF+V TIME columns,\n",
    "# which contain semicolon-separated values\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    for solver_ind in range(len(solver_stubs)):\n",
    "        solver_type = solver_stubs[solver_ind]\n",
    "        orig_col = rounds_nodes_cols[solver_ind]\n",
    "        new_col = new_rounds_nodes_cols[solver_ind][seed_ind]\n",
    "        rounds_nodes_df[new_col] = rounds_nodes_df[orig_col].str.split(';').str[seed_ind]\n",
    "        rounds_nodes_df[new_col] = rounds_nodes_df[new_col].astype(float)\n",
    "\n",
    "# Add wins columns\n",
    "for solver_ind in range(len(solver_stubs)):\n",
    "    other_solver_ind = 1 - solver_ind\n",
    "\n",
    "    rounds_nodes_df[new_rounds_wins_cols[solver_ind]] = sum(\n",
    "        rounds_nodes_df[new_rounds_nodes_cols[other_solver_ind][seed_ind]]\n",
    "        > \n",
    "        WIN_BY_TIME_FACTOR * rounds_nodes_df[new_rounds_nodes_cols[solver_ind][seed_ind]]\n",
    "        for seed_ind in range(NUM_SEEDS)\n",
    "    )\n",
    "\n",
    "display(rounds_nodes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_nodes_df.to_csv(\"rounds_nodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rounds_nodes_geomean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row names\n",
    "rounds_nodes_geomean_cols = [\n",
    "    [\n",
    "        new_rounds_nodes_cols[solver_ind][seed_ind] \n",
    "        for seed_ind in range(NUM_SEEDS) \n",
    "        for solver_ind in range(len(solver_stubs))\n",
    "    ]\n",
    "    + [ref_nodes_col, refv_nodes_col]\n",
    "    + new_rounds_wins_cols\n",
    "]\n",
    "rounds_nodes_geomean_df = pd.DataFrame(\n",
    "    columns = rounds_nodes_geomean_cols,\n",
    "    index = rounds_bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "rounds_nodes_geomean_df.loc[:,(rounds_nodes_geomean_cols[0])] = rounds_nodes_geomean_df.loc[:,(rounds_nodes_geomean_cols[0])].astype(object)\n",
    "\n",
    "# Prepare num_inst columns\n",
    "rounds_num_inst_by_seed_bucket_class = [\n",
    "    np.zeros(len(rounds_nodes_geomean_df),dtype = np.int64)\n",
    "    for _ in range(NUM_SEEDS) \n",
    "]\n",
    "rounds_num_inst_avg = np.zeros(len(rounds_nodes_geomean_df),dtype = np.int64)\n",
    "rounds_num_ref_seed_wins = np.zeros(len(rounds_nodes_geomean_df),dtype = np.int64)\n",
    "rounds_num_refv_seed_wins = np.zeros(len(rounds_nodes_geomean_df),dtype = np.int64)\n",
    "\n",
    "# For each class, calculate geomean\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    row_ind = 0\n",
    "    for class_ind in range(rounds_num_bb_classes):\n",
    "        print(\"\\nClass: {}\".format(rounds_bb_classes[class_ind]))\n",
    "        curr_df = rounds_nodes_df.loc[rounds_bb_classes_lists[class_ind]]\n",
    "        seed_cols = [ new_rounds_nodes_cols[solver_ind][seed_ind] for solver_ind in range(len(solver_stubs)) ]\n",
    "        seed_curr_df = curr_df[seed_cols]\n",
    "        \n",
    "        time_curr_df = rounds_timing_df.loc[rounds_bb_classes_lists[class_ind]]\n",
    "        time_seed_cols = [ new_rounds_timing_cols[solver_ind][seed_ind] for solver_ind in range(len(solver_stubs)) ]\n",
    "        seed_time_curr_df = time_curr_df[time_seed_cols]\n",
    "\n",
    "        # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply nodes processed by TIMEOUT_NODE_FACTOR\n",
    "        for col_ind in range(len(time_seed_cols)):\n",
    "            time_col = time_seed_cols[col_ind]\n",
    "            node_col = seed_cols[col_ind]\n",
    "            curr_selected_indices = seed_time_curr_df[time_col] >= MAX_TIME - EPS\n",
    "            seed_curr_df.loc[curr_selected_indices, node_col] = TIMEOUT_NODE_FACTOR * seed_curr_df[node_col]\n",
    "\n",
    "        for bucket_ind in range(num_buckets):\n",
    "            print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "            # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "            curr_selected_indices = \\\n",
    "                (seed_time_curr_df[time_seed_cols].min(axis=1) >= bucket_min[bucket_ind]) \\\n",
    "                & (seed_time_curr_df[time_seed_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "            seed_curr_df = seed_curr_df[curr_selected_indices]\n",
    "            seed_time_curr_df = seed_time_curr_df[curr_selected_indices]\n",
    "\n",
    "            # Calculate geomean\n",
    "            metric_ind = 0\n",
    "            rounds_nodes_geomean_df.loc[\n",
    "                (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                seed_cols\n",
    "            ] = [\n",
    "                    geometric_mean(seed_curr_df[col] + SHIFT_NODES) - SHIFT_NODES\n",
    "                    for col in seed_cols\n",
    "                ]\n",
    "            \n",
    "            # Calculate wins\n",
    "            metric_ind = 1\n",
    "            for solver_ind in range(len(solver_stubs)):\n",
    "                other_solver_ind = 1 - solver_ind\n",
    "                rounds_nodes_geomean_df.loc[\n",
    "                    (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                    seed_cols[solver_ind]\n",
    "                ] = sum(\n",
    "                    seed_curr_df[seed_cols[other_solver_ind]]\n",
    "                    > \n",
    "                    WIN_BY_NODES_FACTOR * seed_curr_df[seed_cols[solver_ind]]\n",
    "                )\n",
    "\n",
    "            print(\"row {:d}: {:d}\".format(row_ind,len(seed_curr_df)))\n",
    "            rounds_num_inst_by_seed_bucket_class[seed_ind][row_ind:row_ind+len(bb_metrics)] = [len(seed_curr_df)] * len(bb_metrics)\n",
    "\n",
    "            row_ind += len(bb_metrics)\n",
    "\n",
    "row_ind = 0\n",
    "nodes_cols = [ ref_nodes_col, refv_nodes_col ]\n",
    "time_cols = [ ref_time_col, refv_time_col ]\n",
    "wins_cols = new_rounds_wins_cols\n",
    "for class_ind in range(rounds_num_bb_classes):\n",
    "    print(\"\\nClass: {}\".format(rounds_bb_classes[class_ind]))\n",
    "    curr_df = rounds_nodes_df.loc[rounds_bb_classes_lists[class_ind]]\n",
    "    time_curr_df = rounds_timing_df.loc[rounds_bb_classes_lists[class_ind]]\n",
    "\n",
    "    # For every instance in which curr_by_depth_df[col] >= MAX_TIME - EPS, multiply nodes processed by TIMEOUT_NODE_FACTOR\n",
    "    for col_ind in range(len(time_cols)):\n",
    "        time_col = time_cols[col_ind]\n",
    "        node_col = nodes_cols[col_ind]\n",
    "        curr_selected_indices = time_curr_df[time_col] >= MAX_TIME - EPS\n",
    "        curr_df.loc[curr_selected_indices, node_col] = TIMEOUT_NODE_FACTOR * curr_df[node_col]\n",
    "        \n",
    "    for bucket_ind in range(num_buckets):\n",
    "        print(\"Bucket: [{:d},{:d})\".format(bucket_min[bucket_ind],bucket_max[bucket_ind]))\n",
    "\n",
    "        # Take subset of instances for which both solvers (with this seed) solve the instance in the time frame for the bucket\n",
    "        curr_selected_indices = \\\n",
    "            (time_curr_df[time_cols].min(axis=1) >= bucket_min[bucket_ind]) \\\n",
    "            & (time_curr_df[time_cols].min(axis=1) < bucket_max[bucket_ind] - EPS)\n",
    "        curr_df = curr_df[curr_selected_indices]\n",
    "        time_curr_df = time_curr_df[curr_selected_indices]\n",
    "\n",
    "        # Calculate geomean\n",
    "        metric_ind = 0\n",
    "        rounds_nodes_geomean_df.loc[\n",
    "            (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "            nodes_cols\n",
    "        ] = [\n",
    "                geometric_mean(curr_df[col] + SHIFT_NODES) - SHIFT_NODES\n",
    "                for col in nodes_cols\n",
    "            ]\n",
    "        \n",
    "        # Calculate wins\n",
    "        metric_ind = 1\n",
    "        for solver_ind in range(len(solver_stubs)):\n",
    "            other_solver_ind = 1 - solver_ind\n",
    "            rounds_nodes_geomean_df.loc[\n",
    "                (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[metric_ind]),\n",
    "                nodes_cols[solver_ind]\n",
    "            ] = sum(\n",
    "                curr_df[nodes_cols[other_solver_ind]]\n",
    "                > \n",
    "                WIN_BY_NODES_FACTOR * curr_df[nodes_cols[solver_ind]]\n",
    "            )\n",
    "\n",
    "            # Also report average number of seeds for which REF and REF+V win\n",
    "            rounds_nodes_geomean_df.loc[\n",
    "                (rounds_bb_classes[class_ind], bb_buckets[bucket_ind], bb_metrics[0:len(bb_metrics)]),\n",
    "                wins_cols[solver_ind]\n",
    "            ] = sum(curr_df[wins_cols[solver_ind]]) / len(curr_df)\n",
    "\n",
    "        print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "        rounds_num_inst_avg[row_ind:row_ind+len(bb_metrics)] = [len(curr_df)] * len(bb_metrics)\n",
    "\n",
    "        row_ind += len(bb_metrics)\n",
    "\n",
    "# Insert columns with # inst (seed)\n",
    "for seed_ind in range(NUM_SEEDS):\n",
    "    rounds_nodes_geomean_df.insert(\n",
    "        2 + seed_ind * (len(solver_stubs) + 1),\n",
    "        \"# inst ({:d})\".format(seed_ind+1),\n",
    "        rounds_num_inst_by_seed_bucket_class[seed_ind]\n",
    "    )\n",
    "\n",
    "# Insert columns with # inst (avg)\n",
    "rounds_nodes_geomean_df.insert(\n",
    "    2 + NUM_SEEDS * (len(solver_stubs) + 1),\n",
    "    \"# inst (avg)\",\n",
    "    rounds_num_inst_avg\n",
    ")\n",
    "\n",
    "rounds_nodes_geomean_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: `rounds_avg_bb_df` for rounds instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up empty `rounds_avg_bb_df`\n",
    "\n",
    "rounds_time_cols_short = time_cols_short\n",
    "rounds_node_cols_short = node_cols_short\n",
    "\n",
    "rounds_avg_bb_cols = pd.MultiIndex.from_arrays(\n",
    "    [\n",
    "      [time_col_header]*len(rounds_time_cols_short) + [node_col_header]*len(rounds_node_cols_short),\n",
    "      rounds_time_cols_short + rounds_node_cols_short\n",
    "    ],\n",
    "    names = ['criterion', 'type'])\n",
    "\n",
    "#bb_row_names = pd.MultiIndex.from_product([bb_buckets, bb_row_names], names=['bucket', 'metric'])\n",
    "rounds_bb_row_names = pd.MultiIndex.from_product(\n",
    "    [rounds_bb_classes, bb_buckets, bb_metrics],\n",
    "    names=['class', 'bucket', 'metric'])\n",
    "\n",
    "rounds_avg_bb_df = pd.DataFrame(\n",
    "    columns = rounds_avg_bb_cols,\n",
    "    index = rounds_bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "rounds_avg_bb_df.loc[:,(time_col_header,rounds_time_cols_short)] = rounds_avg_bb_df.loc[:,(time_col_header,rounds_time_cols_short)].astype(object)\n",
    "rounds_avg_bb_df.loc[:,(node_col_header,rounds_node_cols_short)] = rounds_avg_bb_df.loc[:,(node_col_header,rounds_node_cols_short)].astype(object)\n",
    "\n",
    "num_inst = np.zeros(len(rounds_avg_bb_df),dtype = np.int64)\n",
    "row_ind = 0\n",
    "cols = rounds_time_cols_short + rounds_node_cols_short\n",
    "\n",
    "# Fill in values from rounds_time_geomean_df and rounds_nodes_geomean_df (from the ref and refv cols)\n",
    "# Fill in values from rounds_time_geomean_df\n",
    "for index, row in rounds_time_geomean_df.iterrows():\n",
    "    class_val, bucket_val, metric_val = index\n",
    "    for col in rounds_time_cols_short:\n",
    "        rounds_avg_bb_df.loc[(class_val, bucket_val, metric_val), (time_col_header, col)] = \\\n",
    "            row[map_short_to_cols_time[col]]\n",
    "\n",
    "# Fill in values from rounds_nodes_geomean_df\n",
    "for index, row in rounds_nodes_geomean_df.iterrows():\n",
    "    class_val, bucket_val, metric_val = index\n",
    "    for col in rounds_node_cols_short:\n",
    "        rounds_avg_bb_df.loc[(class_val, bucket_val, metric_val), (node_col_header, col)] = \\\n",
    "            row[map_short_to_cols_nodes[col]]\n",
    "        \n",
    "# Fill in num inst\n",
    "rounds_avg_bb_df[inst_col_name] = rounds_num_inst_avg\n",
    "\n",
    "display(rounds_avg_bb_df.loc[:,rounds_avg_bb_df.columns.get_level_values(0)==node_col_header].head(6))\n",
    "#display(avg_bb_df.loc[(bb_classes[0], bb_buckets[1], bb_metrics[0]),:])\n",
    "display(rounds_avg_bb_df.loc[(rounds_bb_classes, bb_buckets, bb_metrics[0]),:])\n",
    "display(rounds_avg_bb_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `avg_bb_df` which will have values from disjset, rounds, and avg_bb_df_by_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge disjset, rounds, and avg_bb_df_by_depth for 'All' class\n",
    "# avg_bb_df = disjset_avg_bb_df.loc[(disjset_bb_classes[0], bb_buckets, bb_metrics),:]\n",
    "avg_bb_df = pd.concat([\n",
    "        disjset_avg_bb_df.loc[(disjset_bb_classes[0], bb_buckets, bb_metrics),:],\n",
    "        rounds_avg_bb_df.loc[(rounds_bb_classes[0], bb_buckets, bb_metrics),:],\n",
    "        avg_bb_by_depth_df.loc[:,avg_bb_by_depth_df.columns != ('Time (s)', 'Total')]\n",
    "    ], \n",
    "    axis=0)\n",
    "\n",
    "# Fill missing values with 0\n",
    "avg_bb_df = avg_bb_df.fillna(0)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "avg_bb_df.head(20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Objective and time analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `obj_and_time_df`: objectives, successes, fails, and time per obj or cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = best_gap_df.index\n",
    "# inst_set = ['10teams_presolved', 'bm23_presolved', 'vpm1_presolved']\n",
    "\n",
    "# Define rows to add\n",
    "inst_depth_set = [(inst, best_gap_df.loc[inst, 'BEST VPC DISJ']) for inst in inst_set]\n",
    "\n",
    "# Define columns to add\n",
    "fail_rate_col_name = 'Fail rate (%)'\n",
    "time_col_name = 'Time (s)'\n",
    "sec_per_obj_col_name = '(s) / obj'\n",
    "sec_per_cut_col_name = '(s) / cut'\n",
    "obj_and_time_new_cols = [\n",
    "    fail_rate_col_name,\n",
    "    time_col_name,\n",
    "    sec_per_obj_col_name,\n",
    "    sec_per_cut_col_name,\n",
    "]\n",
    "\n",
    "obj_and_time_df = df.loc[inst_depth_set,[col_num_obj, col_num_vpc, col_num_fails]].copy(deep=True)\n",
    "obj_and_time_df[fail_rate_col_name] = 100. * obj_and_time_df[col_num_fails] / obj_and_time_df[col_num_obj]\n",
    "obj_and_time_df[time_col_name] = df[col_vpc_gen_time]\n",
    "obj_and_time_df[sec_per_obj_col_name] = obj_and_time_df[time_col_name] / obj_and_time_df[col_num_obj]\n",
    "obj_and_time_df[sec_per_cut_col_name] = obj_and_time_df[time_col_name] / obj_and_time_df[col_num_vpc]\n",
    "\n",
    "# Replace Fail rate = NaN when all cuts are one-sided cuts\n",
    "SKIP_CHAR = '-'\n",
    "obj_and_time_df.fillna(SKIP_CHAR, inplace = True)\n",
    "obj_and_time_df.replace(np.inf, SKIP_CHAR, inplace = True)\n",
    "\n",
    "# Add average row\n",
    "# obj_and_time_df.loc['Average'] = 0\n",
    "obj_and_time_df.loc['Average', obj_and_time_new_cols] =\\\n",
    "    [obj_and_time_df[obj_and_time_df[col] != SKIP_CHAR][col].mean() for col in obj_and_time_new_cols]\n",
    "# for col in obj_and_time_new_cols:\n",
    "#     obj_and_time_df.at[('Average',0),col] =\\\n",
    "#         obj_and_time_df[obj_and_time_df[col] != SKIP_CHAR][col].mean()\n",
    "\n",
    "obj_and_time_df.loc['Average',[col_num_obj, col_num_vpc, col_num_fails]] = \"\"\n",
    "\n",
    "display(obj_and_time_df)\n",
    "# obj_and_time_df[obj_and_time_df['NUM CUTS'] == 0]\n",
    "# obj_and_time_df[obj_and_time_df['(s) / obj'] > 100000]\n",
    "# obj_and_time_df.loc['neos18_presolved']\n",
    "# obj_and_time_df[obj_and_time_df['(s) / obj'] != SKIP_CHAR]['(s) / obj'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = obj_and_time_new_cols[0]\n",
    "print(selected_col)\n",
    "\n",
    "# Print string entries for column\n",
    "curr_col = obj_and_time_df[selected_col]\n",
    "\n",
    "# Check if any entries of curr_col cannot be a float\n",
    "meanvals = 0.0\n",
    "maxval = 0.0\n",
    "numvals = 0\n",
    "for val in curr_col:\n",
    "  try:\n",
    "    floatval = np.float64(val)\n",
    "    meanvals += floatval\n",
    "    numvals += 1\n",
    "    if floatval > maxval:\n",
    "      maxval = floatval\n",
    "  except:\n",
    "    print(\"val {} not convertible\".format(val))\n",
    "print(meanvals / numvals)\n",
    "print(maxval)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG finding max difference in time between TOTAL_TIME and sum of individual times\n",
    "# cuts_cols = [col for col in df.columns if col.startswith('NUM CUTS')]\n",
    "# time_cols = [\n",
    "#     'INIT_SOLVE_TIME',\n",
    "#     'VPC_GEN_TIME',\n",
    "#     'VPC_APPLY_TIME',\n",
    "#     'BB_TIME',\n",
    "#     'TOTAL_TIME'\n",
    "# ]\n",
    "# display(df.loc['bell3b_presolved',['NUM OBJ', 'NUM FAILS'] + cuts_cols])\n",
    "\n",
    "# obj_and_time_df = df.loc[inst_depth_set].copy(deep = True)['NUM OBJ', 'NUM CUTS', 'NUM FAILS', 'VPC_GEN_TIME']\n",
    "# display(obj_and_time_df)\n",
    "\n",
    "# max_diff_time = 0.\n",
    "# max_diff_inst = ''\n",
    "# for inst in best_gap_df.index:\n",
    "#     depth = best_gap_df.loc[inst, 'BEST VPC DISJ']\n",
    "#     curr_num_obj   = df.loc[(inst,depth)]['NUM OBJ']\n",
    "#     curr_num_vpc   = df.loc[(inst,depth)]['NUM VPC']\n",
    "#     curr_num_1side = df.loc[(inst,depth)]['NUM CUTS ONE_SIDED']\n",
    "#     curr_num_fails = df.loc[(inst,depth)]['NUM FAILS']\n",
    "#     if curr_num_vpc + curr_num_fails != curr_num_obj + curr_num_1side:\n",
    "#         raise ValueError(\"{}: curr_num_vpc ({:d}) + curr_num_fails ({:d}) != curr_num_obj ({:d}) + curr_num_1side ({:d})\".format(inst, curr_num_vpc, curr_num_fails, curr_num_obj, curr_num_1side))\n",
    "    \n",
    "#     curr_fail_pct = 100. * curr_num_fails / curr_num_obj\n",
    "#     curr_init_solve = df.loc[(inst,depth)]['INIT_SOLVE_TIME']\n",
    "#     curr_vpc_gen = df.loc[(inst,depth)]['VPC_GEN_TIME']\n",
    "#     curr_vpc_apply = df.loc[(inst,depth)]['VPC_APPLY_TIME']\n",
    "#     curr_bb_time = df.loc[(inst,depth)]['BB_TIME']\n",
    "#     curr_total_time = df.loc[(inst,depth)]['TOTAL_TIME']\n",
    "\n",
    "#     curr_diff_time = curr_total_time - (curr_init_solve + curr_vpc_gen + curr_vpc_apply + curr_bb_time)\n",
    "#     if curr_diff_time < -EPS:\n",
    "#         display(df.loc[inst,time_cols])\n",
    "#         raise ValueError(\"{} (depth {:d}): curr_diff_time {} < 0.\".format(inst,depth,curr_diff_time))\n",
    "    \n",
    "#     if max_diff_time < curr_diff_time:\n",
    "#         max_diff_inst = inst\n",
    "#         max_diff_time = curr_diff_time\n",
    "\n",
    "# print(\"Max diff time = {} for inst {}\".format(max_diff_time,max_diff_inst))\n",
    "# display(df.loc[(max_diff_inst,best_gap_df.loc[max_diff_inst, 'BEST VPC DISJ']),time_cols])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `best_disj_gap_df`: Number of times a particular depth achieves best result and beats baseline by at least EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_cols_to_compare = {\n",
    "    col_vpc:                      col_gmic,\n",
    "    col_vpc_gmic:                 col_gmic,\n",
    "    col_first_cut_pass_gap_ref_v: col_first_cut_pass_gap_ref,\n",
    "    col_last_cut_pass_gap_ref_v:  col_last_cut_pass_gap_ref,\n",
    "}\n",
    "short_cols_to_compare = [map_cols_to_short_gap[col] for col in long_cols_to_compare.keys()]\n",
    "row_no_improvement = 'No improvement'\n",
    "\n",
    "best_disj_gap_df = pd.DataFrame(\n",
    "    columns = short_cols_to_compare,\n",
    "    index = [row_no_improvement] + [0] + sizes,\n",
    "    dtype = int,\n",
    ")\n",
    "\n",
    "for curr_depth in [0] + sizes:\n",
    "    curr_depth_df = selected_gap_df.xs(curr_depth,level='disj_terms')\n",
    "    for col in long_cols_to_compare.keys():\n",
    "        shortcol = map_cols_to_short_gap[col]\n",
    "        refcol = long_cols_to_compare[col]\n",
    "        \n",
    "        # Calculate num times this depth yielded the best result\n",
    "        curr_num_wins = sum(\n",
    "            (curr_depth_df[col] == best_gap_df[shortcol]) & \n",
    "            (best_gap_df[shortcol] > best_gap_df[map_cols_to_short_gap[refcol]] + GAP_DIFF_EPS)\n",
    "        )\n",
    "        best_disj_gap_df.at[curr_depth,shortcol] = curr_num_wins\n",
    "        \n",
    "# Add no improvement row\n",
    "curr_depth = row_no_improvement\n",
    "for col in long_cols_to_compare.keys():\n",
    "    shortcol = map_cols_to_short_gap[col]\n",
    "    refcol = long_cols_to_compare[col]\n",
    "\n",
    "    # Calculate num times no improvement over the baseline\n",
    "    curr_num_wins = sum(\n",
    "            (best_gap_df[shortcol] <= best_gap_df[map_cols_to_short_gap[refcol]] + GAP_DIFF_EPS)\n",
    "        )\n",
    "    best_disj_gap_df.at[curr_depth,shortcol] = curr_num_wins\n",
    "\n",
    "\n",
    "# Reindex to add \"leaves\" to index\n",
    "idx = [row_no_improvement] + ['Best'] + [str(size) + \" leaves\" for size in sizes]\n",
    "reidx = {old_id : new_id for old_id, new_id in zip(best_disj_gap_df.index,idx)}\n",
    "best_disj_gap_df.rename(reidx, inplace=True)\n",
    "\n",
    "# Remove best row (it is good to verify this is the same as the relevant entries in win_df or Table 1)\n",
    "best_disj_gap_df.drop('Best', axis=0, inplace=True)\n",
    "\n",
    "# Make sure all cols are int\n",
    "best_disj_gap_df = best_disj_gap_df.astype(int)\n",
    "\n",
    "best_disj_gap_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `best_disj_time_df`: Number of times depth is best and improvement is at least 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_no_improvement = 'No improvement'\n",
    "\n",
    "# sizes_to_check = [0] + sizes\n",
    "sizes_to_check = sizes\n",
    "\n",
    "best_disj_time_df = pd.DataFrame(\n",
    "    columns = bb_classes,\n",
    "    index = [row_no_improvement] + sizes_to_check,\n",
    "    dtype = int,\n",
    ")\n",
    "\n",
    "cols_time_by_depth = time_cols_long[:2] #[ref_time_col, refv_time_col, refv_w_cut_time_col]\n",
    "curr_df = df.loc[selected_time_instances, cols_time_by_depth + [col_num_vpc]]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# Calculate minimum value for each column, per instance (index level 0)\n",
    "best_time_df = curr_df.groupby(level=0).min()\n",
    "display(best_time_df)\n",
    "\n",
    "# Report shifted geometric mean of both columns\n",
    "for col in cols_time_by_depth:\n",
    "    print(\"{}: {}\".format(col, geometric_mean(best_time_df[col] + SHIFT_TIME) - SHIFT_TIME))\n",
    "\n",
    "inst_sets = [selected_time_instances, all6_instances, binary_x_time_instances]\n",
    "\n",
    "# display(curr_df.xs(2,level='disj_terms'))\n",
    "# display(curr_df[curr_df.index.get_level_values(1) == 2])\n",
    "\n",
    "for curr_size in sizes_to_check:\n",
    "    curr_by_depth_df = curr_df.xs(curr_size,level='disj_terms')\n",
    "\n",
    "    # Check if the value for this depth is roughly the best value overall (in best_time_df)\n",
    "    bb_class_ind = 0\n",
    "    for inst_set in inst_sets:\n",
    "        curr_class_df = curr_by_depth_df.loc[inst_set]\n",
    "        curr_best_df = best_time_df.loc[inst_set]\n",
    "\n",
    "        # Give wiggle room of ~5% for the best time\n",
    "        curr_num_wins = sum(\n",
    "            (curr_class_df[refv_time_col] < 1.05 * curr_best_df[refv_time_col]) & \n",
    "            (curr_best_df[ref_time_col] > WIN_BY_TIME_FACTOR * curr_best_df[refv_time_col])\n",
    "        )\n",
    "        best_disj_time_df.at[curr_size,bb_classes[bb_class_ind]] = curr_num_wins\n",
    "        \n",
    "        # Increment bb_class_ind for next set\n",
    "        bb_class_ind += 1\n",
    "\n",
    "# Add no improvement row\n",
    "# Calculate num times no improvement over the baseline\n",
    "curr_size = row_no_improvement\n",
    "\n",
    "bb_class_ind = 0\n",
    "for inst_set in inst_sets:\n",
    "    curr_best_df = best_time_df.loc[inst_set]\n",
    "    curr_num_wins = sum(\n",
    "            (curr_best_df[ref_time_col] <= WIN_BY_TIME_FACTOR * curr_best_df[refv_time_col])\n",
    "        )\n",
    "    best_disj_time_df.at[curr_size,bb_classes[bb_class_ind]] = curr_num_wins\n",
    "    bb_class_ind += 1\n",
    "\n",
    "# Convert to int\n",
    "best_disj_time_df = best_disj_time_df.astype(int)\n",
    "\n",
    "# Reindex to add \"leaves\" to index\n",
    "idx = [row_no_improvement] + [str(size) + \" leaves\" for size in sizes]\n",
    "reidx = {old_id : new_id for old_id, new_id in zip(best_disj_time_df.index,idx)}\n",
    "best_disj_time_df.rename(reidx, inplace=True)\n",
    "\n",
    "# Remove best row (it is good to verify this is the same as the relevant entries in win_df or Table 1)\n",
    "# best_disj_time_df.drop('Best', axis=0, inplace=True)\n",
    "\n",
    "best_disj_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `best_disj_nodes_df`: Number of times depth is best for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_nodes_by_depth = nodes_cols[:2] #[ref_time_col, refv_time_col, refv_w_cut_time_col]\n",
    "# curr_df = df.loc[selected_time_instances, cols_time_by_depth + cols_nodes_by_depth + [col_num_vpc]]\n",
    "# curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# # Calculate minimum value for each column, per instance (index level 0)\n",
    "# curr_argmin_df = curr_df.groupby(level=0).idxmin()\n",
    "# best_df = curr_df.loc[curr_argmin_df[refv_time_col]]\n",
    "#cols_nodes_by_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_no_improvement = 'No improvement'\n",
    "\n",
    "# sizes_to_check = [0] + sizes\n",
    "sizes_to_check = sizes\n",
    "\n",
    "best_disj_nodes_df = pd.DataFrame(\n",
    "    columns = bb_classes,\n",
    "    index = [row_no_improvement] + sizes_to_check,\n",
    "    dtype = int,\n",
    ")\n",
    "\n",
    "cols_nodes_by_depth = nodes_cols[:2] #[ref_time_col, refv_time_col, refv_w_cut_time_col]\n",
    "curr_df = df.loc[selected_time_instances, cols_nodes_by_depth + [col_num_vpc]]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# Calculate minimum value for each column, per instance (index level 0)\n",
    "best_nodes_df = curr_df.groupby(level=0).min()\n",
    "display(best_nodes_df)\n",
    "\n",
    "# Report shifted geometric mean of both columns\n",
    "for col in cols_nodes_by_depth:\n",
    "    print(\"{}: {}\".format(col, geometric_mean(best_nodes_df[col] + SHIFT_NODES) - SHIFT_NODES))\n",
    "\n",
    "inst_sets = [selected_time_instances, all6_instances, binary_x_time_instances]\n",
    "\n",
    "# display(curr_df.xs(2,level='disj_terms'))\n",
    "# display(curr_df[curr_df.index.get_level_values(1) == 2])\n",
    "\n",
    "for curr_size in sizes_to_check:\n",
    "    curr_by_depth_df = curr_df.xs(curr_size,level='disj_terms')\n",
    "\n",
    "    # Check if the value for this depth is roughly the best value overall (in best_time_df)\n",
    "    bb_class_ind = 0\n",
    "    for inst_set in inst_sets:\n",
    "        curr_class_df = curr_by_depth_df.loc[inst_set]\n",
    "        curr_best_df = best_nodes_df.loc[inst_set]\n",
    "\n",
    "        # Give wiggle room of ~5% for the best time\n",
    "        curr_num_wins = sum(\n",
    "            (curr_class_df[refv_nodes_col] <= curr_best_df[refv_nodes_col]) & \n",
    "            (curr_best_df[ref_nodes_col] > WIN_BY_NODES_FACTOR * curr_best_df[refv_nodes_col])\n",
    "        )\n",
    "        best_disj_nodes_df.at[curr_size,bb_classes[bb_class_ind]] = curr_num_wins\n",
    "        \n",
    "        # Increment bb_class_ind for next set\n",
    "        bb_class_ind += 1\n",
    "\n",
    "# Add no improvement row\n",
    "# Calculate num times no improvement over the baseline\n",
    "curr_size = row_no_improvement\n",
    "\n",
    "bb_class_ind = 0\n",
    "for inst_set in inst_sets:\n",
    "    curr_best_df = best_nodes_df.loc[inst_set]\n",
    "    curr_num_wins = sum(\n",
    "            (curr_best_df[ref_nodes_col] <= WIN_BY_NODES_FACTOR * curr_best_df[refv_nodes_col])\n",
    "        )\n",
    "    best_disj_nodes_df.at[curr_size,bb_classes[bb_class_ind]] = curr_num_wins\n",
    "    bb_class_ind += 1\n",
    "\n",
    "# Convert to int\n",
    "best_disj_nodes_df = best_disj_nodes_df.astype(int)\n",
    "\n",
    "# Reindex to add \"leaves\" to index\n",
    "idx = [row_no_improvement] + [str(size) + \" leaves\" for size in sizes]\n",
    "reidx = {old_id : new_id for old_id, new_id in zip(best_disj_nodes_df.index,idx)}\n",
    "best_disj_nodes_df.rename(reidx, inplace=True)\n",
    "\n",
    "# Remove best row (it is good to verify this is the same as the relevant entries in win_df or Table 1)\n",
    "# best_disj_time_df.drop('Best', axis=0, inplace=True)\n",
    "\n",
    "best_disj_nodes_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `density_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    '\\# inst w/VPCs and time < 3600s',\n",
    "    '\\# wins by time',\n",
    "    'Avg min cut density',\n",
    "    'Avg max cut density',\n",
    "    'Avg avg cut density',\n",
    "    'Avg avg cut density (win by time)',\n",
    "    'Avg avg cut density (lose by time)',\n",
    "]\n",
    "\n",
    "columns = ['V ({:d})'.format(size) for size in sizes]\n",
    "\n",
    "density_df = pd.DataFrame(\n",
    "    columns = columns,\n",
    "    index = rows,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Calculate stats for 6 trees instances by depth\n",
    "# inst_set = all6_instances_dict.keys()\n",
    "inst_set = selected_time_instances\n",
    "support_cols = [col for col in df.columns if \"SUPPORT VPC\" in col]\n",
    "selected_cols = [ref_time_col, refv_time_col]+support_cols+[col_num_cols,col_num_vpc,ref_timeout_col,refv_timeout_col]\n",
    "selected_cols = list(set(selected_cols)) # remove duplicates\n",
    "curr_df = df.loc[inst_set,selected_cols]\n",
    "\n",
    "# Take only indices that are > 0\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# curr_df0 = curr_df.xs(0,level='disj_terms')\n",
    "\n",
    "for curr_size_ind in range(0,len(sizes)):\n",
    "    # Select only this depth\n",
    "    # curr_by_depth_df = curr_df[curr_df.index.get_level_values(1) == sizes[curr_size_ind]]\n",
    "    curr_by_depth_df = curr_df.xs(sizes[curr_size_ind], level='disj_terms')\n",
    "    \n",
    "    # Remove instances that take more than an hour\n",
    "    INSTANCES_TO_KEEP = curr_by_depth_df[[ref_timeout_col, refv_timeout_col]].min(axis=1) < MAX_TIME\n",
    "    curr_by_depth_df = curr_by_depth_df[INSTANCES_TO_KEEP]\n",
    "\n",
    "    # Count number of instances having cuts\n",
    "    curr_row_ind = 0\n",
    "    density_df.iloc[curr_row_ind,curr_size_ind] = sum(curr_by_depth_df[col_num_vpc] > 0)\n",
    "    \n",
    "    # Mean of min, max, avg density\n",
    "    curr_row_ind = 2\n",
    "    for col_ind in range(len(support_cols)):\n",
    "        curr_series = curr_by_depth_df[support_cols[col_ind]] / curr_by_depth_df[col_num_cols]\n",
    "        density_df.iloc[curr_row_ind,curr_size_ind] = curr_series.mean()\n",
    "        curr_row_ind += 1\n",
    "\n",
    "    ## Count wins1 (should be same as in avg_bb_by_depth_df)\n",
    "    # A win in terms of time is counted when the ``Gur1'' baseline seconds taken \n",
    "    # is at least 10\\% slower, to account for some variability in runtimes.\n",
    "    # A win in terms of nodes is when the ``Gur1'' baseline number of nodes is higher.\n",
    "    curr_wins_df = curr_by_depth_df[curr_by_depth_df[ref_time_col] > WIN_BY_TIME_FACTOR * curr_by_depth_df[refv_time_col]]\n",
    "    curr_row_ind = 1\n",
    "    density_df.iloc[curr_row_ind,curr_size_ind] = len(curr_wins_df)\n",
    "    curr_row_ind = len(rows)-2\n",
    "    density_df.iloc[curr_row_ind,curr_size_ind] = (curr_wins_df[support_cols[2]] / curr_wins_df[col_num_cols]).mean()\n",
    "\n",
    "    # curr_lose_df = curr_by_depth_df[1.1*curr_df0[gur1time_col] < curr_by_depth_df[gur1vtime_col]]\n",
    "    curr_lose_df = curr_by_depth_df[WIN_BY_TIME_FACTOR * curr_by_depth_df[ref_time_col] < curr_by_depth_df[refv_time_col]]\n",
    "    #curr_lose_df = curr_by_depth_df[curr_by_depth_df[ref_time_col] <= WIN_BY_TIME_FACTOR * curr_by_depth_df[refv_time_col]]\n",
    "    curr_row_ind = len(rows)-1\n",
    "    density_df.iloc[curr_row_ind,curr_size_ind] = (curr_lose_df[support_cols[2]] / curr_lose_df[col_num_cols]).mean()\n",
    "\n",
    "density_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `obj_fails_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = best_gap_df.index\n",
    "# inst_set = ['10teams_presolved', 'bm23_presolved', 'vpm1_presolved']\n",
    "\n",
    "# Define rows to add\n",
    "inst_depth_set = [(inst, best_gap_df.loc[inst, 'BEST VPC DISJ']) for inst in inst_set]\n",
    "\n",
    "rows = [\n",
    "    '\\# inst w/obj',\n",
    "    '\\# inst w/succ obj',\n",
    "    '\\# inst no obj',\n",
    "    '\\# inst all obj fail',\n",
    "    '\\# inst all obj succ',\n",
    "    '\\% obj fails',\n",
    "    '\\% fails dup',\n",
    "    '\\% fails unbdd',\n",
    "    '\\% fails tilim',\n",
    "    '\\% fails dyn',\n",
    "    '\\% fails all ones',\n",
    "    '\\% fails post-GMIC obj',\n",
    "    '\\% fails DB',\n",
    "    '\\# obj / cut',\n",
    "    '(s) / obj',\n",
    "    '(s) / cut',\n",
    "]\n",
    "\n",
    "columns = ['V ({:d})'.format(size) for size in sizes]\n",
    "\n",
    "obj_fails_df = pd.DataFrame(\n",
    "    columns = columns,\n",
    "    index = rows,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Choose columns to pull\n",
    "selected_cols = [\n",
    "    col_num_obj,\n",
    "    col_num_vpc,\n",
    "    'NUM FAILS DUMMY_OBJ',\n",
    "    'NUM FAILS ALL_ONES',\n",
    "    'NUM FAILS CUT_VERTICES',\n",
    "    'NUM FAILS ITER_BILINEAR',\n",
    "    'NUM FAILS UNIT_VECTORS',\n",
    "    'NUM FAILS DISJ_LB',\n",
    "    'NUM FAILS TIGHT_POINTS',\n",
    "    'NUM FAILS TIGHT_RAYS',\n",
    "    'NUM FAILS TIGHT_POINTS2',\n",
    "    'NUM FAILS TIGHT_RAYS2',\n",
    "    'NUM FAILS USER',\n",
    "    'NUM FAILS OBJ_CUT',\n",
    "    'NUM FAILS ONE_SIDED',\n",
    "    'NUM FAILS',\n",
    "    'ABANDONED',\n",
    "    'BAD_DYNAMISM',\n",
    "    'BAD_SUPPORT',\n",
    "    'BAD_VIOLATION',\n",
    "    'CUT_LIMIT',\n",
    "    'DUAL_INFEASIBLE',\n",
    "    'DUPLICATE_SIC',\n",
    "    'DUPLICATE_VPC',\n",
    "    'ITERATION_LIMIT',\n",
    "    'ORTHOGONALITY_SIC',\n",
    "    'ORTHOGONALITY_VPC',\n",
    "    'PRIMAL_INFEASIBLE',\n",
    "    'TIME_LIMIT',\n",
    "    'NUMERICAL_ISSUES_WARNING',\n",
    "    'DLB_EQUALS_DUB_NO_OBJ',\n",
    "    'DLB_EQUALS_LPOPT_NO_OBJ',\n",
    "    'PRIMAL_INFEASIBLE_NO_OBJ',\n",
    "    'NUMERICAL_ISSUES_NO_OBJ',\n",
    "    'UNKNOWN',\n",
    "    col_vpc_gen_time,\n",
    "]\n",
    "\n",
    "curr_df = df.loc[inst_set,selected_cols]\n",
    "curr_df0 = curr_df.xs(0,level='disj_terms')\n",
    "\n",
    "for curr_size_ind in range(0,len(sizes)):\n",
    "    # Select only this depth\n",
    "    # curr_by_depth_df = curr_df[curr_df.index.get_level_values(1) == sizes[curr_size_ind]]\n",
    "    curr_by_depth_df = curr_df.xs(sizes[curr_size_ind], level='disj_terms')\n",
    "\n",
    "    # Num inst with objectives tried\n",
    "    obj_fails_df.iloc[0,curr_size_ind] =\\\n",
    "        sum(curr_by_depth_df[col_num_obj] > 0)\n",
    "\n",
    "    # Num inst with successful objectives\n",
    "    obj_fails_df.iloc[1,curr_size_ind] =\\\n",
    "        sum((curr_by_depth_df[col_num_obj] > 0) & (curr_by_depth_df[col_num_vpc] > 0))\n",
    "\n",
    "    # Num inst with no objectives tried\n",
    "    obj_fails_df.iloc[2,curr_size_ind] =\\\n",
    "        sum(curr_by_depth_df[col_num_obj] == 0)\n",
    "\n",
    "    # Num inst with objectives tried but all failed\n",
    "    obj_fails_df.iloc[3,curr_size_ind] =\\\n",
    "        sum((curr_by_depth_df[col_num_obj] > 0) & (curr_by_depth_df[col_num_vpc] == 0))\n",
    "\n",
    "    # Num inst with objectives tried and all succeeded\n",
    "    obj_fails_df.iloc[4,curr_size_ind] =\\\n",
    "        sum((curr_by_depth_df[col_num_obj] > 0) & (curr_by_depth_df[col_num_obj] == curr_by_depth_df[col_num_vpc]))\n",
    "\n",
    "    # Percent objective failures\n",
    "    inst_w_obj_df = curr_by_depth_df[curr_by_depth_df[col_num_obj] > 0]\n",
    "    obj_fails_df.iloc[5,curr_size_ind] =\\\n",
    "        (100. * inst_w_obj_df[col_num_fails] / inst_w_obj_df[col_num_obj]).mean()\n",
    "\n",
    "    ## Percent of failures caused by:\n",
    "    inst_w_fails_df = curr_by_depth_df[curr_by_depth_df[col_num_fails] > 0]\n",
    "\n",
    "    # duplicates\n",
    "    obj_fails_df.iloc[6,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['DUPLICATE_VPC']+inst_w_fails_df['DUPLICATE_SIC']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "\n",
    "    # unbdd\n",
    "    obj_fails_df.iloc[7,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['DUAL_INFEASIBLE']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "\n",
    "    # tilim\n",
    "    obj_fails_df.iloc[8,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['TIME_LIMIT']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "    \n",
    "    # dynamism\n",
    "    obj_fails_df.iloc[9,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['BAD_DYNAMISM']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "\n",
    "    # all-ones\n",
    "    obj_fails_df.iloc[10,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['NUM FAILS ALL_ONES']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "\n",
    "    # post-GMIC\n",
    "    obj_fails_df.iloc[11,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['NUM FAILS ITER_BILINEAR']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "\n",
    "    # disj_lb\n",
    "    obj_fails_df.iloc[12,curr_size_ind] =\\\n",
    "        (100. * (inst_w_fails_df['NUM FAILS DISJ_LB']) / inst_w_fails_df[col_num_fails]).mean()\n",
    "    \n",
    "    # num obj / cut\n",
    "    inst_w_cuts_df = curr_by_depth_df[curr_by_depth_df[col_num_vpc] > 0]\n",
    "    obj_fails_df.iloc[13,curr_size_ind] = (inst_w_cuts_df[col_num_obj] / inst_w_cuts_df[col_num_vpc]).mean()\n",
    "\n",
    "    # (s) / obj\n",
    "    obj_fails_df.iloc[14,curr_size_ind] = (inst_w_obj_df[col_vpc_gen_time] / inst_w_obj_df[col_num_obj]).mean()\n",
    "\n",
    "    # (s) / cut\n",
    "    obj_fails_df.iloc[15,curr_size_ind] = (inst_w_cuts_df[col_vpc_gen_time] / inst_w_cuts_df[col_num_vpc]).mean()\n",
    "\n",
    "obj_fails_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `active_cuts_df`: when generated cuts are active, by objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_set = best_gap_df.index\n",
    "# inst_set = ['10teams_presolved', 'bm23_presolved', 'vpm1_presolved']\n",
    "\n",
    "# Define rows to add\n",
    "inst_depth_set = [(inst, best_gap_df.loc[inst, 'BEST VPC DISJ']) for inst in inst_set]\n",
    "\n",
    "rows = [\n",
    "    '\\% active GMIC',\n",
    "    '\\% active VPC',\n",
    "    '\\% cuts one-sided',\n",
    "    '\\% active one-sided',\n",
    "    '\\% cuts all ones',\n",
    "    '\\% active all ones',\n",
    "    '\\% cuts post-GMIC opt',\n",
    "    '\\% active post-GMIC opt',\n",
    "    '\\% cuts DB',\n",
    "    '\\% active DB',\n",
    "]\n",
    "\n",
    "columns = ['V+G ({:d})'.format(size) for size in sizes]\n",
    "\n",
    "active_cuts_df = pd.DataFrame(\n",
    "    columns = columns,\n",
    "    index = rows,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Choose columns to pull\n",
    "active_gmic_col = 'ACTIVE GMIC (all cuts)'\n",
    "active_vpc_col = 'ACTIVE VPC (all cuts)'\n",
    "selected_cols = [\n",
    "    col_num_gmic,\n",
    "    col_num_vpc,\n",
    "    active_gmic_col,\n",
    "    active_vpc_col,\n",
    "] + [col for col in df.columns if \"NUM CUTS \" in col] + [col for col in df.columns if \"NUM ACTIVE\" in col]\n",
    "\n",
    "curr_df = df.loc[inst_set,selected_cols]\n",
    "\n",
    "num_inst_with_one_sided_cuts = [0 for size in sizes]\n",
    "for curr_size_ind in range(0,len(sizes)):\n",
    "    # Select only this depth\n",
    "    # curr_by_depth_df = curr_df[curr_df.index.get_level_values(1) == sizes[curr_size_ind]]\n",
    "    curr_by_depth_df = curr_df.xs(sizes[curr_size_ind], level='disj_terms')\n",
    "\n",
    "    # active gmic\n",
    "    active_cuts_df.iloc[0,curr_size_ind] =\\\n",
    "        (100. * curr_by_depth_df[active_gmic_col] / curr_by_depth_df[col_num_gmic]).mean()\n",
    "\n",
    "    # active vpc\n",
    "    active_cuts_df.iloc[1,curr_size_ind] =\\\n",
    "        (100. * curr_by_depth_df[active_vpc_col] / curr_by_depth_df[col_num_vpc]).mean()\n",
    "\n",
    "    # percent of active cuts among those generated by a specific objective type\n",
    "    obj_types = ['ONE_SIDED', 'ALL_ONES', 'ITER_BILINEAR', 'DISJ_LB']\n",
    "    curr_row_index = 2\n",
    "    inst_w_vpc = curr_by_depth_df[curr_by_depth_df['NUM VPC'] > 0]\n",
    "    for obj in obj_types:\n",
    "        active_cuts_df.iloc[curr_row_index,curr_size_ind] =\\\n",
    "            (100. * inst_w_vpc['NUM CUTS '+obj] / inst_w_vpc[col_num_vpc]).mean()\n",
    "        curr_row_index += 1\n",
    "\n",
    "        inst_w_cuts = inst_w_vpc[inst_w_vpc['NUM CUTS '+obj] > 0]\n",
    "        \n",
    "        active_cuts_df.iloc[curr_row_index,curr_size_ind] =\\\n",
    "            (100. * inst_w_cuts['NUM ACTIVE '+obj] / inst_w_cuts['NUM CUTS '+obj]).mean()\n",
    "        curr_row_index += 1\n",
    "    \n",
    "    # num one-sided cuts\n",
    "    num_inst_with_one_sided_cuts[curr_size_ind] = sum(curr_by_depth_df['NUM CUTS ONE_SIDED'] > 0)\n",
    "\n",
    "display(active_cuts_df)\n",
    "\n",
    "print(\"Num inst with one-sided cuts (should be same across partial trees) =\",num_inst_with_one_sided_cuts)\n",
    "print(\"Total num one-sided cuts =\", sum(curr_by_depth_df['NUM CUTS ONE_SIDED']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Export tables to LaTeX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 1: gap closed and num wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 1: gap closed and num wins\n",
    "\n",
    "# Create copy of table then remove values we do not want (wins for 'G)\n",
    "# TABLE1 = avg_df.copy(deep=True)[[inst_col_name, 'G', 'DB', 'V', 'V+G', 'GurF', 'V+GurF', 'GurL', 'V+GurL']]\n",
    "TABLE1 = avg_gap_df.copy(deep=True)[[inst_col_name]+gap_cols_short]\n",
    "\n",
    "TABLE1['G'].loc[:,wins_row_name] = \"\"\n",
    "\n",
    "# Process the column with # inst to only report number of instances for each set\n",
    "TABLE1[inst_col_name].loc[:,wins_row_name] = \"\"\n",
    "val = TABLE1[inst_col_name].loc[all_set_name,avg_row_name]\n",
    "TABLE1[inst_col_name].loc[all_set_name,avg_row_name] = \\\n",
    "    create_multirow_string(str(val), extra_format=r\"\\tablenum[table-format=3]\")\n",
    "val = TABLE1[inst_col_name].loc[good_vpc_set_name,avg_row_name]\n",
    "TABLE1[inst_col_name].loc[good_vpc_set_name,avg_row_name] = \\\n",
    "    create_multirow_string(str(val), extra_format=r\"\\tablenum[table-format=3]\")\n",
    "val = TABLE1[inst_col_name].loc[binary_set_name,avg_row_name]\n",
    "TABLE1[inst_col_name].loc[binary_set_name,avg_row_name] = \\\n",
    "    create_multirow_string(str(val), extra_format=r\"\\tablenum[table-format=3]\")\n",
    "\n",
    "# Reset index to appear as cols\n",
    "TABLE1.reset_index(inplace=True)\n",
    "\n",
    "# Place column with # inst as second column\n",
    "inst_col = TABLE1[inst_col_name]\n",
    "TABLE1.drop(columns=[inst_col_name], inplace=True)\n",
    "TABLE1.insert(loc=1, column=inst_col_name, value=inst_col)\n",
    "\n",
    "# Set column should have multirow\n",
    "setseries = TABLE1['Set']\n",
    "format_col_as_multirow(setseries)\n",
    "\n",
    "# for i in TABLE1.index:\n",
    "#     curr_name = tex_escape(str(i))\n",
    "#     print(\"Changing {} to {}\".format(i, curr_name))\n",
    "#     TABLE1.rename({i: curr_name}, inplace=True)\n",
    "# print(\"\")\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE1.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE1.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "# styler.format({\n",
    "#     (\"Numeric\", \"Integers\"): '\\${}',\n",
    "#     (\"Numeric\", \"Floats\"): '{:.3f}',\n",
    "#     (\"Non-Numeric\", \"Strings\"): str.upper\n",
    "# })\n",
    "# styler.format_index(escape=\"latex\", axis=0).format_index(escape=\"latex\", axis=1)\n",
    "# styler.hide(level=0,axis=0)\n",
    "table1_str = TABLE1.style.\\\n",
    "    hide(axis=0).\\\n",
    "    format(formatter = int_format).\\\n",
    "    to_latex(\n",
    "        #@{}l@{\\hskip 5pt}\n",
    "        column_format=\"\"\"\n",
    "        @{}l@{}\n",
    "        S[table-format=2.0,table-auto-round,table-number-alignment=center]\n",
    "        l\n",
    "        *{1}{S[table-auto-round]}\n",
    "        H\n",
    "        *{8}{S[table-auto-round]}\n",
    "        @{}\"\"\",\n",
    "        hrules = True,\n",
    "        #clines = \"skip-last;data\",\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        # float_format=\"%.2f\", \n",
    "        # escape=False, \n",
    "        siunitx=True,\n",
    "        # index_names=False,\n",
    "        #columns=['\\# inst', 'G', 'R', 'DB', 'V', 'max(G,V)', 'V+G', 'GurF', 'V+GurF', 'GurL', 'V+GurL']\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"tab:gap-closed-summary\",\n",
    "        caption = \"\"\"\n",
    "            Summary statistics for percent gap closed by VPCs.\n",
    "            The wins row reports how many instances close at least $\\epsilon$ more gap when comparing DB, V, V+G to G on its own, V+GurF to GurF, and V+GurL to GurL.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Add a midrule between the two sets; the \"3\" is hand-coded but can be automated\n",
    "table1_str = add_midrule(table1_str, -3)\n",
    "table1_str = add_midrule(table1_str, -6)\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "table1_str = add_adjustbox_environment(table1_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table1_str = add_sisetup(table1_str)\n",
    "\n",
    "print(table1_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 2: depth x gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 2: percent gap closed by depth\n",
    "TABLE2 = gap_by_size_df.copy(deep=True)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE2.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE2.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "table2_str = TABLE2.style.\\\n",
    "    format(formatter = int_format).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "        @{}l\n",
    "        *{6}{S[table-auto-round]}\n",
    "        @{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"tab:depth\",\n",
    "        caption = \"\"\"\n",
    "            Average percent gap closed broken down by the number of leaf nodes used to construct the partial branch-and-bound tree,\n",
    "            for VPCs with and without GMICs, as well as at the root by \\Gurobi{} after the first and last round of cuts. \n",
    "            ``Best'' refers to the maximum gap closed across all partial tree sizes.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table2_str = add_adjustbox_environment(table2_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table2_str = add_sisetup(table2_str)\n",
    "\n",
    "# Add midrule after 2nd row and before last two rows\n",
    "toprule_row_ind = table2_str.count(\"\\n\", 0, table2_str.index(\"\\\\toprule\")) + 1\n",
    "bottomrule_row_ind = table2_str.count(\"\\n\", 0, table2_str.index(\"\\\\bottomrule\")) + 1\n",
    "table2_str = add_midrule(table2_str, toprule_row_ind + 3)\n",
    "table2_str = add_midrule(table2_str, bottomrule_row_ind + (-2))\n",
    "\n",
    "print(table2_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 3: summary of b&b results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full avg_bb_df for reference\n",
    "\n",
    "# Set the maximum number of rows to be displayed\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display only up to two significant digits\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Display the DataFrame\n",
    "display(avg_bb_df)\n",
    "\n",
    "# Reset the maximum number of rows to be displayed\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 3: summary of b&b results\n",
    "TABLE3 = avg_bb_df.copy(deep=True)\n",
    "\n",
    "# Retrieve classes, buckets, and metrics\n",
    "final_bb_classes = avg_bb_df.index.get_level_values(0).unique()\n",
    "final_bb_buckets = avg_bb_df.index.get_level_values(1).unique()\n",
    "final_bb_metrics = avg_bb_df.index.get_level_values(2).unique()\n",
    "\n",
    "# display(final_bb_classes)\n",
    "# display(final_bb_buckets)\n",
    "# display(final_bb_metrics)\n",
    "\n",
    "# Indices of wins metrics\n",
    "wins_metric_ind = [ ind for ind, metric in enumerate(final_bb_metrics) if metric.startswith('Wins') ]\n",
    "\n",
    "# Change dtype of inst_col_name to string\n",
    "TABLE3.loc[:, inst_col_name] = TABLE3.loc[:, inst_col_name].astype(str)\n",
    "\n",
    "# Process the column with # inst to only report number of instances for each set\n",
    "TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]), inst_col_name] = \"\"\n",
    "\n",
    "for curr_class in final_bb_classes:\n",
    "    for curr_bucket in final_bb_buckets:\n",
    "        curr_name = (curr_class, curr_bucket, final_bb_metrics[0])\n",
    "        val = TABLE3.loc[curr_name, inst_col_name]\n",
    "        TABLE3.loc[curr_name, inst_col_name] = \\\n",
    "            create_multirow_string(str(val), num_rows = len(final_bb_metrics), extra_format=r\"\\tablenum[table-format=3]\")\n",
    "\n",
    "# Set num wins in int format or enclose in braces (center)\n",
    "# tmp_df = TABLE3.loc[(slice(None), slice(None), bb_metrics[1:3]),time_col_header].applymap(int_format, num_digits=6)\n",
    "tmp_df = TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]),time_col_header].map(int_format, num_digits=4, add_phantom=True)\n",
    "# tmp_df = TABLE3.loc[(slice(None), slice(None), bb_metrics[1:3]),time_col_header].applymap(enclose_in_braces)\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[time_col_header],tmp_df.columns])\n",
    "TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]),time_col_header] = TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]),time_col_header].astype(str)\n",
    "TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]),time_col_header] = tmp_df\n",
    "\n",
    "# tmp_df = TABLE3.loc[(slice(None), slice(None), bb_metrics[1:3]),node_col_header].applymap(int_format, num_digits=6)\n",
    "tmp_df = TABLE3.loc[(slice(None), slice(None), bb_metrics[1:3]),node_col_header].map(int_format, num_digits=6, add_phantom=False)\n",
    "# tmp_df = TABLE3.loc[(slice(None), slice(None), bb_metrics[1:3]),node_col_header].applymap(enclose_in_braces)\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[node_col_header],tmp_df.columns])\n",
    "TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]),node_col_header] = tmp_df\n",
    "\n",
    "# Remove unnecessary entries, which are the wins entries for the last time col\n",
    "TABLE3.loc[(slice(None), slice(None), final_bb_metrics[wins_metric_ind]),([time_col_header,node_col_header],map_cols_to_short_time[col_vpc_gen_time])] = \"\"\n",
    "\n",
    "# Reset index to appear as cols\n",
    "TABLE3.reset_index(inplace=True)\n",
    "\n",
    "# Add new col combining class and bucket in one\n",
    "class_bucket_col = \"\\multirow{\" + str(len(final_bb_metrics)) + \"}{*}{\\shortstack[l]{\" + TABLE3['class'] + \"\\\\\\\\\\\\relax \" + TABLE3['bucket'] + \"}}\"\n",
    "for i in range(len(class_bucket_col)):\n",
    "    if i%len(final_bb_metrics)!=0:\n",
    "        class_bucket_col[i] = \"\"\n",
    "TABLE3.drop(columns = ['class', 'bucket'], inplace = True, level = 0)\n",
    "TABLE3.insert(loc=0, column=\"Set\", value=class_bucket_col)\n",
    "\n",
    "# Place column with # inst as second column\n",
    "inst_col = TABLE3[inst_col_name]\n",
    "TABLE3.drop(columns=[inst_col_name], inplace=True, level=0)\n",
    "TABLE3.insert(loc=1, column=inst_col_name, value=inst_col)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE3.columns:\n",
    "    if isinstance(col, tuple):\n",
    "        for lvl_ind, lvl_col in enumerate(col):\n",
    "            curr_col = tex_escape(str(lvl_col))\n",
    "            TABLE3.rename({lvl_col: curr_col}, inplace=True, axis=1, level=lvl_ind)\n",
    "    else:\n",
    "        # curr_col = '{' + tex_escape(col) + '}'\n",
    "        curr_col = tex_escape(str(col))\n",
    "        TABLE3.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format(formatter = int_format).\\\n",
    "table3_str = TABLE3.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "        @{}l    % set\n",
    "        c       % inst\n",
    "        l       % stat\n",
    "        *{2}{S[table-auto-round,table-format=4.2]}\n",
    "        *{2}{H}\n",
    "        *{2}{S[table-auto-round,table-format=4.2]}        \n",
    "        *{2}{S[table-auto-round,table-format=6.0]}\n",
    "        *{1}{H}\n",
    "        *{1}{S[table-auto-round,table-format=6.0]}\n",
    "        @{}}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"tab:bb-summary\",\n",
    "        caption = \"\"\"\n",
    "            Summary statistics for time to solve instances with branch-and-bound.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Add a midrule between the two sets; the \"9\" is hand-coded but can be automated\n",
    "for ind in [-57,-49,-41,-33,-25,-17,-9]:\n",
    "    table3_str = add_midrule(table3_str, ind)\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "table3_str = add_adjustbox_environment(table3_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table3_str = add_sisetup(table3_str, table_format=\"4.2\")\n",
    "\n",
    "print(table3_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 4: number of leaf nodes yielding the best result for each experiment per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_disj_gap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_disj_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_disj_nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 4: frequency of when each size is best\n",
    "TABLE4 = pd.concat([best_disj_gap_df,best_disj_time_df,best_disj_nodes_df],axis=1)\n",
    "# TABLE4 = best_disj_gap_df.copy(deep=True)\n",
    "# TABLE4.drop('Best', axis=0, inplace=True)\n",
    "\n",
    "# Create new column index\n",
    "TABLE4.columns = pd.MultiIndex.from_tuples(\n",
    "    [('Gap',col) for col in best_disj_gap_df.columns]\n",
    "    + [('Time',col) for col in best_disj_time_df.columns]\n",
    "    + [('Nodes',col) for col in best_disj_nodes_df.columns]\n",
    ")\n",
    "\n",
    "# From Time and Nodes, Drop all but 'All' column\n",
    "TABLE4.drop(columns=[col for col in TABLE4.columns if col[0] == 'Time' and col[1] != 'All'], inplace=True)\n",
    "TABLE4.drop(columns=[col for col in TABLE4.columns if col[0] == 'Nodes' and col[1] != 'All'], inplace=True)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE4.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE4.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # hide(axis=0).\\\n",
    "table4_str = TABLE4.style.\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"@{}l*4{S}*1{S}@{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:size\",\n",
    "        caption = \"\"\"\n",
    "            Number of leaf nodes yielding the best result for each experiment per instance.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table9_str = add_adjustbox_environment(table9_str)\n",
    "\n",
    "# table9_str = add_midrule(table9_str, -2)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table4_str = add_sisetup(table4_str, table_format=\"3.0\")\n",
    "\n",
    "print(table4_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute summary metrics: shifted geometric mean and wins for ref_time_col, refv_time_col, ref_nodes_col, and refv_nodes_col\n",
    "# curr_df = df.loc[selected_time_instances, [col_num_vpc] + cols_time_by_depth + cols_nodes_by_depth]\n",
    "# curr_df = curr_df[curr_df.index.get_level_values(1) > 0]\n",
    "\n",
    "# # Calculate minimum value for each column, per instance (index level 0)\n",
    "# curr_argmin_df = curr_df.groupby(level=0).idxmin()\n",
    "# best_df = curr_df.loc[curr_argmin_df[refv_time_col]]\n",
    "\n",
    "# # Sort by increasing value of cols_times_by_depth[1\n",
    "# print(\"Sorting by increasing value of\",cols_time_by_depth[1],\"...\")\n",
    "# best_df = best_df.sort_values(by=cols_time_by_depth[1])\n",
    "\n",
    "# best_df.head()\n",
    "\n",
    "# # Move 'disj_terms' (second level of index) to column\n",
    "# best_df.reset_index(level=1, inplace=True)\n",
    "# best_df.head()\n",
    "\n",
    "# # Set up best_df_summary_metrics_df to store the summary metrics\n",
    "# best_df_summary_metrics_df = best_df.copy(deep=True)\n",
    "\n",
    "# # Compute shifted geometric mean\n",
    "# cols_for_shifted_time_gmean = [col for col in best_df.columns if 'TIME' in col]\n",
    "# cols_for_shifted_nodes_gmean = [col for col in best_df.columns if 'NODES' in col]\n",
    "\n",
    "# # Apply shift to each column\n",
    "# for col in cols_for_shifted_time_gmean:\n",
    "#     best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] + SHIFT_TIME\n",
    "# for col in cols_for_shifted_nodes_gmean:\n",
    "#     best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] + SHIFT_NODES\n",
    "\n",
    "# # Compute shifted geometric mean for time\n",
    "# best_df_summary_metrics_df.loc['Gmean'] = best_df_summary_metrics_df[cols_for_shifted_time_gmean+cols_for_shifted_nodes_gmean].apply(geometric_mean, axis=0)\n",
    "\n",
    "# # Change shift back\n",
    "# for col in cols_for_shifted_time_gmean:\n",
    "#     best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] - SHIFT_TIME\n",
    "# for col in cols_for_shifted_nodes_gmean:\n",
    "#     best_df_summary_metrics_df[col] = best_df_summary_metrics_df[col] - SHIFT_NODES\n",
    "# best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_time_gmean] = best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_time_gmean] - SHIFT_TIME\n",
    "# best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_nodes_gmean] = best_df_summary_metrics_df.loc['Gmean', cols_for_shifted_nodes_gmean] - SHIFT_NODES\n",
    "\n",
    "# # Compute wins for ref_time_col, refv_time_col, ref_nodes_col, and refv_nodes_col\n",
    "# best_df_summary_metrics_df.loc['Wins', cols_for_shifted_time_gmean] = \\\n",
    "#     [ \n",
    "#       int(sum( best_df_summary_metrics_df[refv_time_col] > WIN_BY_TIME_FACTOR * best_df_summary_metrics_df[ref_time_col] )),\n",
    "#       int(sum( best_df_summary_metrics_df[ref_time_col] > WIN_BY_TIME_FACTOR * best_df_summary_metrics_df[refv_time_col] )),\n",
    "#       int(sum( best_df_summary_metrics_df[ref_time_col] > WIN_BY_TIME_FACTOR * (best_df_summary_metrics_df[refv_time_col] + best_df_summary_metrics_df[col_vpc_gen_time]) )),\n",
    "#     ]\n",
    "\n",
    "# best_df_summary_metrics_df.loc['Wins', cols_for_shifted_nodes_gmean] = \\\n",
    "#     [ \n",
    "#       int(sum(best_df_summary_metrics_df[refv_nodes_col] > best_df_summary_metrics_df[ref_nodes_col])),\n",
    "#       int(sum(best_df_summary_metrics_df[refcol] > best_df_summary_metrics_df[refv_nodes_col]))\n",
    "#     ]\n",
    "\n",
    "# best_df_summary_metrics_df.loc[['Gmean','Wins']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 5: density statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 10: density statistics\n",
    "TABLE5 = density_df.copy(deep=True)\n",
    "\n",
    "# Make # inst and wins row int\n",
    "#tmp_df = TABLE10.iloc[0:2].applymap(int_format, num_digits=3, add_phantom=False)\n",
    "tmp_df = TABLE5.iloc[0:2].map(int_format, num_digits=3, add_phantom=False)\n",
    "\n",
    "TABLE5.iloc[0:2] = tmp_df\n",
    "# tmp_df = TABLE10.loc['\\# wins by time',:].apply(int_format, num_digits=2, add_phantom=False)\n",
    "# TABLE10.loc['\\# wins by time'] = tmp_df\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # hide(axis=0).\\\n",
    "table5_str = TABLE5.style.\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"@{}l*{6}{S[table-format=0.3,table-auto-round,table-number-alignment=center]}@{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:density\",\n",
    "        caption = \"\"\"\n",
    "            Statistics about the density of generated cuts broken down by partial tree size.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table5_str = add_adjustbox_environment(table5_str)\n",
    "\n",
    "# table5_str = add_midrule(table5_str, -2)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table5_str = add_sisetup(table5_str, table_format=\"0.3\")\n",
    "\n",
    "print(table5_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 6: failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 11: failures\n",
    "TABLE6 = obj_fails_df.copy(deep=True)\n",
    "\n",
    "# Make num inst rows int\n",
    "tmp_df = TABLE6.iloc[0:5].map(int_format, num_digits=3, add_phantom=False)\n",
    "TABLE6.iloc[0:5] = tmp_df\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # hide(axis=0).\\\n",
    "table6_str = TABLE6.style.\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"@{}l*{6}{S[table-format=2.2,table-auto-round,table-number-alignment=center]}@{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:objectives\",\n",
    "        caption = \"\"\"\n",
    "            Statistics about the objectives leading to failures, broken down by partial tree size used for cut generation.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table11_str = add_adjustbox_environment(table11_str)\n",
    "\n",
    "table6_str = add_midrule(table6_str, -4)\n",
    "table6_str = add_midrule(table6_str, -8)\n",
    "table6_str = add_midrule(table6_str, -13)\n",
    "table6_str = add_midrule(table6_str, -15)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table6_str = add_sisetup(table6_str, table_format=\"2.2\")\n",
    "\n",
    "print(table6_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 7: active cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 12: active cuts\n",
    "TABLE7 = active_cuts_df.copy(deep=True)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # hide(axis=0).\\\n",
    "table7_str = TABLE7.style.\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"@{}l*{6}{S[table-format=3.2,table-auto-round,table-number-alignment=center]}@{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:activity\",\n",
    "        caption = \"\"\"\n",
    "            Statistics about when generated cuts are active, broken down by partial tree size.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table7_str = add_adjustbox_environment(table7_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table7_str = add_sisetup(table7_str, table_format=\"2.2\")\n",
    "\n",
    "print(table7_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 8: objective + time analysis per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 8: obj and time analysis\n",
    "TABLE8 = obj_and_time_df.copy(deep=True)\n",
    "\n",
    "# Move instance names into a column\n",
    "TABLE8.reset_index(inplace=True)\n",
    "TABLE8.drop('disj_terms',axis=1,inplace=True)\n",
    "\n",
    "# Create new column index\n",
    "TABLE8.columns = pd.MultiIndex.from_tuples(\n",
    "    [('','Instance'),\n",
    "    ('Objectives','Obj'),\n",
    "    ('Objectives','Succ'),\n",
    "    ('Objectives','Fails'),\n",
    "    ('Objectives','\\% fails'),\n",
    "    ('Time (s)','Total'),\n",
    "    ('Time (s)','(s) / obj'),\n",
    "    ('Time (s)','(s) / cut')]\n",
    ")\n",
    "\n",
    "# Format instance column correctly\n",
    "TABLE8[('',\"Instance\")] = TABLE8[('',\"Instance\")].apply(remove_presolved_from_name)\n",
    "TABLE8[('',\"Instance\")] = TABLE8[('',\"Instance\")].apply(tex_escape)\n",
    "\n",
    "# Format SKIP_CHAR correctly\n",
    "for col in TABLE8.columns:\n",
    "    TABLE8[col] = TABLE8[col].apply(enclose_in_braces, val_to_match=SKIP_CHAR)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE8.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE8.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "table8_str = TABLE8.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "        @{}\n",
    "        l\n",
    "        *{3}{S[table-format=3.0,table-auto-round,table-number-alignment=center]}\n",
    "        *{1}{S[table-format=2.1,table-auto-round,table-number-alignment=center]}\n",
    "        *{1}{S[table-format=4.1,table-auto-round,table-number-alignment=center]}\n",
    "        *{2}{S[table-format=4.1,table-auto-round,table-number-alignment=center]}\n",
    "        @{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:obj-and-time-best\",\n",
    "        caption = \"\"\"\n",
    "            Information about objectives and time to generate cuts corresponding to the results in Table~\\ref{app:tab:gap-closed}.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table8_str = add_adjustbox_environment(table8_str)\n",
    "\n",
    "table8_str = add_midrule(table8_str, -2)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table8_str = add_sisetup(table8_str)\n",
    "\n",
    "print(table8_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Table 9: rejected instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rejection_reason.to_csv('rejection_reason.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbose version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *Verbose version*: For each instance that was not selected, print the reason\n",
    "df_rejection_reason_rejected = df_rejection_reason[df_rejection_reason['SELECTED_GAP'] == False]\n",
    "rejected_instance_list = df_rejection_reason_rejected.index\n",
    "rejected_instance_list.name = 'Instance'\n",
    "cols = ['Set', 'Reason']\n",
    "df_rejected_instances = pd.DataFrame(columns=cols, index=rejected_instance_list)\n",
    "df_rejected_instances['Set'] = df_ipopt.loc[rejected_instance_list,'SET']\n",
    "df_rejected_instances.loc[df_rejection_reason_rejected['OPTIMAL_SOLUTION_FOUND'] > 0, \n",
    "                            'Reason'] = \"Integer-optimal solution found constructing partial tree\"\n",
    "df_rejected_instances.loc[(df_rejection_reason_rejected['OPTIMAL_SOLUTION_FOUND'] == 0) \n",
    "                                & (df_rejection_reason_rejected['LP=DLB=DUB'] == 6), \n",
    "                            'Reason'] = \"Max leaf value = LP value\"\n",
    "df_rejected_instances.loc[(df_rejection_reason_rejected['OPTIMAL_SOLUTION_FOUND'] == 0) \n",
    "                                & (df_rejection_reason_rejected['LP=DLB=DUB'] < 6) \n",
    "                                & (df_rejection_reason_rejected['LP=DLB=DUB'] + df_rejection_reason_rejected['PRLP_INFEASIBLE'] == 6), \n",
    "                            'Reason'] = \"Max leaf value = LP value or PRLP primal infeasible\"\n",
    "df_rejected_instances.loc[(df_rejection_reason_rejected['OPTIMAL_SOLUTION_FOUND'] == 0)\n",
    "                                & (df_rejection_reason_rejected['LP=DLB=DUB'] < 6) \n",
    "                                & (df_rejection_reason_rejected['LP=DLB=DUB'] + df_rejection_reason_rejected['PRLP_INFEASIBLE'] < 6)\n",
    "                                & (df_rejection_reason_rejected['LP=DLB=DUB'] + df_rejection_reason_rejected['PRLP_INFEASIBLE'] + df_rejection_reason_rejected['PRLP_TIME_LIMIT'] == 6), \n",
    "                            'Reason'] = \"Max leaf value = LP value or PRLP primal infeasible / hits time limit\"\n",
    "df_rejected_instances.loc[df_rejection_reason_rejected['<7_ATTEMPTS'] > 0, \n",
    "                            'Reason'] = \"Numerical issues\"\n",
    "display(df_rejected_instances.head())\n",
    "col_format = \"\"\"@{}*{2}{l}X@{}\"\"\"\n",
    "\n",
    "tmp_df_remaining_rejected_instances = df_rejection_reason.loc[df_rejected_instances[df_rejected_instances['Reason'].isna()].index]\n",
    "if len(tmp_df_remaining_rejected_instances) > 0:\n",
    "    display(tmp_df_remaining_rejected_instances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Succinct version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *Succinct version*: For each instance that was not selected, print the reason\n",
    "df_rejected_instances = df_status_by_depth.loc[df_rejection_reason[df_rejection_reason['SELECTED_GAP'] == False].index]\n",
    "df_rejected_instances.insert(loc = 0, column = 'Set', value = df_ipopt.loc[rejected_instance_list,'SET'])\n",
    "col_format=\"\"\"@{}*{2}{l}*{6}{c}@{}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Table 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 4: rejected instances reasons\n",
    "TABLE9 = df_rejected_instances.copy(deep=True)\n",
    "TABLE9.reset_index(inplace=True)\n",
    "\n",
    "TABLE9[\"Instance\"] = TABLE9[\"Instance\"].apply(remove_presolved_from_name)\n",
    "TABLE9[\"Instance\"] = TABLE9[\"Instance\"].apply(tex_escape)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE9.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE9.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format_index(escape=\"latex\", axis=0).\\\n",
    "table9_str = TABLE9.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=col_format,\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=False,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:discarded-instances\",\n",
    "        caption = \"\"\"\n",
    "            Instances that were not considered with reason for being discarded.\n",
    "        \"\"\",\n",
    "        )\n",
    "        \n",
    "print(table9_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TABLE9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG: Test Table 9 code and make sure \"set\" is properly identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEBUG\n",
    "# df_rejection_reason[df_rejection_reason['NUM_WITH_OBJS'] != df_rejection_reason['NUM_WITH_CUTS']]\n",
    "# df_rejection_reason[(df_rejection_reason['NUM_WITH_CUTS'] > 0) & (df_rejection_reason['DLB=DUB'] > 0) & (df_rejection_reason['OPTIMAL_SOLUTION_FOUND'] == 0)]\n",
    "# df_rejection_reason[(df_rejection_reason['LP=DLB=DUB'] == 6)]\n",
    "\n",
    "# inst = 'chromaticindex32-8_presolved'\n",
    "# # df_rejection_reason.loc[inst]\n",
    "# tmp = df_bb.loc[(inst,64)]\n",
    "# tmp[25:50]\n",
    "\n",
    "# len(df_rejection_reason[df_rejection_reason['SELECTED'] == True])\n",
    "# inst = 'berlin_5_8_0_presolved'\n",
    "# gap_df.loc[inst]\n",
    "#df_rejection_reason.loc['bnatt400_presolved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEBUG: Verify \"Set\" col is correct\n",
    "for inst in rejected_instance_list:\n",
    "    curr_set = df_ipopt.loc[inst,'SET']\n",
    "    has_error = False\n",
    "    if isinstance(curr_set, pd.Series):\n",
    "        # check that all sets are same, then just take first\n",
    "        first_set = curr_set[0]\n",
    "        for tmp_set in curr_set:\n",
    "            if tmp_set != first_set:\n",
    "                print(\"*** ERROR: not all sets are equal ({} != {})\".format(first_set, tmp_set))\n",
    "                has_error = True\n",
    "                break\n",
    "        curr_set = first_set\n",
    "    ref_set = df_rejected_instances.loc[inst, 'Set']\n",
    "    if ref_set != curr_set:\n",
    "        print(\"*** ERROR: for inst {}, df_rej_inst set {} != df_ipopt set {}\".format(inst, ref_set, curr_set))\n",
    "        has_error = True\n",
    "    \n",
    "    if has_error:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 10: app:tab:gap-closed: full gap closed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all but column named 'R' of all_gap_results_df\n",
    "subset_all_gap_results_df = all_gap_results_df.drop(columns=('% gap closed','R'), inplace=False)\n",
    "subset_all_gap_results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 10: full gap closed results\n",
    "TABLE10 = subset_all_gap_results_df.copy(deep=True)\n",
    "\n",
    "# Set wins row to be integer valued\n",
    "TABLE10.loc['Wins'] = TABLE10.loc['Wins'].apply(int_format)\n",
    "# TABLE5.iloc[len(TABLE5)-1] = TABLE5.iloc[len(TABLE5)-1].apply(int_format)\n",
    "\n",
    "# Move instance names into a column\n",
    "TABLE10.reset_index(inplace=True, col_level=1)\n",
    "\n",
    "TABLE10[('',\"Instance\")] = TABLE10[('',\"Instance\")].apply(remove_presolved_from_name)\n",
    "TABLE10[('',\"Instance\")] = TABLE10[('',\"Instance\")].apply(tex_escape)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE10.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE10.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format(formatter = int_format).\\\n",
    "table10_str = TABLE10.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "\t@{}l*{2}{S[table-format=4.0,table-auto-round,table-number-alignment=center]}\n",
    "\t*{2}{S[table-format=4.0,table-auto-round,table-number-alignment=center]}\n",
    "\t*{8}{S[table-auto-round]}\n",
    "\t@{}\n",
    "        \"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:gap-closed\",\n",
    "        caption = \"\"\"\n",
    "            Percent gap closed by instance for GMICs (G), VPCs (V), both VPCs and GMICs used together, \n",
    "            and the bound implied by the partial branch-and-bound tree with 64 leaf nodes (DB).\n",
    "            Also shown are the sizes of the instances, the number of cuts added, and the percent gap closed by \n",
    "            \\Gurobi{} at the root (after one round (GurF) and after the last round (GurL)). \n",
    "            Entries in which DB appears to be 0.00 are actually small strictly positive numbers.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table10_str = add_adjustbox_environment(table10_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table10_str = add_sisetup(table10_str)\n",
    "\n",
    "# Add a midrule between the instances and 3 summary rows; the \"5\" is hand-coded but can be automated\n",
    "table10_str = add_midrule(table10_str, -5)\n",
    "\n",
    "print(table10_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 11: \"all\" time/nodes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Table 11: \"all\" time/nodes results\n",
    "TABLE11 = best_df_summary_metrics_df.copy(deep=True)\n",
    "\n",
    "# Remove nan values from 'Wins' row\n",
    "rows_to_remove_nan = ['Gmean','Wins']\n",
    "TABLE11.loc[rows_to_remove_nan] = TABLE11.loc[rows_to_remove_nan].fillna(\"\")\n",
    "\n",
    "# Set wins row to be integer valued\n",
    "TABLE11.loc['Wins'] = TABLE11.loc['Wins'].apply(int_format)\n",
    "\n",
    "# Move instance names into a column\n",
    "TABLE11.reset_index(inplace=True, col_level=1)\n",
    "TABLE11.rename(columns={'INSTANCE': 'Instance'}, inplace=True)\n",
    "\n",
    "# Remove presolved from name and escape\n",
    "TABLE11[('',\"Instance\")] = TABLE11[('',\"Instance\")].apply(remove_presolved_from_name)\n",
    "TABLE11[('',\"Instance\")] = TABLE11[('',\"Instance\")].apply(tex_escape)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE11.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE11.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format(formatter = int_format).\\\n",
    "table11_str = TABLE11.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "          @{}l % instance\n",
    "          *{1}{S[table-format=4.0,table-auto-round,table-number-alignment=center]} % # terms\n",
    "          *{1}{S[table-format=4.0,table-auto-round,table-number-alignment=center]} % # cuts\n",
    "          *{1}{S[table-format=4.2,table-auto-round]} % Gur\n",
    "          *{2}{S[table-format=4.2,table-auto-round]} % V, Gen\n",
    "          *{2}{S[table-format=8.0,table-auto-round,table-number-alignment=center]} % Nodes\n",
    "          @{}\n",
    "        \"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:bb\",\n",
    "        caption = \"\"\"\n",
    "            Time (in seconds) and number nodes taken to solve each instance,\n",
    "            for the disjunction size with best solving time with VPCs per instance.\n",
    "            The table is sorted by column 4 (``V'' under ``Time (s)'').\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table11_str = add_sisetup(table11_str)\n",
    "\n",
    "# Add a midrule between the instances and 3 summary rows; the \"6\" is hand-coded but can be automated\n",
    "table11_str = add_midrule(table11_str, -5)\n",
    "# table11_str = add_midrule(table11_str, -10)\n",
    "\n",
    "print(table11_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Format Table 6: \"all\" time/nodes results\n",
    "TABLE6 = all_bb_results_df.copy(deep=True)\n",
    "\n",
    "# Rename summary rows to reflect the set\n",
    "rename_metrics_all = {metric : metric + ' (All)' for metric in bb_metrics}\n",
    "TABLE6.rename(rename_metrics_all, inplace=True)\n",
    "\n",
    "# Add summary rows from 6 trees set\n",
    "summary_metrics_6trees = all6_bb_results_df.tail(3).copy(deep=True)\n",
    "rename_metrics_6trees = {metric : metric + ' (6 trees)' for metric in bb_metrics}\n",
    "summary_metrics_6trees.rename(rename_metrics_6trees, inplace=True)\n",
    "\n",
    "TABLE6 = pd.concat([TABLE6, summary_metrics_6trees])\n",
    "\n",
    "# Drop rows, cols, (time,V7)\n",
    "TABLE6.drop([('','Rows'),('','Cols'),(node_col_header,map_cols_to_short_time[mintime_col])], axis=1, inplace=True)\n",
    "\n",
    "# Set wins row to be integer valued\n",
    "TABLE6.loc['Wins1 (All)'] = TABLE6.loc['Wins1 (All)'].apply(int_format)\n",
    "TABLE6.loc['Wins7 (All)'] = TABLE6.loc['Wins7 (All)'].apply(int_format)\n",
    "TABLE6.loc['Wins1 (6 trees)'] = TABLE6.loc['Wins1 (6 trees)'].apply(int_format)\n",
    "TABLE6.loc['Wins7 (6 trees)'] = TABLE6.loc['Wins7 (6 trees)'].apply(int_format)\n",
    "# TABLE6.iloc[len(TABLE6)-1] = TABLE6.iloc[len(TABLE6)-1].apply(int_format)\n",
    "\n",
    "# Move instance names into a column\n",
    "TABLE6.reset_index(inplace=True, col_level=1)\n",
    "\n",
    "# Store indices of rows of 6-tree instances\n",
    "six_trees_instances = list(all6_instances_dict.keys())\n",
    "# mask = TABLE6[('','Instance')].isin(six_trees_instances)\n",
    "# six_trees_indices = TABLE6.loc[mask, :].index.tolist()\n",
    "\n",
    "# Remove presolved from name and escape\n",
    "TABLE6[('',\"Instance\")] = TABLE6[('',\"Instance\")].apply(remove_presolved_from_name)\n",
    "TABLE6[('',\"Instance\")] = TABLE6[('',\"Instance\")].apply(tex_escape)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE6.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE6.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format(formatter = int_format).\\\n",
    "table6_str = TABLE6.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "\t@{}l % instance\n",
    "\t*{1}{S[table-format=4.0,table-auto-round,table-number-alignment=center]} % # cuts\n",
    "\t*{2}{S[table-format=4.2,table-auto-round]} % Gur1, Gur7\n",
    "\t*{2}{H} % V, Total\n",
    "\t*{2}{S[table-format=4.2,table-auto-round]} % V7, Total7\n",
    "\t*{3}{S[table-format=8.0,table-auto-round,table-number-alignment=center]} % Nodes\n",
    "\t@{}\n",
    "        \"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:bb\",\n",
    "        caption = \"\"\"\n",
    "            Time (in seconds) and number nodes taken to solve each instance.\n",
    "            The table is sorted by column 4 (``V'' under ``Time (s)'').\n",
    "            ``Gur1'' indicates \\Gurobi{} run with one random seed.\n",
    "            ``Gur7'' indicates the minimum from seven runs of \\Gurobi{} with different random seeds.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table6_str = add_adjustbox_environment(table6_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table6_str = add_sisetup(table6_str)\n",
    "\n",
    "# Add a midrule between the instances and 3 summary rows; the \"6\" is hand-coded but can be automated\n",
    "table6_str = add_midrule(table6_str, -6)\n",
    "table6_str = add_midrule(table6_str, -10)\n",
    "\n",
    "# Add color to six tree instances\n",
    "splitlines = table6_str.splitlines()\n",
    "for i in range(len(splitlines)):\n",
    "    line = splitlines[i]\n",
    "    curr_line = line.split('&')\n",
    "    if len(curr_line) > 0 and curr_line[0].strip()+'_presolved' in six_trees_instances:\n",
    "        splitlines[i] = '\\\\rowcolor{lightgray!30} ' + line\n",
    "table6_str = '\\n'.join(splitlines).replace('NaN', '')\n",
    "\n",
    "print(table6_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX Format Table X: \"6 trees\" time/nodes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Format Table 7: \"6 trees\" time/nodes results\n",
    "TABLEX = all6_bb_results_df.copy(deep=True)\n",
    "\n",
    "# Set wins row to be integer valued\n",
    "TABLEX.loc['Wins1'] = TABLEX.loc['Wins1'].apply(int_format)\n",
    "TABLEX.loc['Wins7'] = TABLEX.loc['Wins7'].apply(int_format)\n",
    "# TABLEX.iloc[len(TABLEX)-1] = TABLEX.iloc[len(TABLEX)-1].apply(int_format)\n",
    "\n",
    "# Move instance names into a column\n",
    "TABLEX.reset_index(inplace=True, col_level=1)\n",
    "\n",
    "TABLEX[('',\"Instance\")] = TABLEX[('',\"Instance\")].apply(remove_presolved_from_name)\n",
    "TABLEX[('',\"Instance\")] = TABLEX[('',\"Instance\")].apply(tex_escape)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLEX.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLEX.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format(formatter = int_format).\\\n",
    "tableX_str = TABLEX.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"@{}l*{2}{c}*{2}{c}H*{8}{c}@{}\"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:bb-7trees\",\n",
    "        caption = \"\"\"\n",
    "  Time (in seconds) and number nodes taken to solve each of the instances for which all six branch-and-bound trees successfully yielded VPCs.\n",
    "  %The columns with V1x are those in which we do not terminate the VPC computation as soon as the time exceeds \\Gurobi{}'s time.  \n",
    "  The table is sorted by column 4 (``V7'' under ``Time (s)'').\n",
    "  ``Gur1'' indicates Gurobi run with one random seed.\n",
    "  ``Gur7'' indicates the minimum from seven runs of Gurobi with different random seeds.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# tableX_str = add_adjustbox_environment(tableX_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "tableX_str = add_sisetup(tableX_str)\n",
    "\n",
    "# Add a midrule between the instances and 3 summary rows; the \"6\" is hand-coded but can be automated\n",
    "tableX_str = add_midrule(tableX_str, -6)\n",
    "\n",
    "print(tableX_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX Format Table X: (now Table 3) b&b summary by depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Format Table 7: summary of b&b results\n",
    "# TABLE7 = avg_bb_by_depth_df.copy(deep=True)\n",
    "\n",
    "# # Remove unnecessary entries\n",
    "# # TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:]),([time_col_header,node_col_header],map_cols_to_short_time[gur1time_col])] = \"\"\n",
    "# # TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[2]),([time_col_header,node_col_header],map_cols_to_short_time[gur7time_col])] = \"\"\n",
    "\n",
    "# # Process the column with # inst to only report number of instances for each set\n",
    "# TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:]), inst_col_name] = \"\"\n",
    "\n",
    "# for curr_class in bb_classes_by_depth:\n",
    "#     for curr_bucket in bb_buckets_by_depth:\n",
    "#         curr_name = (curr_class, curr_bucket, bb_metrics_by_depth[0])\n",
    "#         val = TABLE7.loc[curr_name, inst_col_name]\n",
    "#         TABLE7.loc[curr_name, inst_col_name] = \\\n",
    "#             create_multirow_string(str(val), num_rows = 2, extra_format=r\"\\tablenum[table-format=3]\")\n",
    "\n",
    "# # Set num wins in int format or enclose in braces (center)\n",
    "# # tmp_df = TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),time_col_header].applymap(int_format, num_digits=6)\n",
    "# tmp_df = TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),time_col_header].applymap(int_format, num_digits=4, add_phantom=True)\n",
    "# # tmp_df = TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),time_col_header].applymap(enclose_in_braces)\n",
    "# tmp_df.columns = pd.MultiIndex.from_product([[time_col_header],tmp_df.columns])\n",
    "# TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),time_col_header] = tmp_df\n",
    "\n",
    "# # tmp_df = TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),node_col_header].applymap(int_format, num_digits=6)\n",
    "# tmp_df = TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),node_col_header].applymap(int_format, num_digits=6, add_phantom=False)\n",
    "# # tmp_df = TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),node_col_header ].applymap(enclose_in_braces)\n",
    "# tmp_df.columns = pd.MultiIndex.from_product([[node_col_header],tmp_df.columns])\n",
    "# TABLE7.loc[(slice(None), slice(None), bb_metrics_by_depth[1:3]),node_col_header] = tmp_df\n",
    "\n",
    "# # Reset index to appear as cols\n",
    "# TABLE7.reset_index(inplace=True)\n",
    "\n",
    "# # Add new col combining class and bucket in one\n",
    "# class_bucket_col = \"\\multirow{2}{*}{\\shortstack[l]{\" + TABLE7['class'] + \"\\\\\\\\\\\\relax \" + TABLE7['bucket'] + \"}}\"\n",
    "# for i in range(len(class_bucket_col)):\n",
    "#     if i%len(bb_metrics_by_depth)!=0:\n",
    "#         class_bucket_col[i] = \"\"\n",
    "# TABLE7.drop(columns = ['class', 'bucket'], inplace = True, level = 0)\n",
    "# TABLE7.insert(loc=0, column=\"Set\", value=class_bucket_col)\n",
    "\n",
    "# # Place column with # inst as second column\n",
    "# inst_col = TABLE7[inst_col_name]\n",
    "# TABLE7.drop(columns=[inst_col_name], inplace=True, level=0)\n",
    "# TABLE7.insert(loc=1, column=inst_col_name, value=inst_col)\n",
    "\n",
    "# # If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "# for col in TABLE7.columns:\n",
    "#     if isinstance(col, tuple):\n",
    "#         for lvl_ind, lvl_col in enumerate(col):\n",
    "#             curr_col = tex_escape(str(lvl_col))\n",
    "#             TABLE7.rename({lvl_col: curr_col}, inplace=True, axis=1, level=lvl_ind)\n",
    "#     else:\n",
    "#         # curr_col = '{' + tex_escape(col) + '}'\n",
    "#         # curr_col = col\n",
    "#         curr_col = tex_escape(str(col))\n",
    "#         TABLE7.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# # Finally, apply the desired style\n",
    "#     # format(formatter = int_format).\\\n",
    "# table7_str = TABLE7.style.\\\n",
    "#     hide(axis=0).\\\n",
    "#     to_latex(\n",
    "#         column_format=\"\"\"\n",
    "#         @{}l    % set\n",
    "#         c       % inst\n",
    "#         l       % stat\n",
    "#         *{3}{S[table-auto-round,table-format=4.2]}\n",
    "#         *{2}{S[table-auto-round,table-format=6.0]}\n",
    "#         @{}\"\"\",\n",
    "#         hrules = True,\n",
    "#         sparse_index = True,\n",
    "#         multirow_align = \"c\",\n",
    "#         siunitx=True,\n",
    "#         convert_css = True,\n",
    "#         environment = \"table\",\n",
    "#         position_float = \"centering\",\n",
    "#         label = \"tab:bb-summary\",\n",
    "#         caption = \"\"\"\n",
    "#             Summary statistics for time to solve instances with branch-and-bound.\n",
    "#         \"\"\",\n",
    "#         )\n",
    "\n",
    "# # Add a midrule between the two sets; the \"9\" is hand-coded but can be automated\n",
    "# table7_str = add_midrule(table7_str, -41)\n",
    "# table7_str = add_midrule(table7_str, -33)\n",
    "# table7_str = add_midrule(table7_str, -25)\n",
    "# table7_str = add_midrule(table7_str, -17)\n",
    "# table7_str = add_midrule(table7_str, -9)\n",
    "\n",
    "# # Adjustbox environment sets width to pagewidth\n",
    "# table7_str = add_adjustbox_environment(table7_str)\n",
    "\n",
    "# # Set default siunitx options for this table\n",
    "# table7_str = add_sisetup(table7_str, table_format=\"4.2\")\n",
    "\n",
    "# print(table7_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Table 13: instances with best bb improvement from VPCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Format Table 13: \"best\" time/nodes results\n",
    "TABLE13 = all_bb_results_df.copy(deep=True)\n",
    "\n",
    "# Drop all rows in which (time_col_header, 'Gur7') is ''\n",
    "TABLE13 = TABLE13[TABLE13[(time_col_header, 'Gur7')] != '']\n",
    "\n",
    "# TABLE13[('','V7-Gur7')]\n",
    "# Change column (time_col_header, V7) to be float valued\n",
    "#TABLE13[(time_col_header, 'V7')] = TABLE13[(time_col_header, 'V7')].apply(float_format, num_digits=4)\n",
    "TABLE13 = TABLE13.astype({(time_col_header, 'V7'): float})\n",
    "TABLE13 = TABLE13.astype({(time_col_header, 'Gur7'): float})\n",
    "\n",
    "# Add new column for difference between V7 and Gur7\n",
    "TABLE13[('Time (s)','V7 - Gur7')] = TABLE13[('Time (s)','V7')] - TABLE13[('Time (s)','Gur7')]\n",
    "\n",
    "# Sort by V7 - Gur7\n",
    "TABLE13.sort_values(by=[(time_col_header,'V7 - Gur7')], inplace=True)\n",
    "\n",
    "TABLE13.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "### DEBUG DEBUG DEBUG\n",
    "# inst = 'cost266-UUE_presolved'\n",
    "# hawea instance\n",
    "inst = 'neos-3592146-hawea_presolved'\n",
    "if inst in all_bb_results_df.index:\n",
    "    display(all_bb_results_df.loc[inst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Format Table 13: \"best\" time/nodes results\n",
    "TABLE13 = all_bb_results_df.copy(deep=True)\n",
    "\n",
    "# Drop all rows in which (time_col_header, 'Gur7') is ''\n",
    "TABLE13 = TABLE13[TABLE13[(time_col_header, 'Gur7')] != '']\n",
    "\n",
    "# TABLE13[('','V7-Gur7')]\n",
    "# Change column (time_col_header, V7) to be float valued\n",
    "#TABLE13[(time_col_header, 'V7')] = TABLE13[(time_col_header, 'V7')].apply(float_format, num_digits=4)\n",
    "TABLE13 = TABLE13.astype({(time_col_header, 'V7'): float})\n",
    "TABLE13 = TABLE13.astype({(time_col_header, 'Gur7'): float})\n",
    "\n",
    "# Add new column for difference between V7 and Gur7\n",
    "TABLE13[('Time (s)','V7 - Gur7')] = TABLE13[('Time (s)','V7')] - TABLE13[('Time (s)','Gur7')]\n",
    "\n",
    "# Sort by V7 - Gur7\n",
    "TABLE13.sort_values(by=[(time_col_header,'V7 - Gur7')], inplace=True)\n",
    "\n",
    "# Rename summary rows to reflect the set\n",
    "rename_metrics_all = {metric : metric + ' (All)' for metric in bb_metrics}\n",
    "TABLE13.rename(rename_metrics_all, inplace=True)\n",
    "\n",
    "# Add summary rows from 6 trees set\n",
    "summary_metrics_6trees = all6_bb_results_df.tail(3).copy(deep=True)\n",
    "rename_metrics_6trees = {metric : metric + ' (6 trees)' for metric in bb_metrics}\n",
    "summary_metrics_6trees.rename(rename_metrics_6trees, inplace=True)\n",
    "\n",
    "TABLE13 = pd.concat([TABLE13, summary_metrics_6trees])\n",
    "\n",
    "# Drop rows, cols, (time,V7)\n",
    "TABLE13.drop([('','Rows'),('','Cols'),(node_col_header,map_cols_to_short_time[mintime_col])], axis=1, inplace=True)\n",
    "\n",
    "# Set wins row to be integer valued\n",
    "TABLE13.loc['Wins1 (All)'] = TABLE13.loc['Wins1 (All)'].apply(int_format)\n",
    "# TABLE13.loc['Wins7 (All)'] = TABLE13.loc['Wins7 (All)'].apply(int_format)\n",
    "# TABLE13.loc['Wins1 (6 trees)'] = TABLE13.loc['Wins1 (6 trees)'].apply(int_format)\n",
    "# TABLE13.loc['Wins7 (6 trees)'] = TABLE13.loc['Wins7 (6 trees)'].apply(int_format)\n",
    "# TABLE13.iloc[len(TABLE13)-1] = TABLE13.iloc[len(TABLE13)-1].apply(int_format)\n",
    "\n",
    "# Move instance names into a column\n",
    "TABLE13.reset_index(inplace=True, col_level=1)\n",
    "\n",
    "# Store indices of rows of 6-tree instances\n",
    "six_trees_instances = list(all6_instances_dict.keys())\n",
    "# mask = TABLE13[('','Instance')].isin(six_trees_instances)\n",
    "# six_trees_indices = TABLE13.loc[mask, :].index.tolist()\n",
    "\n",
    "# Remove presolved from name and escape\n",
    "TABLE13[('',\"Instance\")] = TABLE13[('',\"Instance\")].apply(remove_presolved_from_name)\n",
    "TABLE13[('',\"Instance\")] = TABLE13[('',\"Instance\")].apply(tex_escape)\n",
    "\n",
    "# If we are not using the automatic tex-escaper, we need to do it ourselves\n",
    "for col in TABLE13.columns:\n",
    "    # curr_col = '{' + tex_escape(col) + '}'\n",
    "    curr_col = tex_escape(str(col))\n",
    "    TABLE13.rename({col: curr_col}, inplace=True, axis=1)\n",
    "\n",
    "# Finally, apply the desired style\n",
    "    # format(formatter = int_format).\\\n",
    "table13_str = TABLE13.style.\\\n",
    "    hide(axis=0).\\\n",
    "    to_latex(\n",
    "        column_format=\"\"\"\n",
    "\t@{}l % instance\n",
    "\t*{1}{S[table-format=4.0,table-auto-round,table-number-alignment=center]} % # cuts\n",
    "\t*{2}{S[table-format=4.2,table-auto-round]} % Gur1, Gur7\n",
    "\t*{2}{H} % V, Total\n",
    "\t*{2}{S[table-format=4.2,table-auto-round]} % V7, Total7\n",
    "\t*{3}{S[table-format=8.0,table-auto-round,table-number-alignment=center]} % Nodes\n",
    "\t@{}\n",
    "        \"\"\",\n",
    "        hrules = True,\n",
    "        sparse_index = True,\n",
    "        multirow_align = \"c\",\n",
    "        siunitx=True,\n",
    "        convert_css = True,\n",
    "        environment = \"table\",\n",
    "        position_float = \"centering\",\n",
    "        label = \"app:tab:bb\",\n",
    "        caption = \"\"\"\n",
    "            Time (in seconds) and number nodes taken to solve each instance.\n",
    "            The table is sorted by column 4 (``V'' under ``Time (s)'').\n",
    "            ``Gur1'' indicates \\Gurobi{} run with one random seed.\n",
    "            ``Gur7'' indicates the minimum from seven runs of \\Gurobi{} with different random seeds.\n",
    "        \"\"\",\n",
    "        )\n",
    "\n",
    "# Adjustbox environment sets width to pagewidth\n",
    "# table13_str = add_adjustbox_environment(table13_str)\n",
    "\n",
    "# Set default siunitx options for this table\n",
    "table13_str = add_sisetup(table13_str)\n",
    "\n",
    "# Add a midrule between the instances and 3 summary rows; the \"6\" is hand-coded but can be automated\n",
    "table13_str = add_midrule(table13_str, -6)\n",
    "table13_str = add_midrule(table13_str, -10)\n",
    "\n",
    "# Add color to six tree instances\n",
    "splitlines = table13_str.splitlines()\n",
    "for i in range(len(splitlines)):\n",
    "    line = splitlines[i]\n",
    "    curr_line = line.split('&')\n",
    "    if len(curr_line) > 0 and curr_line[0].strip()+'_presolved' in six_trees_instances:\n",
    "        splitlines[i] = '\\\\rowcolor{lightgray!30} ' + line\n",
    "table13_str = '\\n'.join(splitlines).replace('NaN', '')\n",
    "\n",
    "print(table13_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD Section 3: Time tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `time_df`: Create subset of dataframe relevant to time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Create subset of dataframe relevant to time\n",
    "time_df = df.loc[:,\n",
    "                [\n",
    "                    'NUM DISJ TERMS',\n",
    "                    'ROWS',\n",
    "                    'COLS',\n",
    "                    'LP OBJ',\n",
    "                    'IP OBJ',\n",
    "                    'FIRST REF OBJ',\n",
    "                    'AVG REF OBJ',\n",
    "                    'BEST REF OBJ',\n",
    "                    'FIRST REF+V OBJ',\n",
    "                    'AVG REF+V OBJ',\n",
    "                    'FIRST REF BOUND',\n",
    "                    'AVG REF BOUND',\n",
    "                    'BEST REF BOUND',\n",
    "                    'FIRST REF+V BOUND',\n",
    "                    'AVG REF+V BOUND',\n",
    "                    'FIRST REF ITERS',\n",
    "                    'AVG REF ITERS',\n",
    "                    'BEST REF ITERS',\n",
    "                    'FIRST REF+V ITERS',\n",
    "                    'AVG REF+V ITERS',\n",
    "                    'FIRST REF NODES',\n",
    "                    'AVG REF NODES',\n",
    "                    'BEST REF NODES',\n",
    "                    'FIRST REF+V NODES',\n",
    "                    'AVG REF+V NODES',\n",
    "                    'FIRST REF TIME',\n",
    "                    'AVG REF TIME',\n",
    "                    'BEST REF TIME',\n",
    "                    'FIRST REF+V TIME',\n",
    "                    'AVG REF+V TIME',\n",
    "                    'VPC_GEN_TIME',\n",
    "                    'NUM GMIC',\n",
    "                    'NUM VPC',\n",
    "                    'NUM OBJ',\n",
    "                    'ALL REF TIME',\n",
    "                    'ALL REF+V TIME',\n",
    "                    'ExitReason']\n",
    "               ]\n",
    "#display(time_df.loc[(\"bm23_presolved\",2)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare short/long column names for time dfs\n",
    "1. First run of Gurobi without VPCs\n",
    "2. Best among 7 runs of Gurobi without VPCs\n",
    "3. First run of Gurobi with VPCs for each disjunction size\n",
    "4. First run of Gurobi with VPCs for each disjunction size, adding cut generation time\n",
    "5. Best run across first Gurobi without VPCs and first Gurobi with VPCs (across all terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Gur1/Gur7 names\n",
    "gur1_col_stub = 'AVG REF' # Should we change to AVG?\n",
    "gur7_col_stub = 'BEST REF'\n",
    "# gur_w_v_col_stub = 'AVG REF'\n",
    "gur1v_col_stub = gur1_col_stub + '+V'\n",
    "gur1v_w_cut_col_stub = gur1v_col_stub + ' W/CUTGEN'\n",
    "\n",
    "# gur1time: first run of Gurobi without VPCs\n",
    "gur1time_col = gur1_col_stub + ' TIME'\n",
    "gur1nodes_col = gur1_col_stub + ' NODES'\n",
    "\n",
    "# gur7time: best among 7 runs of Gurobi without VPCs\n",
    "gur7time_col = gur7_col_stub + ' TIME'\n",
    "gur7nodes_col = gur7_col_stub + ' NODES'\n",
    "\n",
    "# gur1vtime: first run of Gurobi w/VPCs for each disj size\n",
    "gur1vtime_col = gur1v_col_stub + ' TIME'\n",
    "gur1vnodes_col = gur1v_col_stub + ' NODES'\n",
    "\n",
    "# gur1v_w_cut_time: first run of Gurobi w/VPCs for each disj size, counting cut generation time\n",
    "gur1v_w_cut_time_col = gur1v_w_cut_col_stub + ' TIME'\n",
    "\n",
    "# Track best disjunction used in 0-row\n",
    "gurv_disj_col = gur1v_col_stub + ' DISJ'\n",
    "gurv_w_cut_disj_col = gur1v_w_cut_col_stub + ' DISJ'\n",
    "\n",
    "# Best Gurobi run across the first without VPCs and first w/VPCs for each disj size\n",
    "mintime_col       = 'MIN BB TIME'\n",
    "mintime_w_cut_col = 'MIN BB W/CUTGEN TIME'\n",
    "mintime_disj_col  = 'MIN BB TIME DISJ'\n",
    "minnodes_col      = 'MIN BB NODES'\n",
    "\n",
    "map_cols_to_short_time = {\n",
    "    gur1time_col         : 'Gur1',\n",
    "    gur7time_col         : 'Gur7',\n",
    "    gur1vtime_col        : 'V',\n",
    "    gur1v_w_cut_time_col : 'Total',\n",
    "    mintime_col          : 'V7',\n",
    "    mintime_w_cut_col    : 'Total7',\n",
    "}\n",
    "\n",
    "map_cols_to_short_nodes = {\n",
    "    gur1nodes_col        : 'Gur1',\n",
    "    gur7nodes_col        : 'Gur7',\n",
    "    gur1vnodes_col       : 'V',\n",
    "    minnodes_col         : 'V7',\n",
    "}\n",
    "\n",
    "map_short_to_cols_time = {v: k for k, v in map_cols_to_short_time.items()}\n",
    "map_short_to_cols_nodes = {v: k for k, v in map_cols_to_short_nodes.items()}\n",
    "\n",
    "time_cols_short = list(map_short_to_cols_time.keys())\n",
    "node_cols_short = list(map_short_to_cols_nodes.keys())\n",
    "# display(time_cols, node_cols)\n",
    "\n",
    "# Select a subset of columns for the \"long\" list used when updating the 0-row\n",
    "time_cols_long = [map_short_to_cols_time[col] for col in time_cols_short]\n",
    "node_cols_long = [map_short_to_cols_nodes[col] for col in node_cols_short]\n",
    "\n",
    "# # Update list of columns with mintime cols\n",
    "# newshortcol1 = 'V7'\n",
    "# newshortcol2 = 'Total7'\n",
    "# newshortcol3 = 'V7'\n",
    "# map_cols_to_short_time [mintime_col]       = newshortcol1\n",
    "# map_cols_to_short_time [mintime_w_cut_col] = newshortcol2\n",
    "# map_cols_to_short_nodes[minnodes_col]      = newshortcol3\n",
    "\n",
    "# map_short_to_cols_time [newshortcol1]      = mintime_col\n",
    "# map_short_to_cols_time [newshortcol2]      = mintime_w_cut_col\n",
    "# map_short_to_cols_nodes[newshortcol3]      = minnodes_col\n",
    "\n",
    "# time_cols_short.append(newshortcol1)\n",
    "# time_cols_short.append(newshortcol2)\n",
    "# node_cols_short.append(newshortcol3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add total time for running solver + generating cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Add total time for running solver + generating cuts\n",
    "time_df[gur1v_w_cut_time_col] = time_df[gur1vtime_col] + time_df[col_vpc_gen_time]\n",
    "\n",
    "display(time_df.loc['bm23_presolved'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `selected_time_df`: Solving and cut-generation time for instances selected for time reporting; 0-row with min values across all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Solving and cut-generation time for instances selected for time reporting\n",
    "selected_time_df = time_df.loc[selected_time_instances_dict.keys()]\n",
    "selected_time_df.index = selected_time_df.index.remove_unused_levels()\n",
    "selected_time_df[minnodes_col] = 0\n",
    "\n",
    "## Fill in 0-row with min values across all rows\n",
    "## Also fill in gur1 values (present only in 0 row currently) for all disj terms\n",
    "comparison_time_cols = [gur1vtime_col, gur1v_w_cut_time_col]\n",
    "comparison_node_cols = [gur1vnodes_col]\n",
    "cols_to_display = [col_num_vpc]+[gur1time_col,gur1vtime_col]+[gur1nodes_col,gur1vnodes_col]+[mintime_col,mintime_w_cut_col,minnodes_col,gurv_disj_col,gurv_w_cut_disj_col,mintime_disj_col]\n",
    "inst_set = selected_time_df.index.levels[0]\n",
    "# tmp_inst = '23588_presolved'\n",
    "# inst_set = ['10teams_presolved',tmp_inst]\n",
    "for i, inst in enumerate(inst_set):\n",
    "    print(\"{}/{}\".format(i+1,len(inst_set)), end='\\r', flush=True)\n",
    "    curr_df = selected_time_df.loc[inst].copy() # copy needed to not throw SettingWithCopyWarning\n",
    "    \n",
    "    # Select only the rows in which VPCs were generated\n",
    "    curr_df_with_vpcs = curr_df[curr_df[col_num_vpc] > 0]\n",
    "    \n",
    "    # display(inst)\n",
    "    # display(curr_df_with_vpcs[[col_num_vpcs]+[gur1time_col,gur1vtime_col]])\n",
    "\n",
    "    # Set 0-row to have min time values across all (non-0-vpc) rows for this instance\n",
    "    # best_vals = curr_df_with_vpcs[comparison_time_cols].min()\n",
    "    # selected_time_df.loc[(inst,0),comparison_time_cols] = best_vals\n",
    "    best_vals_idx = curr_df_with_vpcs[comparison_time_cols].idxmin()\n",
    "    for curr_col, curr_disj_id in zip(comparison_time_cols, best_vals_idx):\n",
    "        selected_time_df.at[(inst,0),curr_col] = curr_df_with_vpcs.at[curr_disj_id, curr_col]\n",
    "    \n",
    "    # display(best_vals_idx)\n",
    "    # print(\"selected_time_df.at[('{}',0),gur1vtime_col] = {}\".format(inst,selected_time_df.at[(inst,0),gur1vtime_col]))\n",
    "    # display(selected_time_df[[col_num_vpcs]+[gur1time_col,gur1vtime_col]].head(14))\n",
    "    # print(\"selected_time_df.at[('{}',0),gur1vtime_col] = {}\".format(inst,selected_time_df.at[(inst,0),gur1vtime_col]))\n",
    "\n",
    "\n",
    "    # Also add id of the best disj to the 0-row\n",
    "    selected_time_df.at[(inst,0),gurv_disj_col]       = int(best_vals_idx.iloc[0])\n",
    "    selected_time_df.at[(inst,0),gurv_w_cut_disj_col] = int(best_vals_idx.iloc[1])\n",
    "\n",
    "    # Update 0-row of mintime (V7) entries\n",
    "    curr_gur1time       = selected_time_df.at[(inst,0),gur1time_col]\n",
    "    curr_gur1vtime      = selected_time_df.at[(inst,0),gur1vtime_col]\n",
    "    curr_gur1vcuts_time = selected_time_df.at[(inst,0),gur1v_w_cut_time_col]\n",
    "    \n",
    "    curr_vals = [curr_gur1time, curr_gur1vtime]\n",
    "    min_id = np.argmin(curr_vals)\n",
    "\n",
    "    # If min_id is 0, then no cuts are used and we report the gur1 time\n",
    "    # If min_id is 1, then gur1v < gur1 and we can report the number of cuts used\n",
    "    selected_time_df.at[(inst,0),mintime_col] = curr_vals[min_id]\n",
    "\n",
    "    # Add num cuts from mintime disj into num vpc col\n",
    "    best_disj_size = 0 if min_id == 0 else best_vals_idx.iloc[0]\n",
    "    selected_time_df.at[(inst,0),mintime_disj_col] = best_disj_size\n",
    "    best_num_cuts = selected_time_df.at[(inst,best_disj_size),col_num_vpc]\n",
    "    selected_time_df.at[(inst,0),col_num_vpc] = best_num_cuts\n",
    "\n",
    "    # Update with cuts into Total7 column\n",
    "    curr_vals = [curr_gur1time, curr_gur1vcuts_time]\n",
    "    selected_time_df.at[(inst,0),mintime_w_cut_col] = min(curr_vals)\n",
    "\n",
    "    # Repeat for nodes\n",
    "    best_vals = curr_df_with_vpcs[comparison_node_cols].min()\n",
    "    selected_time_df.loc[(inst,0),comparison_node_cols] = best_vals\n",
    "    # selected_time_df.at[(inst,0),minnodes_col] = int(selected_time_df.loc[(inst,0),[gur1nodes_col,gur1vnodes_col]].min())\n",
    "\n",
    "    curr_gur1nodes       = selected_time_df.at[(inst,0),gur1nodes_col]\n",
    "    curr_gur1vnodes      = selected_time_df.at[(inst,0),gur1vnodes_col]\n",
    "    curr_vals = [curr_gur1nodes, curr_gur1vnodes]\n",
    "    min_id = np.argmin(curr_vals)\n",
    "    selected_time_df.at[(inst,0),minnodes_col] = int(curr_vals[min_id])\n",
    "\n",
    "    # Propogate down 0-row values for gur1 columns\n",
    "    selected_time_df.loc[inst, gur1time_col] = curr_gur1time\n",
    "    selected_time_df.loc[inst, gur1nodes_col] = curr_gur1nodes\n",
    "\n",
    "    #### FOR SOME REASON, THE BELOW ZEROES OUT selected_time_df.loc[[(inst,0)]][gur1vtime_col]\n",
    "    # display(selected_time_df.loc[(inst,0),[gur1nodes_col,gur1vnodes_col]])\n",
    "\n",
    "    ## OLD CODE BELOW\n",
    "    # best_vals_idx = curr_df_with_vpcs[comparison_node_cols].idxmin()\n",
    "    # for curr_col, curr_disj_id in zip(comparison_node_cols, best_vals_idx):\n",
    "    #     selected_time_df.at[(inst,0),curr_col] = curr_df_with_vpcs.loc[curr_disj_id, curr_col]\n",
    "\n",
    "    # # Also add id of the best disj to the 0-row\n",
    "    # selected_time_df.at[(inst,0),gurv_disj_col + ' (NODES)'] = int(best_vals_idx[0])\n",
    "    # selected_time_df.at[(inst,0),gurv_w_cut_disj_col+ ' (NODES)'] = int(best_vals_idx[1])\n",
    "\n",
    "    # for ind in curr_df.index:\n",
    "    #     if ind == 0:\n",
    "    #         continue\n",
    "\n",
    "    #     # Propogate GurF and GurL down\n",
    "    #     subinds = [4,6]\n",
    "    #     sel_gap = [gap_cols[i] for i in subinds]\n",
    "    #     selected_gap_df.loc[(inst,ind),sel_gap] = curr_df.loc[0,sel_gap]\n",
    "\n",
    "    #     # If no VPCs produced, the values for V+GurF and V+GurL have not been provided\n",
    "    #     # We replace these by GurF and GurL\n",
    "    #     # Currently disabled: update max for that column too (if disabled, we instead keep max as the value among those that generated VPCs)\n",
    "    #     num_vpc = curr_df.loc[ind,col_num_vpcs]\n",
    "    #     if num_vpc == 0:\n",
    "    #         # print(\"Zero cuts for inst {} at depth {:d}\".format(inst, ind))\n",
    "    #         subinds = [5,7]\n",
    "    #         refinds = [4,6]\n",
    "    #         sel_gap = [gap_cols[i] for i in subinds]\n",
    "    #         selected_gap_df.loc[(inst,ind),sel_gap] = curr_df.loc[0,[gap_cols[i] for i in refinds]].to_numpy()\n",
    "\n",
    "    #         # for i in refinds:\n",
    "    #         #     if curr_df.loc[0,gap_cols[i]] > selected_gap_df.loc[(inst,0),gap_cols[i+1]]:\n",
    "    #         #         if curr_df.loc[0,gap_cols[i]] > 0:\n",
    "    #         #             # print(\"DEBUG: Updating {} for inst {} from {:f} to {:f}\".format(\n",
    "    #         #             #     gap_cols[i+1], \n",
    "    #         #             #     inst, \n",
    "    #         #             #     selected_gap_df.loc[(inst,0),gap_cols[i+1]], \n",
    "    #         #             #     curr_df.loc[0,gap_cols[i]]))\n",
    "    #         #         selected_gap_df.loc[(inst,0),gap_cols[i+1]] = curr_df.loc[0,gap_cols[i]]\n",
    "\n",
    "# Add minimum time when using cuts and when not using cuts\n",
    "# selected_time_df[mintime_col] = selected_time_df[[gur1time_col, gur1vtime_col]].min(axis=1)\n",
    "# selected_time_df[mintime_w_cut_col] = selected_time_df[[gur1time_col, gur1v_w_cut_time_col]].min(axis=1)\n",
    "# selected_time_df[minnodes_col] = selected_time_df[[gur1nodes_col,gur1vnodes_col]].min(axis=1)\n",
    "\n",
    "display(selected_time_df.head(35).loc[:,[col_num_vpc]+[gur1time_col,gur1vtime_col]+[gur1nodes_col,gur1vnodes_col]+[mintime_col,mintime_w_cut_col,minnodes_col,gurv_disj_col,gurv_w_cut_disj_col,mintime_disj_col]])\n",
    "# display(selected_time_df.loc['10teams_presolved',[col_num_vpcs]+[gur1time_col,gur1vtime_col]+[gur1nodes_col,gur1vnodes_col]+[mintime_col,mintime_w_cut_col,minnodes_col,gurv_disj_col,gurv_w_cut_disj_col,mintime_disj_col]])\n",
    "# display(selected_time_df.loc[inst_set,cols_to_display])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DEBUGGING that first ref+v time gets zeroed out for some reason?\n",
    "# tmp_df = selected_time_df[[col_num_vpcs]+[gur1time_col,gur1vtime_col]].head(14).copy(deep=True)\n",
    "# display(tmp_df)\n",
    "\n",
    "# print(tmp_df.loc[('23588_presolved',0),gur1vtime_col])\n",
    "# display(tmp_df.loc[[('23588_presolved',0)]][gur1vtime_col])\n",
    "\n",
    "# tmp_df = selected_time_df\n",
    "# print(tmp_df.loc[('23588_presolved',0),gur1vtime_col])\n",
    "# display(tmp_df.loc[[('23588_presolved',0)]][gur1vtime_col])\n",
    "# display(tmp_df.loc['23588_presolved'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: `avg_bb_df`: average time/nodes taken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare variables for row/col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Prepare variables for row/col names\n",
    "\n",
    "bb_classes = ['All', '6 trees', 'Binary']\n",
    "num_bb_classes = len(bb_classes)\n",
    "\n",
    "bucket_min = [0, 10, 100, 1000]\n",
    "bucket_max = [3600, 3600, 3600, 3600]\n",
    "num_buckets = len(bucket_min)\n",
    "assert(len(bucket_max) == num_buckets)\n",
    "bb_buckets = ['[' + str(bucket_min[j]) + ',' + str(bucket_max[j]) + ')' for j in range(num_buckets)]\n",
    "# bucket_names = [classes[i] + ' [' + str(bucket_min[j]) + ',' + str(bucket_max[j]) + ')' for i in range(num_classes) for j in range(num_buckets)]\n",
    "# display(bucket_names)\n",
    "\n",
    "bb_metrics = ['Gmean', 'Wins1', 'Wins7']\n",
    "\n",
    "time_col_header = 'Time (s)'\n",
    "node_col_header = 'Nodes (#)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up empty `avg_bb_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Prepare avg_bb_df\n",
    "\n",
    "avg_bb_cols = pd.MultiIndex.from_arrays(\n",
    "    [[time_col_header]*len(time_cols_short) + [node_col_header]*len(node_cols_short), time_cols_short + node_cols_short],\n",
    "    names = ['criterion', 'type'])\n",
    "\n",
    "#bb_row_names = pd.MultiIndex.from_product([bb_buckets, bb_row_names], names=['bucket', 'metric'])\n",
    "bb_row_names = pd.MultiIndex.from_product(\n",
    "    [bb_classes, bb_buckets, bb_metrics],\n",
    "    names=['class', 'bucket', 'metric'])\n",
    "\n",
    "avg_bb_df = pd.DataFrame(\n",
    "    columns = avg_bb_cols,\n",
    "    index = bb_row_names,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "display(avg_bb_df.loc[:,avg_bb_cols.get_level_values(0)==node_col_header].head(6))\n",
    "#display(avg_bb_df.loc[(bb_classes[0], bb_buckets[1], bb_metrics[0]),:])\n",
    "display(avg_bb_df.loc[(bb_classes[0], bb_buckets, bb_metrics[0]),:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `avg_bb_df`: shifted geometric mean of time taken across instances, in various buckets, and geomean of nodes too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Create gmean_df\n",
    "#   = shifted geometric mean of time taken across instances, in various buckets\n",
    "#     and geomean of nodes too\n",
    "\n",
    "# Custom functions for prior to python 3.8\n",
    "# def geo_mean(iterable):\n",
    "#     a = np.array(iterable)\n",
    "#     return a.prod()**(1.0/len(a))\n",
    "# def geo_mean_overflow(iterable):\n",
    "#     return np.exp(np.log(iterable).mean())\n",
    "from statistics import geometric_mean\n",
    "SHIFT_TIME  = 60\n",
    "SHIFT_NODES = 1000\n",
    "\n",
    "num_inst = np.zeros(len(avg_bb_df),dtype = np.int64)\n",
    "row_ind = 0\n",
    "\n",
    "#avg_bb_df.loc[(bb_classes[0], bb_buckets, bb_metrics[0]),:] = \\\n",
    "shortcols_time = time_cols_short\n",
    "cols_time = [map_short_to_cols_time[shortcol] for shortcol in shortcols_time]\n",
    "shortcols_nodes = node_cols_short\n",
    "cols_nodes = [map_short_to_cols_nodes[shortcol] for shortcol in shortcols_nodes]\n",
    "\n",
    "cols = cols_time + cols_nodes\n",
    "shortcols = shortcols_time + shortcols_nodes\n",
    "\n",
    "# First calculate stats for \"all\" instances\n",
    "curr_df = selected_time_df.loc[:,cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only \"best\" values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    avg_bb_df.loc[(bb_classes[0], bb_buckets[i], bb_metrics[0]),(time_col_header,shortcols_time)] = \\\n",
    "        [geometric_mean(curr_df[col] + SHIFT_TIME) - SHIFT_TIME for col in cols_time]\n",
    "    avg_bb_df.loc[(bb_classes[0], bb_buckets[i], bb_metrics[0]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [geometric_mean(curr_df[col] + SHIFT_NODES) - SHIFT_NODES for col in cols_nodes]\n",
    "    \n",
    "    print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "    \n",
    "    num_inst[row_ind:row_ind+len(bb_metrics)] = len(bb_metrics)*[len(curr_df)]\n",
    "    row_ind += len(bb_metrics)\n",
    "\n",
    "# Now calculate stats for \"6 trees\" instances\n",
    "curr_df = selected_time_df.loc[all6_instances_dict.keys(),cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    avg_bb_df.loc[(bb_classes[1], bb_buckets[i], bb_metrics[0]),(time_col_header,shortcols_time)] = \\\n",
    "        [geometric_mean(curr_df[col] + SHIFT_TIME) - SHIFT_TIME for col in cols_time]\n",
    "    avg_bb_df.loc[(bb_classes[1], bb_buckets[i], bb_metrics[0]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [geometric_mean(curr_df[col] + SHIFT_NODES) - SHIFT_NODES for col in cols_nodes]\n",
    "    \n",
    "    print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "\n",
    "    num_inst[row_ind:row_ind+len(bb_metrics)] = len(bb_metrics)*[len(curr_df)]\n",
    "    row_ind += len(bb_metrics)\n",
    "\n",
    "avg_bb_df[inst_col_name] = num_inst\n",
    "# avg_bb_df['NUM INST'] = avg_bb_df['NUM INST'].astype(np.int64)\n",
    "\n",
    "# Repeat for \"binary\" instances\n",
    "# identify pure_binary_instances that are in selected_time_instances_dict\n",
    "binary_x_time_instances = [inst for inst in pure_binary_instances if inst in selected_time_instances_dict.keys()]\n",
    "curr_df = selected_time_df.loc[binary_x_time_instances,cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    avg_bb_df.loc[(bb_classes[2], bb_buckets[i], bb_metrics[0]),(time_col_header,shortcols_time)] = \\\n",
    "        [geometric_mean(curr_df[col] + SHIFT_TIME) - SHIFT_TIME for col in cols_time]\n",
    "    avg_bb_df.loc[(bb_classes[2], bb_buckets[i], bb_metrics[0]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [geometric_mean(curr_df[col] + SHIFT_NODES) - SHIFT_NODES for col in cols_nodes]\n",
    "    \n",
    "    print(\"row {:d}: {:d}\".format(row_ind,len(curr_df)))\n",
    "\n",
    "    num_inst[row_ind:row_ind+len(bb_metrics)] = len(bb_metrics)*[len(curr_df)]\n",
    "    row_ind += len(bb_metrics)\n",
    "\n",
    "avg_bb_df[inst_col_name] = num_inst\n",
    "# avg_bb_df['NUM INST'] = avg_bb_df['NUM INST'].astype(np.int64)\n",
    "\n",
    "display(avg_bb_df.loc[(bb_classes, bb_buckets, bb_metrics[0]),:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update wins1 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Update wins1 rows\n",
    "# A win in terms of time is counted when the ``Gur1'' baseline seconds taken \n",
    "# is at least 10\\% slower, to account for some variability in runtimes.\n",
    "# A win in terms of nodes is when the ``Gur1'' baseline number of nodes is higher.\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "avg_bb_df.loc[:,(time_col_header,shortcols_time)] = avg_bb_df.loc[:,(time_col_header,shortcols_time)].astype(object)\n",
    "avg_bb_df.loc[:,(node_col_header,shortcols_nodes)] = avg_bb_df.loc[:,(node_col_header,shortcols_nodes)].astype(object)\n",
    "\n",
    "# First calculate stats for \"all\" instances\n",
    "curr_df = selected_time_df.loc[:,cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    refcol = gur1time_col\n",
    "    avg_bb_df.loc[(bb_classes[0], bb_buckets[i], bb_metrics[1]),(time_col_header,shortcols_time)] = \\\n",
    "        [ int(sum(curr_df[refcol] > 1.1*curr_df[col])) for col in cols_time ]\n",
    "\n",
    "    refcol = gur1nodes_col\n",
    "    avg_bb_df.loc[(bb_classes[0], bb_buckets[i], bb_metrics[1]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [ int(sum(curr_df[refcol] > curr_df[col])) for col in cols_nodes ]\n",
    "\n",
    "# Now calculate stats for \"6 trees\" instances\n",
    "curr_df = selected_time_df.loc[all6_instances_dict.keys(),cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    refcol = gur1time_col\n",
    "    avg_bb_df.loc[(bb_classes[1], bb_buckets[i], bb_metrics[1]),(time_col_header,shortcols_time)] = \\\n",
    "        [ int(sum(curr_df[refcol] > 1.1*curr_df[col])) for col in cols_time ]\n",
    "\n",
    "    refcol = gur1nodes_col\n",
    "    avg_bb_df.loc[(bb_classes[1], bb_buckets[i], bb_metrics[1]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [ int(sum(curr_df[refcol] > curr_df[col])) for col in cols_nodes ]\n",
    "    \n",
    "# Repeat for binary instances\n",
    "curr_df = selected_time_df.loc[binary_x_time_instances,cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    refcol = gur1time_col\n",
    "    avg_bb_df.loc[(bb_classes[2], bb_buckets[i], bb_metrics[1]),(time_col_header,shortcols_time)] = \\\n",
    "        [ int(sum(curr_df[refcol] > 1.1*curr_df[col])) for col in cols_time ]\n",
    "\n",
    "    refcol = gur1nodes_col\n",
    "    avg_bb_df.loc[(bb_classes[2], bb_buckets[i], bb_metrics[1]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [ int(sum(curr_df[refcol] > curr_df[col])) for col in cols_nodes ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update wins7 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Update wins7 rows\n",
    "# A win in terms of time is counted when the ``Gur1'' baseline seconds taken \n",
    "# is at least 10\\% slower, to account for some variability in runtimes.\n",
    "# A win in terms of nodes is when the ``Gur1'' baseline number of nodes is higher.\n",
    "\n",
    "# First calculate stats for \"all\" instances\n",
    "curr_df = selected_time_df.loc[:,cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    refcol = gur7time_col\n",
    "    avg_bb_df.loc[(bb_classes[0], bb_buckets[i], bb_metrics[2]),(time_col_header,shortcols_time)] = \\\n",
    "        [ int(sum(curr_df[refcol] > 1.1*curr_df[col])) for col in cols_time ]\n",
    "\n",
    "    refcol = gur7nodes_col\n",
    "    avg_bb_df.loc[(bb_classes[0], bb_buckets[i], bb_metrics[2]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [ int(sum(curr_df[refcol] > curr_df[col])) for col in cols_nodes ]\n",
    "\n",
    "# Now calculate stats for \"6 trees\" instances\n",
    "curr_df = selected_time_df.loc[all6_instances_dict.keys(),cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    refcol = gur7time_col\n",
    "    avg_bb_df.loc[(bb_classes[1], bb_buckets[i], bb_metrics[2]),(time_col_header,shortcols_time)] = \\\n",
    "        [ int(sum(curr_df[refcol] > 1.1*curr_df[col])) for col in cols_time ]\n",
    "\n",
    "    refcol = gur7nodes_col\n",
    "    avg_bb_df.loc[(bb_classes[1], bb_buckets[i], bb_metrics[2]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [ int(sum(curr_df[refcol] > curr_df[col])) for col in cols_nodes ]\n",
    "    \n",
    "# Repeat for binary instances\n",
    "curr_df = selected_time_df.loc[binary_x_time_instances,cols_time + cols_nodes]\n",
    "curr_df = curr_df[curr_df.index.get_level_values(1) == 0] # take only best values\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "    refcol = gur7time_col\n",
    "    avg_bb_df.loc[(bb_classes[2], bb_buckets[i], bb_metrics[2]),(time_col_header,shortcols_time)] = \\\n",
    "        [ int(sum(curr_df[refcol] > 1.1*curr_df[col])) for col in cols_time ]\n",
    "\n",
    "    refcol = gur7nodes_col\n",
    "    avg_bb_df.loc[(bb_classes[2], bb_buckets[i], bb_metrics[2]),(node_col_header,shortcols_nodes)] = \\\n",
    "        [ int(sum(curr_df[refcol] > curr_df[col])) for col in cols_nodes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# display(avg_bb_df.loc[:,cols.get_level_values(0)=='Nodes'].head(6))\n",
    "display(avg_bb_df.loc[(bb_classes[0:2], bb_buckets, bb_metrics[0:3]),:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 6: `all_bb_results_df`: all time/nodes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "inst_set = selected_time_df.index.levels[0]\n",
    "inst_set.set_names(\"Instance\",inplace=True)\n",
    "numcuts_col_header = '# cuts'\n",
    "\n",
    "col_idx = pd.MultiIndex.from_arrays(\n",
    "    [\n",
    "        ['', '', numcuts_col_header] + [time_col_header]*len(time_cols_short) + [node_col_header]*len(node_cols_short),\n",
    "        ['Rows', 'Cols', map_cols_to_short_time[gur1vtime_col]] + time_cols_short + node_cols_short\n",
    "    ],\n",
    ")\n",
    "\n",
    "all_bb_results_df = pd.DataFrame(\n",
    "    columns = col_idx,\n",
    "    index = inst_set,\n",
    "    dtype = object,\n",
    ")\n",
    "\n",
    "# Enter number of rows and cols\n",
    "tmp_df = df.xs(0, level='disj_terms').loc[inst_set,['ROWS','COLS']]\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[''],['Rows','Cols']])\n",
    "all_bb_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "# Enter number of cuts\n",
    "# tmp_df = selected_time_df.loc[(inst_set,0), ['NUM VPC']]\n",
    "tmp_df = selected_time_df.xs(0, level='disj_terms')['NUM VPC']\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[numcuts_col_header],[map_cols_to_short_time[gur1vtime_col]]])\n",
    "all_bb_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "# Enter time\n",
    "tmp_df = selected_time_df.xs(0, level='disj_terms')[time_cols_long]\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[time_col_header],time_cols_short])\n",
    "all_bb_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "# Enter nodes\n",
    "tmp_df = selected_time_df.xs(0, level='disj_terms')[node_cols_long]\n",
    "tmp_df.columns = pd.MultiIndex.from_product([[node_col_header],node_cols_short])\n",
    "all_bb_results_df.loc[:,tmp_df.columns] = tmp_df\n",
    "\n",
    "all_bb_results_df = all_bb_results_df.sort_values(by=[(time_col_header, map_cols_to_short_time[mintime_col])])\n",
    "\n",
    "# Add average + wins rows\n",
    "# Replace missing entries with empty string\n",
    "tmp_df = avg_bb_df.xs((bb_classes[0],bb_buckets[0])).copy(deep=True)\n",
    "tmp_df.drop(inst_col_name, axis=1, level=0, inplace=True)\n",
    "all_bb_results_df = pd.concat([all_bb_results_df, tmp_df]).fillna('',downcast=False)\n",
    "\n",
    "# Remove unnecessary entries\n",
    "all_bb_results_df.loc['Wins1',[\n",
    "        (time_col_header,map_cols_to_short_time[gur1time_col]),\n",
    "        (node_col_header,map_cols_to_short_nodes[gur1nodes_col])\n",
    "    ]] = \"\"\n",
    "# all_bb_results_df.loc['Wins1',([time_col_header,node_col_header],'Gur1')] = \"\"\n",
    "all_bb_results_df.loc['Wins7',[\n",
    "        (time_col_header,map_cols_to_short_time[gur1time_col]),\n",
    "        (time_col_header,map_cols_to_short_time[gur7time_col]),\n",
    "        (node_col_header,map_cols_to_short_nodes[gur1nodes_col]),\n",
    "        (node_col_header,map_cols_to_short_nodes[gur7nodes_col]),\n",
    "    ]] = \"\"\n",
    "# all_bb_results_df.loc['Wins7',([time_col_header,node_col_header],['Gur1','Gur7'])] = \"\"\n",
    "# all_bb_results_df = all_bb_results_df.fillna('',downcast=False)\n",
    "\n",
    "# Convert rows, cols, # cuts to int values\n",
    "tmp_cols = pd.MultiIndex.from_product([[''],['Rows','Cols']])\n",
    "all_bb_results_df.loc[inst_set,tmp_cols] = all_bb_results_df.loc[inst_set,tmp_cols].astype(np.int64)\n",
    "tmp_cols = pd.MultiIndex.from_product([[numcuts_col_header],[map_cols_to_short_time[gur1vtime_col]]])\n",
    "all_bb_results_df.loc[inst_set,tmp_cols] = all_bb_results_df.loc[inst_set,tmp_cols].astype(np.int64)\n",
    "\n",
    "# Rename inst col back to inst_row_name\n",
    "all_bb_results_df.index.set_names(\"Instance\",inplace=True)\n",
    "\n",
    "display(all_bb_results_df.head(15))\n",
    "display(all_bb_results_df.tail(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 7: ``all6_bb_results_df``: 6-trees time/nodes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "inst_set = all6_instances_dict.keys()\n",
    "all6_bb_results_df = all_bb_results_df.loc[inst_set]\n",
    "\n",
    "all6_bb_results_df = all6_bb_results_df.sort_values(by=[(time_col_header, map_cols_to_short_time[mintime_col])])\n",
    "\n",
    "# Add average + wins rows\n",
    "# Replace missing entries with empty string\n",
    "tmp_df = avg_bb_df.xs((bb_classes[1],bb_buckets[0])).copy(deep=True)\n",
    "tmp_df.drop(inst_col_name, axis=1, level=0, inplace=True)\n",
    "all6_bb_results_df = pd.concat([all6_bb_results_df, tmp_df]).fillna('',downcast=False)\n",
    "\n",
    "# Remove unnecessary entries\n",
    "all6_bb_results_df.loc['Wins1',[\n",
    "        (time_col_header,map_cols_to_short_time[gur1time_col]),\n",
    "        (node_col_header,map_cols_to_short_nodes[gur1nodes_col])\n",
    "    ]] = \"\"\n",
    "all6_bb_results_df.loc['Wins7',[\n",
    "        (time_col_header,map_cols_to_short_time[gur1time_col]),\n",
    "        (time_col_header,map_cols_to_short_time[gur7time_col]),\n",
    "        (node_col_header,map_cols_to_short_nodes[gur1nodes_col]),\n",
    "        (node_col_header,map_cols_to_short_nodes[gur7nodes_col]),\n",
    "    ]] = \"\"\n",
    "\n",
    "# Convert rows, cols, # cuts to int values\n",
    "tmp_cols = pd.MultiIndex.from_product([[''],['Rows','Cols']])\n",
    "all6_bb_results_df.loc[inst_set,tmp_cols] = all6_bb_results_df.loc[inst_set,tmp_cols].astype(np.int64)\n",
    "tmp_cols = pd.MultiIndex.from_product([[numcuts_col_header],[map_cols_to_short_time[gur1vtime_col]]])\n",
    "all6_bb_results_df.loc[inst_set,tmp_cols] = all6_bb_results_df.loc[inst_set,tmp_cols].astype(np.int64)\n",
    "\n",
    "# Rename inst col back to inst_row_name\n",
    "all6_bb_results_df.index.set_names(\"Instance\",inplace=True)\n",
    "\n",
    "display(all6_bb_results_df.head(15))\n",
    "display(all6_bb_results_df.tail(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 8: `avg_bb_by_depth_df`: average time/nodes by depth for all-six set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Prepare avg_bb_by_depth_df\n",
    "## Prepare variables for row/col names\n",
    "inst_set = all6_instances_dict.keys()\n",
    "\n",
    "bb_classes_by_depth = [str(t) + ' leaves' for t in sizes]\n",
    "num_bb_classes_by_depth = len(bb_classes_by_depth)\n",
    "\n",
    "bb_buckets_by_depth = bb_buckets\n",
    "bb_metrics_by_depth = bb_metrics[0:2]\n",
    "\n",
    "cols_time_by_depth       = [gur1time_col, gur1vtime_col, gur1v_w_cut_time_col]\n",
    "shortcols_time_by_depth  = [map_cols_to_short_time[col] for col in cols_time_by_depth]\n",
    "cols_nodes_by_depth      = [gur1nodes_col, gur1vnodes_col]\n",
    "shortcols_nodes_by_depth = [map_cols_to_short_nodes[col] for col in cols_nodes_by_depth]\n",
    "\n",
    "avg_bb_cols_by_depth = pd.MultiIndex.from_arrays(\n",
    "    [[time_col_header]*len(shortcols_time_by_depth) + \n",
    "     [node_col_header]*len(shortcols_nodes_by_depth), \n",
    "     shortcols_time_by_depth + shortcols_nodes_by_depth],\n",
    "    names = ['criterion', 'type'])\n",
    "\n",
    "# bucket_min = [0, 10, 100, 1000]\n",
    "# bucket_max = [3600, 3600, 3600, 3600]\n",
    "# num_buckets = len(bucket_min)\n",
    "# assert(len(bucket_max) == num_buckets)\n",
    "# bb_buckets = ['[' + str(bucket_min[j]) + ',' + str(bucket_max[j]) + ')' for j in range(num_buckets)]\n",
    "# # bucket_names = [classes[i] + ' [' + str(bucket_min[j]) + ',' + str(bucket_max[j]) + ')' for i in range(num_classes) for j in range(num_buckets)]\n",
    "# # display(bucket_names)\n",
    "\n",
    "# bb_metrics = ['Gmean', 'Wins1', 'Wins7']\n",
    "\n",
    "# time_col_header = 'Time (s)'\n",
    "# node_col_header = 'Nodes (\\\\#)'\n",
    "\n",
    "#bb_row_names = pd.MultiIndex.from_product([bb_buckets, bb_row_names], names=['bucket', 'metric'])\n",
    "bb_row_names_by_depth = pd.MultiIndex.from_product(\n",
    "    [bb_classes_by_depth, bb_buckets_by_depth, bb_metrics_by_depth],\n",
    "    names=['class', 'bucket', 'metric'])\n",
    "\n",
    "avg_bb_by_depth_df = pd.DataFrame(\n",
    "    columns = avg_bb_cols_by_depth,\n",
    "    index = bb_row_names_by_depth,\n",
    "    dtype = float\n",
    ")\n",
    "\n",
    "# Fill in values for Gur1 from avg_bb_df\n",
    "# display(\n",
    "#     avg_bb_df.loc[\n",
    "#         (bb_classes[1], bb_buckets, bb_metrics[0:2]),\n",
    "#         [(time_col_header,map_cols_to_short_time[gur1time_col]),\n",
    "#         (node_col_header,map_cols_to_short_nodes[gur1nodes_col])]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Make all columns \"object\" type to allow for integer values\n",
    "avg_bb_by_depth_df.loc[:,(time_col_header,shortcols_time_by_depth)] = avg_bb_by_depth_df.loc[:,(time_col_header,shortcols_time_by_depth)].astype(object)\n",
    "avg_bb_by_depth_df.loc[:,(node_col_header,shortcols_nodes_by_depth)] = avg_bb_by_depth_df.loc[:,(node_col_header,shortcols_nodes_by_depth)].astype(object)\n",
    "\n",
    "## Create gmean_df by depth\n",
    "#   = shifted geometric mean of time taken across instances, in various buckets\n",
    "#     and geomean of nodes too\n",
    "\n",
    "num_inst_by_depth = np.zeros(len(avg_bb_by_depth_df),dtype = np.int64)\n",
    "row_ind = 0\n",
    "\n",
    "cols = cols_time_by_depth + cols_nodes_by_depth\n",
    "shortcols = shortcols_time_by_depth + shortcols_nodes_by_depth\n",
    "\n",
    "# Calculate stats for 6 trees instances by depth\n",
    "curr_df = selected_time_df.loc[inst_set,cols]\n",
    "# curr_df = selected_time_df.loc[all6_binary_x_time_instances,cols]\n",
    "for curr_size_ind in range(0,len(bb_classes_by_depth)):\n",
    "    # print(\"{}\".format(bb_classes_by_depth[curr_size_ind]))\n",
    "    curr_by_depth_df = curr_df[curr_df.index.get_level_values(1) == sizes[curr_size_ind]] # take only best values\n",
    "\n",
    "    for i in range(num_buckets):\n",
    "        curr_by_depth_df = curr_by_depth_df[curr_by_depth_df[gur1time_col] > bucket_min[i]]\n",
    "        avg_bb_by_depth_df.loc[\n",
    "                (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[i], bb_metrics_by_depth[0]),\n",
    "                (time_col_header,shortcols_time_by_depth)] = \\\n",
    "            [geometric_mean(curr_by_depth_df[col] + SHIFT_TIME) - SHIFT_TIME for col in cols_time_by_depth]\n",
    "\n",
    "        # display(avg_bb_by_depth_df.loc[\n",
    "        #         (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[i], bb_metrics_by_depth[0]),\n",
    "        #         (time_col_header,shortcols_time_by_depth)].head())\n",
    "        avg_bb_by_depth_df.loc[\n",
    "                (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[i], bb_metrics_by_depth[0]),\n",
    "                (node_col_header,shortcols_nodes_by_depth)] = \\\n",
    "            [geometric_mean(curr_by_depth_df[col] + SHIFT_NODES) - SHIFT_NODES for col in cols_nodes_by_depth]\n",
    "        \n",
    "        # print(\"row {:d}: {:d}\".format(row_ind,len(curr_by_depth_df)))\n",
    "\n",
    "        num_inst_by_depth[row_ind:row_ind+len(bb_metrics_by_depth)] = len(bb_metrics_by_depth)*[len(curr_by_depth_df)]\n",
    "        row_ind += len(bb_metrics_by_depth)\n",
    "\n",
    "        ## Update wins1 rows\n",
    "        # A win in terms of time is counted when the ``Gur1'' baseline seconds taken \n",
    "        # is at least 10\\% slower, to account for some variability in runtimes.\n",
    "        # A win in terms of nodes is when the ``Gur1'' baseline number of nodes is higher.\n",
    "        refcol = gur1time_col\n",
    "        avg_bb_by_depth_df.loc[\n",
    "                (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[i], bb_metrics_by_depth[1]),\n",
    "                (time_col_header,shortcols_time_by_depth)] = \\\n",
    "            [ int(sum(curr_by_depth_df[refcol] > 1.1*curr_by_depth_df[col])) for col in cols_time_by_depth ]\n",
    "\n",
    "        refcol = gur1nodes_col\n",
    "        avg_bb_by_depth_df.loc[\n",
    "                (bb_classes_by_depth[curr_size_ind], bb_buckets_by_depth[i], bb_metrics_by_depth[1]),\n",
    "                (node_col_header,shortcols_nodes_by_depth)] = \\\n",
    "            [ int(sum(curr_by_depth_df[refcol] > curr_by_depth_df[col])) for col in cols_nodes_by_depth ]\n",
    "\n",
    "avg_bb_by_depth_df[inst_col_name] = num_inst_by_depth\n",
    "\n",
    "# for i in range(num_buckets):\n",
    "#     curr_df = curr_df[curr_df[gur1time_col] > bucket_min[i]]\n",
    "    \n",
    "display(avg_bb_by_depth_df.loc[(bb_classes_by_depth, bb_buckets_by_depth, bb_metrics_by_depth),:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for selected instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_df.loc['bell5_presolved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess.loc[[inst for inst in df_preprocess.index if 'fast' in inst]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('3.10.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aac1f530a99180ab5e13a2c940cb9e747f1066b1e82a06b7a1e078dd1f217861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
